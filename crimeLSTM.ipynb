{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crimeLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EPj2yuFSCw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8504dc-cf09-4fe0-992e-6792594a21c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JUv-o34UqVY",
        "outputId": "5f8a89de-7f10-4419-e345-c9cb4589deea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shapely.wkt\n",
        "from shapely.geometry import Point, Polygon, MultiPolygon\n",
        "\n",
        "year = '2006'\n",
        "file311 = \"/content/gdrive/MyDrive/Dataset/311_06.csv\"\n",
        "fileCrime = \"/content/gdrive/MyDrive/Dataset/CrimeData.csv\"\n",
        "fileRegions = \"/content/gdrive/MyDrive/Dataset/Regions.csv\"\n",
        "\n",
        "data311 = pd.read_csv(file311,low_memory=False)\n",
        "relevantColumns311 = ['Created Date','Latitude','Longitude','Complaint Type']\n",
        "relevantComplaints311 = ['Blocked Driveway','Building/Use','Noise','Safety']\n",
        "finalColumns311 = ['Created Date','Complaint Type','Precincts']\n",
        "data311 = data311[relevantColumns311]\n",
        "data311 = data311.loc[(data311['Complaint Type'] == 'Blocked Driveway') | (data311['Complaint Type'] == 'Building/Use') | (data311['Complaint Type'] == 'Noise') | (data311['Complaint Type'] == 'Safety')]\n",
        "print('Anomalies preprocessed!!!')\n",
        "\n",
        "crimeData = pd.read_csv(fileCrime, low_memory=False)\n",
        "relevantColumnsCR = ['CMPLNT_TO_DT','CMPLNT_TO_TM','CMPLNT_FR_DT','RPT_DT','Lat_Lon','Latitude','Longitude','OFNS_DESC']\n",
        "finalColumnsCR = ['Date','OFNS_DESC','CMPLNT_TO_TM','Longitude','Latitude','Precincts']\n",
        "relevantCrimesCR = ['ROBBERY', 'BURGLARY', 'FELONY ASSAULT','GRAND LARCENY']\n",
        "locations = pd.read_csv(\"/content/gdrive/MyDrive/Dataset/Regions.csv\", low_memory=False)\n",
        "crime2006 = crimeData.loc[crimeData['RPT_DT'].str.endswith(year)]\n",
        "crime2006 = crime2006[relevantColumnsCR]\n",
        "crime2006 = crime2006.loc[(crime2006['OFNS_DESC'] == 'ROBBERY') | (crime2006['OFNS_DESC'] == 'BURGLARY') | (crime2006['OFNS_DESC'] == 'FELONY ASSAULT') | (crime2006['OFNS_DESC'] == 'GRAND LARCENY')]\n",
        "print('Crime Preprocessed!!!')\n",
        "\n",
        "locations = pd.read_csv(fileRegions, low_memory=False)\n",
        "precincts = {}\n",
        "for index, row in locations.iterrows():\n",
        "  precincts[row['Precinct']] = shapely.wkt.loads(row['the_geom'])\n",
        "\n",
        "print('Locations loaded!!!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomalies preprocessed!!!\n",
            "Crime Preprocessed!!!\n",
            "Locations loaded!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ70JHNMJKpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d13b40-d4c7-4fa3-82e9-bac113ff6dc9"
      },
      "source": [
        "pos = 0\n",
        "prec = np.ndarray((data311.shape[0],))\n",
        "\n",
        "for index,row in data311.iterrows():\n",
        "\n",
        "  poo = Point(row['Longitude'],row['Latitude'])  \n",
        "  for key,val in precincts.items():\n",
        "    if poo.within(val):\n",
        "      prec[pos] = key\n",
        "      break\n",
        "\n",
        "  pos=pos+1\n",
        "  if(pos%1000 == 0):\n",
        "    print (\"processed \"+str(pos)+\" records!!\")\n",
        "data311['Precincts'] = prec.astype(int)\n",
        "print(\"Done!!\")\n",
        "data311['Precincts'] = data311['Precincts'].astype(np.int64)\n",
        "data311 = data311[data311['Precincts'] >= 0]\n",
        "data311 = data311[finalColumns311]\n",
        "\n",
        "print('Anomaly Preprocessing complete!!!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 1000 records!!\n",
            "processed 2000 records!!\n",
            "processed 3000 records!!\n",
            "processed 4000 records!!\n",
            "processed 5000 records!!\n",
            "processed 6000 records!!\n",
            "processed 7000 records!!\n",
            "processed 8000 records!!\n",
            "processed 9000 records!!\n",
            "processed 10000 records!!\n",
            "processed 11000 records!!\n",
            "processed 12000 records!!\n",
            "processed 13000 records!!\n",
            "processed 14000 records!!\n",
            "processed 15000 records!!\n",
            "processed 16000 records!!\n",
            "processed 17000 records!!\n",
            "processed 18000 records!!\n",
            "processed 19000 records!!\n",
            "processed 20000 records!!\n",
            "processed 21000 records!!\n",
            "processed 22000 records!!\n",
            "processed 23000 records!!\n",
            "processed 24000 records!!\n",
            "processed 25000 records!!\n",
            "processed 26000 records!!\n",
            "processed 27000 records!!\n",
            "processed 28000 records!!\n",
            "processed 29000 records!!\n",
            "processed 30000 records!!\n",
            "processed 31000 records!!\n",
            "processed 32000 records!!\n",
            "processed 33000 records!!\n",
            "processed 34000 records!!\n",
            "processed 35000 records!!\n",
            "processed 36000 records!!\n",
            "processed 37000 records!!\n",
            "processed 38000 records!!\n",
            "processed 39000 records!!\n",
            "processed 40000 records!!\n",
            "processed 41000 records!!\n",
            "processed 42000 records!!\n",
            "processed 43000 records!!\n",
            "processed 44000 records!!\n",
            "processed 45000 records!!\n",
            "processed 46000 records!!\n",
            "processed 47000 records!!\n",
            "processed 48000 records!!\n",
            "processed 49000 records!!\n",
            "processed 50000 records!!\n",
            "processed 51000 records!!\n",
            "processed 52000 records!!\n",
            "processed 53000 records!!\n",
            "processed 54000 records!!\n",
            "processed 55000 records!!\n",
            "processed 56000 records!!\n",
            "processed 57000 records!!\n",
            "processed 58000 records!!\n",
            "processed 59000 records!!\n",
            "processed 60000 records!!\n",
            "processed 61000 records!!\n",
            "processed 62000 records!!\n",
            "processed 63000 records!!\n",
            "processed 64000 records!!\n",
            "processed 65000 records!!\n",
            "processed 66000 records!!\n",
            "processed 67000 records!!\n",
            "processed 68000 records!!\n",
            "processed 69000 records!!\n",
            "processed 70000 records!!\n",
            "processed 71000 records!!\n",
            "processed 72000 records!!\n",
            "processed 73000 records!!\n",
            "processed 74000 records!!\n",
            "processed 75000 records!!\n",
            "processed 76000 records!!\n",
            "processed 77000 records!!\n",
            "processed 78000 records!!\n",
            "processed 79000 records!!\n",
            "processed 80000 records!!\n",
            "processed 81000 records!!\n",
            "processed 82000 records!!\n",
            "processed 83000 records!!\n",
            "processed 84000 records!!\n",
            "processed 85000 records!!\n",
            "processed 86000 records!!\n",
            "processed 87000 records!!\n",
            "processed 88000 records!!\n",
            "processed 89000 records!!\n",
            "processed 90000 records!!\n",
            "processed 91000 records!!\n",
            "processed 92000 records!!\n",
            "processed 93000 records!!\n",
            "processed 94000 records!!\n",
            "processed 95000 records!!\n",
            "processed 96000 records!!\n",
            "processed 97000 records!!\n",
            "processed 98000 records!!\n",
            "processed 99000 records!!\n",
            "processed 100000 records!!\n",
            "processed 101000 records!!\n",
            "processed 102000 records!!\n",
            "processed 103000 records!!\n",
            "processed 104000 records!!\n",
            "processed 105000 records!!\n",
            "processed 106000 records!!\n",
            "processed 107000 records!!\n",
            "processed 108000 records!!\n",
            "processed 109000 records!!\n",
            "processed 110000 records!!\n",
            "processed 111000 records!!\n",
            "processed 112000 records!!\n",
            "processed 113000 records!!\n",
            "processed 114000 records!!\n",
            "processed 115000 records!!\n",
            "processed 116000 records!!\n",
            "processed 117000 records!!\n",
            "processed 118000 records!!\n",
            "processed 119000 records!!\n",
            "processed 120000 records!!\n",
            "processed 121000 records!!\n",
            "processed 122000 records!!\n",
            "processed 123000 records!!\n",
            "processed 124000 records!!\n",
            "processed 125000 records!!\n",
            "processed 126000 records!!\n",
            "processed 127000 records!!\n",
            "processed 128000 records!!\n",
            "processed 129000 records!!\n",
            "Done!!\n",
            "Anomaly Preprocessing complete!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdR_e170WZiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303cafc6-ecaa-448e-d847-cb3688e47a22"
      },
      "source": [
        "pos = 0\n",
        "prec = np.ndarray((crime2006.shape[0],))\n",
        "date = np.ndarray((crime2006.shape[0],)).astype(str)\n",
        "\n",
        "for index,row in crime2006.iterrows():\n",
        "  \n",
        "  if not(str(row['CMPLNT_FR_DT']) == 'nan'):\n",
        "    date[pos] = row['CMPLNT_FR_DT']\n",
        "  elif not(str(row['CMPLNT_TO_DT']) == 'nan'):\n",
        "    date[pos] = row['CMPLNT_TO_DT']\n",
        "  else:\n",
        "    date[pos] = row['RPT_DT']\n",
        "  \n",
        "  poo = Point(row['Longitude'],row['Latitude'])  \n",
        "  for key,val in precincts.items():\n",
        "    if poo.within(val):\n",
        "      prec[pos] = key\n",
        "      break\n",
        "\n",
        "  pos=pos+1\n",
        "  if(pos%1000 == 0):\n",
        "    print (\"processed \"+str(pos)+\" records!!\")\n",
        "\n",
        "crime2006['Precincts'] = prec.astype(int)\n",
        "crime2006['Date'] = date\n",
        "crime2006 = crime2006[crime2006['Precincts'] >= 0]\n",
        "\n",
        "print(\"Done!!\")\n",
        "\n",
        "crime2006 = crime2006[finalColumnsCR]\n",
        "crime2006.sort_values(['Precincts','Date','OFNS_DESC'],ascending=[True,True,True],inplace=True)\n",
        "crime2006 = crime2006[crime2006['Date'].str.contains(year)]\n",
        "crime2006['Precincts'] = crime2006['Precincts'].astype(np.int64)\n",
        "\n",
        "print('Crime Preprocessing complete!!!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 1000 records!!\n",
            "processed 2000 records!!\n",
            "processed 3000 records!!\n",
            "processed 4000 records!!\n",
            "processed 5000 records!!\n",
            "processed 6000 records!!\n",
            "processed 7000 records!!\n",
            "processed 8000 records!!\n",
            "processed 9000 records!!\n",
            "processed 10000 records!!\n",
            "processed 11000 records!!\n",
            "processed 12000 records!!\n",
            "processed 13000 records!!\n",
            "processed 14000 records!!\n",
            "processed 15000 records!!\n",
            "processed 16000 records!!\n",
            "processed 17000 records!!\n",
            "processed 18000 records!!\n",
            "processed 19000 records!!\n",
            "processed 20000 records!!\n",
            "processed 21000 records!!\n",
            "processed 22000 records!!\n",
            "processed 23000 records!!\n",
            "processed 24000 records!!\n",
            "processed 25000 records!!\n",
            "processed 26000 records!!\n",
            "processed 27000 records!!\n",
            "processed 28000 records!!\n",
            "processed 29000 records!!\n",
            "processed 30000 records!!\n",
            "processed 31000 records!!\n",
            "processed 32000 records!!\n",
            "processed 33000 records!!\n",
            "processed 34000 records!!\n",
            "processed 35000 records!!\n",
            "processed 36000 records!!\n",
            "processed 37000 records!!\n",
            "processed 38000 records!!\n",
            "processed 39000 records!!\n",
            "processed 40000 records!!\n",
            "processed 41000 records!!\n",
            "processed 42000 records!!\n",
            "processed 43000 records!!\n",
            "processed 44000 records!!\n",
            "processed 45000 records!!\n",
            "processed 46000 records!!\n",
            "processed 47000 records!!\n",
            "processed 48000 records!!\n",
            "processed 49000 records!!\n",
            "processed 50000 records!!\n",
            "processed 51000 records!!\n",
            "processed 52000 records!!\n",
            "processed 53000 records!!\n",
            "processed 54000 records!!\n",
            "processed 55000 records!!\n",
            "processed 56000 records!!\n",
            "processed 57000 records!!\n",
            "processed 58000 records!!\n",
            "processed 59000 records!!\n",
            "processed 60000 records!!\n",
            "processed 61000 records!!\n",
            "processed 62000 records!!\n",
            "processed 63000 records!!\n",
            "processed 64000 records!!\n",
            "processed 65000 records!!\n",
            "processed 66000 records!!\n",
            "processed 67000 records!!\n",
            "processed 68000 records!!\n",
            "processed 69000 records!!\n",
            "processed 70000 records!!\n",
            "processed 71000 records!!\n",
            "processed 72000 records!!\n",
            "processed 73000 records!!\n",
            "processed 74000 records!!\n",
            "processed 75000 records!!\n",
            "processed 76000 records!!\n",
            "processed 77000 records!!\n",
            "processed 78000 records!!\n",
            "processed 79000 records!!\n",
            "processed 80000 records!!\n",
            "processed 81000 records!!\n",
            "processed 82000 records!!\n",
            "processed 83000 records!!\n",
            "processed 84000 records!!\n",
            "processed 85000 records!!\n",
            "processed 86000 records!!\n",
            "processed 87000 records!!\n",
            "processed 88000 records!!\n",
            "processed 89000 records!!\n",
            "processed 90000 records!!\n",
            "processed 91000 records!!\n",
            "processed 92000 records!!\n",
            "processed 93000 records!!\n",
            "processed 94000 records!!\n",
            "processed 95000 records!!\n",
            "processed 96000 records!!\n",
            "processed 97000 records!!\n",
            "processed 98000 records!!\n",
            "processed 99000 records!!\n",
            "processed 100000 records!!\n",
            "processed 101000 records!!\n",
            "processed 102000 records!!\n",
            "processed 103000 records!!\n",
            "processed 104000 records!!\n",
            "processed 105000 records!!\n",
            "processed 106000 records!!\n",
            "processed 107000 records!!\n",
            "processed 108000 records!!\n",
            "processed 109000 records!!\n",
            "Done!!\n",
            "Crime Preprocessing complete!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWknfNTtU9Dt"
      },
      "source": [
        "daterange = pd.date_range(start='01/01/'+year,end='12/31/'+year)\n",
        "daterange = daterange.strftime('%m/%d/%Y')\n",
        "daterange = daterange.tolist()\n",
        "\n",
        "inv_dict = {}\n",
        "for i in range(len(daterange)):\n",
        "  inv_dict[daterange[i]] = i\n",
        "\n",
        "inv_complaints = {}\n",
        "for i in range(len(relevantComplaints311)):\n",
        "  inv_complaints[relevantComplaints311[i]] = i\n",
        "\n",
        "n_precincts = data311['Precincts'].nunique()\n",
        "uniq_precincts = data311['Precincts'].unique()\n",
        "inv_prec = {}\n",
        "for i in range(n_precincts):\n",
        "  inv_prec[uniq_precincts[i]] = i\n",
        "\n",
        "inv_crimes = {}\n",
        "for i in range(len(relevantCrimesCR)):\n",
        "  inv_crimes[relevantCrimesCR[i]] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2ec1hJVWxXO",
        "outputId": "ff4d2a75-11c8-4d83-d90a-c1281d97d088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "matricesCR = [np.zeros((n_precincts,4),dtype=np.int64) for x in range(365)]\n",
        "exceptions = 0\n",
        "\n",
        "for idx, row in crime2006.iterrows():\n",
        "  try:\n",
        "    id1 = inv_prec[row['Precincts']]\n",
        "    id2 = inv_dict[row['Date']]\n",
        "    id3 = inv_crimes[row['OFNS_DESC']]\n",
        "    matricesCR[id2][id1][id3] = matricesCR[id2][id1][id3] + 1\n",
        "  except:\n",
        "    print(\"Exception!!!\")\n",
        "    print(\"Precincts\",id1)\n",
        "    print(\"Date\",id2)\n",
        "    print(\"Offense\",id3)\n",
        "    exceptions = exceptions + 1\n",
        "\n",
        "print ('Created crime matrices!!!')\n",
        "\n",
        "\n",
        "matrices311 = [np.zeros((n_precincts,4),dtype=np.int64) for x in range(365)]\n",
        "exceptions = 0\n",
        "\n",
        "for idx, row in data311.iterrows():\n",
        "  try:\n",
        "    id1 = inv_prec[row['Precincts']]\n",
        "    id2 = inv_dict[str(row['Created Date']).split()[0]]\n",
        "    id3 = inv_complaints[row['Complaint Type']]\n",
        "    matrices311[id2][id1][id3] = matrices311[id2][id1][id3] + 1\n",
        "  except:\n",
        "    print(\"Exception!!!\")\n",
        "    print(\"Precincts\",id1)\n",
        "    print(\"Date\",id2)\n",
        "    print(\"Offense\",id3)\n",
        "    exceptions = exceptions + 1\n",
        "\n",
        "print('Created anomaly matrices!!!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created crime matrices!!!\n",
            "Created anomaly matrices!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47hlZc9naCK9"
      },
      "source": [
        "import pickle\n",
        "output311 = 'matrices311'\n",
        "outputCrime = 'matricesCR'\n",
        "\n",
        "file311 = open(output311,'wb')\n",
        "pickle.dump(matrices311,file311)\n",
        "fileCR = open(outputCrime,'wb')\n",
        "pickle.dump(matricesCR,fileCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shapely.wkt\n",
        "from shapely.geometry import Point, Polygon, MultiPolygon\n",
        "import re\n",
        "\n",
        "filePOI = '/content/gdrive/MyDrive/Dataset/Point_Of_Interest.csv'\n",
        "fileRegions = \"/content/gdrive/MyDrive/Dataset/Regions.csv\"\n",
        "\n",
        "poi = pd.read_csv(filePOI,low_memory=False)\n",
        "locations = pd.read_csv(fileRegions, low_memory=False)\n",
        "\n",
        "relevantColumns = ['the_geom','CREATED','FACILITY_T']\n",
        "finalColumns = ['Precincts','FACILITY_T']\n",
        "poi = poi[relevantColumns]\n",
        "precincts = {}\n",
        "for index, row in locations.iterrows():\n",
        "  precincts[row['Precinct']] = shapely.wkt.loads(row['the_geom'])"
      ],
      "metadata": {
        "id": "YOI4_VNDmnRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lat = []\n",
        "longt = []\n",
        "date = []\n",
        "\n",
        "for idx,row in poi.iterrows():\n",
        "  location = re.split(r' |\\)|\\(' ,row['the_geom'])\n",
        "  year = int(row['CREATED'].split()[0].split('/')[-1])\n",
        "  lat = lat + [location[-2]]\n",
        "  longt = longt + [location[2]]\n",
        "  date = date + [year]\n",
        "\n",
        "poi['Year'] = date\n",
        "poi['Latitude'] = lat\n",
        "poi['Longitude'] = longt\n",
        "pos = 0\n",
        "prec = np.ndarray((poi.shape[0],))\n",
        "\n",
        "for index,row in poi.iterrows():\n",
        "\n",
        "  poo = Point(float(row['Longitude']),float(row['Latitude']))  \n",
        "  for key,val in precincts.items():\n",
        "    if poo.within(val):\n",
        "      prec[pos] = key\n",
        "      break\n",
        "\n",
        "  pos=pos+1\n",
        "  if(pos%1000 == 0):\n",
        "    print (\"processed \"+str(pos)+\" records!!\")\n",
        "    \n",
        "poi['Precincts'] = prec.astype(int)\n",
        "print(\"Done!!\")\n",
        "\n",
        "poi['Precincts'] = poi['Precincts'].astype(np.int64)\n",
        "poi = poi[poi['Precincts'] >= 0]\n",
        "inv_categories = {}\n",
        "n_categories = poi['FACILITY_T'].nunique()\n",
        "uniq_categories = poi['FACILITY_T'].unique()\n",
        "for i in range(n_categories):\n",
        "  inv_categories[uniq_categories[i]] = i\n",
        "\n",
        "n_precincts = poi['Precincts'].nunique()\n",
        "uniq_precincts = poi['Precincts'].unique()\n",
        "inv_prec = {}\n",
        "for i in range(n_precincts):\n",
        "  inv_prec[uniq_precincts[i]] = i\n",
        "\n",
        "poi = poi[poi['Year'] <= 2008]\n",
        "poi = poi[finalColumns]"
      ],
      "metadata": {
        "id": "ldALKjszoLEK",
        "outputId": "9d0629ac-0cbc-49c5-a56c-6588aec0aa55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 1000 records!!\n",
            "processed 2000 records!!\n",
            "processed 3000 records!!\n",
            "processed 4000 records!!\n",
            "processed 5000 records!!\n",
            "processed 6000 records!!\n",
            "processed 7000 records!!\n",
            "processed 8000 records!!\n",
            "processed 9000 records!!\n",
            "processed 10000 records!!\n",
            "processed 11000 records!!\n",
            "processed 12000 records!!\n",
            "processed 13000 records!!\n",
            "processed 14000 records!!\n",
            "processed 15000 records!!\n",
            "processed 16000 records!!\n",
            "processed 17000 records!!\n",
            "processed 18000 records!!\n",
            "processed 19000 records!!\n",
            "processed 20000 records!!\n",
            "Done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrices = np.zeros((n_precincts,n_categories),dtype=np.int64)\n",
        "exceptions = 0\n",
        "\n",
        "for idx, row in poi.iterrows():\n",
        "  try:\n",
        "    id1 = inv_prec[row['Precincts']]\n",
        "    id2 = inv_categories[row['FACILITY_T']]\n",
        "    matrices[id1][id2]= matrices[id1][id2] + 1\n",
        "  except:\n",
        "    print(\"Exception!!!\")\n",
        "    print(\"Precincts\",id1)\n",
        "    print(\"FACILITY_T\",id2)\n",
        "    exceptions = exceptions + 1"
      ],
      "metadata": {
        "id": "ohudjI1bodZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "outputPOI = 'poiMatrices'\n",
        "file = open(outputPOI,'wb')\n",
        "pickle.dump(matrices,file)\n"
      ],
      "metadata": {
        "id": "pbQgI7mloi3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.13.2"
      ],
      "metadata": {
        "id": "jtG2jrKqpq9J",
        "outputId": "39d9c402-8096-4b59-fd2a-d9c47ddc6661",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.21.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.44.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.5.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.0.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.2.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "from tensorflow.contrib.rnn import BasicLSTMCell\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ud9bUrwQou5H",
        "outputId": "2441519b-71cf-4fa1-f255-f1e13d91f31b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input311File = 'matrices311'\n",
        "inputCrimeFile = 'matricesCR'\n",
        "\n",
        "with open(inputCrimeFile, 'rb') as pickle_file:\n",
        "    anomaly = pickle.load(pickle_file)\n",
        "\n",
        "with open(inputCrimeFile,'rb') as pickle_file:\n",
        "    content = pickle.load(pickle_file)"
      ],
      "metadata": {
        "id": "RXUEI2_-rGdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat = []\n",
        "dat2 = []\n",
        "for i in range(len(content)):\n",
        "  a = []\n",
        "  b = []\n",
        "  for j in range(77):\n",
        "    a.extend(content[i][j+1])\n",
        "    b.extend(anomaly[i][j+1])\n",
        "    # print(a)\n",
        "  dat.append(a)\n",
        "  dat2.append(b)\n",
        "\n",
        "inp = np.array(dat)\n",
        "inp1 = np.where(inp>0,1,0)\n",
        "inp = np.array(dat)\n",
        "inp1 = np.where(inp>0,1,0)\n",
        "inpA = np.array(dat2)\n",
        "inpA = np.array(dat2)"
      ],
      "metadata": {
        "id": "Z03J6VMcrO1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = int(len(inp)*0.8)\n",
        "x_train = inp[:size]\n",
        "y_train = inp1[:size]\n",
        "x_test = inp[size:]\n",
        "y_test = inp1[size:]\n",
        "x_train2 = inpA[:size]\n",
        "x_test2 = inpA[size:]"
      ],
      "metadata": {
        "id": "zjnhDhLNrvk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
        "  \n",
        "    if isinstance(inputs, tuple):\n",
        "        inputs = tf.concat(inputs, 2)\n",
        "\n",
        "    if time_major:\n",
        "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
        "\n",
        "    hidden_size = inputs.shape[2].value \n",
        "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
        "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "\n",
        "    with tf.name_scope('v'):\n",
        "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
        "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
        "    alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
        "\n",
        "    if not return_alphas:\n",
        "        return output\n",
        "    else:\n",
        "        return output, alphas"
      ],
      "metadata": {
        "id": "uYW-lEaCBPQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(x_test, y_test, dev_ratio):\n",
        "    test_size = len(x_test)\n",
        "    print(test_size)\n",
        "    dev_size = (int)(test_size * dev_ratio)\n",
        "    print(dev_size)\n",
        "    x_dev = x_test[:dev_size]\n",
        "    x_test = x_test[dev_size:]\n",
        "    y_dev = y_test[:dev_size]\n",
        "    y_test = y_test[dev_size:]\n",
        "    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n",
        "\n",
        "\n",
        "def fill_feed_dict(data_X, data_Y, batch_size):\n",
        "    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n",
        "    for idx in range(data_X.shape[0] // batch_size):\n",
        "        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n",
        "        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n",
        "        yield x_batch, y_batch"
      ],
      "metadata": {
        "id": "EffJpGsKBe7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DOCUMENT_LENGTH = 128\n",
        "EMBEDDING_SIZE = 128\n",
        "HIDDEN_SIZE = 64\n",
        "ATTENTION_SIZE = 64\n",
        "lr = 5e-4\n",
        "learning_rate=0.001\n",
        "hidden_dim = 250\n",
        "BATCH_SIZE = 4\n",
        "KEEP_PROB = 1.0\n",
        "LAMBDA = 0.0001\n",
        "MAX_LABEL = 77*4\n",
        "epochs = 10\n",
        "latent_dim = 8\n",
        "timeSize = 10\n",
        "max_len=10"
      ],
      "metadata": {
        "id": "XFX7zW3wBna-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_label_hot(prediction, threshold=0.5):\n",
        "    prediction = tf.cast(prediction, tf.float32)\n",
        "    threshold = float(threshold)\n",
        "    return tf.cast(tf.greater(prediction, threshold), tf.int64)\n",
        "\n",
        "def get_metrics(labels_tensor, one_hot_prediction, num_classes):\n",
        "    metrics = {}\n",
        "    with tf.variable_scope(\"metrics\"):\n",
        "        for scope in [\"train\", \"val\"]:\n",
        "            with tf.variable_scope(scope):\n",
        "                with tf.variable_scope(\"accuracy\"):\n",
        "                    accuracy, accuracy_update = tf.metrics.accuracy(\n",
        "                        tf.cast(one_hot_prediction, tf.int32),\n",
        "                        labels_tensor,\n",
        "                    )\n",
        "                metrics[scope] = {\n",
        "                    \"accuracy\": accuracy,\n",
        "                    \"updates\": tf.group(accuracy_update),\n",
        "                }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Sej71Q_NBrdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "batch_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "anomaly_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "rnn_outputs1, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=batch_x, dtype=tf.float32,scope='BLSTM_1')\n",
        "fw_outputs1, bw_outputs1 = rnn_outputs1\n",
        "\n",
        "rnn_outputs2, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=anomaly_x, dtype=tf.float32,scope='BLSTM_2')\n",
        "fw_outputs2, bw_outputs2 = rnn_outputs2\n",
        "weight_out = tf.Variable(tf.truncated_normal([4], stddev=0.1))\n",
        "weight_soft = tf.nn.softmax(weight_out)\n",
        "\n",
        "inputAdd = weight_soft[0]*fw_outputs1 + weight_soft[1]**fw_outputs2 + weight_soft[2]*bw_outputs1 + weight_soft[3]*bw_outputs2\n",
        "print(batch_x.shape)\n",
        "print(inputAdd.shape)\n",
        "rnn_outputs, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=inputAdd, dtype=tf.float32,scope='BLSTM_3')\n",
        "fw_outputs, bw_outputs = rnn_outputs\n",
        "W = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=0.1))\n",
        "H = fw_outputs + bw_outputs  \n",
        "M = tf.tanh(H)  \n",
        "\n",
        "alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, HIDDEN_SIZE]),\n",
        "                                                tf.reshape(W, [-1, 1])),\n",
        "                                      (-1, timeSize ))) \n",
        "\n",
        "print(alpha.shape)\n",
        "r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
        "              tf.reshape(alpha, [-1, timeSize, 1]))\n",
        "r = tf.squeeze(r)\n",
        "h_star = tf.tanh(r) \n",
        "\n",
        "h_drop = tf.nn.dropout(h_star, keep_prob)\n",
        "shape = h_drop.get_shape()\n",
        "\n",
        "FC_W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MAX_LABEL], stddev=0.1))\n",
        "FC_b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat2 = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
        "print(y_hat2.shape)\n",
        "FC_W2 = tf.Variable(tf.truncated_normal([MAX_LABEL, MAX_LABEL], stddev=0.1))\n",
        "FC_b2 = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat = tf.nn.xw_plus_b(y_hat2, FC_W2, FC_b2)\n",
        "\n",
        "loss =  tf.nn.l2_loss(y_hat-batch_y) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "\n",
        "prediction = tf.sigmoid(y_hat)\n",
        "one_hot_prediction = multi_label_hot(prediction)\n",
        "\n",
        "accuracy  =  get_metrics(batch_y,one_hot_prediction,77)"
      ],
      "metadata": {
        "id": "GvkP7aT5BvER",
        "outputId": "64340fc9-f7b3-4fc8-cd16-720c35719bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-8e55326b91e7>:7: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-23-8e55326b91e7>:9: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "(?, 10, 308)\n",
            "(?, 10, 64)\n",
            "(?, 10)\n",
            "WARNING:tensorflow:From <ipython-input-23-8e55326b91e7>:40: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "(?, 308)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "!mkdir checkpointDir\n",
        "slim = tf.contrib.slim\n",
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.local_variables_initializer())\n",
        "def model_summary():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "    \n",
        "model_summary()\n",
        "tr = []\n",
        "ts = []"
      ],
      "metadata": {
        "id": "14BRWyfHB_it",
        "outputId": "049d3be6-f989-497d-dc0e-d0c53ce095c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "BLSTM_1/fw/basic_lstm_cell/kernel:0 (float32_ref 372x256) [95232, bytes: 380928]\n",
            "BLSTM_1/fw/basic_lstm_cell/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "BLSTM_1/bw/basic_lstm_cell/kernel:0 (float32_ref 372x256) [95232, bytes: 380928]\n",
            "BLSTM_1/bw/basic_lstm_cell/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "BLSTM_2/fw/basic_lstm_cell/kernel:0 (float32_ref 372x256) [95232, bytes: 380928]\n",
            "BLSTM_2/fw/basic_lstm_cell/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "BLSTM_2/bw/basic_lstm_cell/kernel:0 (float32_ref 372x256) [95232, bytes: 380928]\n",
            "BLSTM_2/bw/basic_lstm_cell/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "Variable:0 (float32_ref 4) [4, bytes: 16]\n",
            "BLSTM_3/fw/basic_lstm_cell/kernel:0 (float32_ref 128x256) [32768, bytes: 131072]\n",
            "BLSTM_3/fw/basic_lstm_cell/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "BLSTM_3/bw/basic_lstm_cell/kernel:0 (float32_ref 128x256) [32768, bytes: 131072]\n",
            "BLSTM_3/bw/basic_lstm_cell/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "Variable_1:0 (float32_ref 64) [64, bytes: 256]\n",
            "Variable_2:0 (float32_ref 64x308) [19712, bytes: 78848]\n",
            "Variable_3:0 (float32_ref 308) [308, bytes: 1232]\n",
            "Variable_4:0 (float32_ref 308x308) [94864, bytes: 379456]\n",
            "Variable_5:0 (float32_ref 308) [308, bytes: 1232]\n",
            "Total size of variables: 563260\n",
            "Total bytes of variables: 2253040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initialized! \")\n",
        "target_names = ['a','b','c','d']\n",
        "print(\"Start trainning\")\n",
        "start = time.time()\n",
        "testA = 0\n",
        "predsAr = []\n",
        "val_loss=[]\n",
        "train_loss=[]\n",
        "for e in range(500):\n",
        "    train_loss.append(e)\n",
        "    val_loss.append(e)\n",
        "    epoch_start = time.time()\n",
        "    print(\"Epoch %d start !\" % (e + 1))\n",
        "    err = []\n",
        "    preds = []\n",
        "    trues = []\n",
        "\n",
        "    x_batch1 =[]\n",
        "    x_batch2 = []\n",
        "    y_batch1 = []\n",
        "    \n",
        "    for i in range(len(x_train)-80):\n",
        "        i+=80\n",
        "        x_batch = x_train[i:min(len(x_train)-1,timeSize+(i))]\n",
        "        x_anomaly = x_train2[i:min(len(x_train)-1,timeSize+(i))]\n",
        "        if len(x_batch) < timeSize:\n",
        "          continue\n",
        "        x_batch = x_batch\n",
        "        x_anomaly = x_anomaly\n",
        "        y_batch = x_train[min(len(x_train)-1,timeSize+(i))].T\n",
        "        x_batch1.append(x_batch)\n",
        "        x_batch2.append(x_anomaly)\n",
        "        y_batch1.append(y_batch)\n",
        "        if (i+1)% BATCH_SIZE >0:\n",
        "          continue\n",
        "        fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "      \n",
        "        l, _, oht = sess.run([loss, optimizer, one_hot_prediction], feed_dict=fd)\n",
        "        for j in range(BATCH_SIZE):\n",
        "          preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "          trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "        x_batch1 =[]\n",
        "        y_batch1 = []\n",
        "        x_batch2 = []\n",
        "\n",
        "        err.append(l)\n",
        "   \n",
        "    epoch_finish = time.time()\n",
        "    \n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "    \n",
        "    f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "    f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "    tr.append([f1,f2])\n",
        "   \n",
        "    print(\"Training  :: loss = \",np.mean(err),\" : micro (f1) = \",f1,\" : macro (f2) = \",f2,\" : Epoch_runtime = \",epoch_finish-epoch_start,\" \\n\")\n",
        "    train_loss[e]=np.mean(err)\n",
        "    print(\"train_loss = \",train_loss[e])\n",
        "\n",
        "    if True:\n",
        "      preds = []\n",
        "      trues = []\n",
        "      x_batch1 =[]\n",
        "      y_batch1 = []\n",
        "      x_batch2 = []\n",
        "      err = []\n",
        "      for i in range(len(x_test)):\n",
        "          x_batch = x_test[i:min(len(x_test)-1,timeSize+(i))]\n",
        "          x_anomaly = x_test2[i:min(len(x_test)-1,timeSize+(i))]\n",
        "          if len(x_batch) < timeSize:\n",
        "            continue\n",
        "          x_batch = x_batch\n",
        "          x_anomaly = x_anomaly\n",
        "          y_batch = x_test[min(len(x_test)-1,timeSize+(i))].T\n",
        "          x_batch1.append(x_batch)\n",
        "          x_batch2.append(x_anomaly)\n",
        "          y_batch1.append(y_batch)\n",
        "          if (i+1)% BATCH_SIZE >0:\n",
        "            continue\n",
        "          fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "          l, acc,oht,weightSupport = sess.run([loss, accuracy,one_hot_prediction,weight_soft], feed_dict=fd)\n",
        "          err.append(l)\n",
        "          \n",
        "          for j in range(BATCH_SIZE):\n",
        "            preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "            trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "\n",
        "          x_batch1 =[]\n",
        "          y_batch1 = []\n",
        "          x_batch2 = []\n",
        "      preds = np.array(preds)\n",
        "      trues = np.array(trues)  \n",
        "      f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "      f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "      if testA < f1:\n",
        "        testA=f1\n",
        "        save_path = saver.save(sess, \"./modelM/model\"+str(f1)[:5]+\".ckpt\")\n",
        "      \n",
        "        predsAr.append(preds)\n",
        "      ts.append([f1,f2])\n",
        "      print(\"Validation  :: loss = \",np.mean(err),\" : micro (f1) = \",f1,\" : macro (f2) = \",f2,\" : Epoch_runtime = \",epoch_finish-epoch_start,\" \\n\")\n",
        "      train_loss[e]=np.mean(err)\n",
        "      print(\"val_loss = \",train_loss[e])\n",
        "     \n",
        "      #print(np.mean(err),\" : micro \",f1,\" : macro\",f2,\" \\n\")\n",
        "      print(\"Weight Support = \",weightSupport)\n",
        "      print(\"\\n================================================================\\n\")\n",
        "      "
      ],
      "metadata": {
        "id": "CRI9VPdFCDXo",
        "outputId": "f53c53c1-3f0b-4747-a588-bab6c8bd1bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "val_loss =  948.67035\n",
            "Weight Support =  [0.5464833  0.08107312 0.23062794 0.14181565]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 47 start !\n",
            "Training  :: loss =  211.024  : micro (f1) =  0.7342544873977203  : macro (f2) =  0.7265243492027604  : Epoch_runtime =  1.7286913394927979  \n",
            "\n",
            "train_loss =  211.024\n",
            "Validation  :: loss =  946.6112  : micro (f1) =  0.6797062350119903  : macro (f2) =  0.6689654545948248  : Epoch_runtime =  1.7286913394927979  \n",
            "\n",
            "val_loss =  946.6112\n",
            "Weight Support =  [0.5471557  0.08122645 0.23022033 0.14139755]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 48 start !\n",
            "Training  :: loss =  210.77142  : micro (f1) =  0.7335047129391602  : macro (f2) =  0.7257799871004931  : Epoch_runtime =  1.7732036113739014  \n",
            "\n",
            "train_loss =  210.77142\n",
            "Validation  :: loss =  1002.28064  : micro (f1) =  0.683086053412463  : macro (f2) =  0.6726712132410013  : Epoch_runtime =  1.7732036113739014  \n",
            "\n",
            "val_loss =  1002.28064\n",
            "Weight Support =  [0.54824907 0.08104065 0.22997102 0.14073925]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 49 start !\n",
            "Training  :: loss =  210.36208  : micro (f1) =  0.7341611550344797  : macro (f2) =  0.7263714497705882  : Epoch_runtime =  1.7402994632720947  \n",
            "\n",
            "train_loss =  210.36208\n",
            "Validation  :: loss =  986.87604  : micro (f1) =  0.6840140454629459  : macro (f2) =  0.6738078718453201  : Epoch_runtime =  1.7402994632720947  \n",
            "\n",
            "val_loss =  986.87604\n",
            "Weight Support =  [0.5488276  0.08096463 0.22988705 0.14032066]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 50 start !\n",
            "Training  :: loss =  211.72253  : micro (f1) =  0.733380265164848  : macro (f2) =  0.7255918700626022  : Epoch_runtime =  1.7649290561676025  \n",
            "\n",
            "train_loss =  211.72253\n",
            "Validation  :: loss =  995.4767  : micro (f1) =  0.6825580096092965  : macro (f2) =  0.6720024642452549  : Epoch_runtime =  1.7649290561676025  \n",
            "\n",
            "val_loss =  995.4767\n",
            "Weight Support =  [0.54951495 0.08060759 0.23056053 0.13931689]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 51 start !\n",
            "Training  :: loss =  210.14998  : micro (f1) =  0.7329973039502988  : macro (f2) =  0.7252658256440164  : Epoch_runtime =  1.7434403896331787  \n",
            "\n",
            "train_loss =  210.14998\n",
            "Validation  :: loss =  984.2942  : micro (f1) =  0.6805643923799544  : macro (f2) =  0.669685364344966  : Epoch_runtime =  1.7434403896331787  \n",
            "\n",
            "val_loss =  984.2942\n",
            "Weight Support =  [0.5497557  0.08092489 0.23061684 0.13870259]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 52 start !\n",
            "Training  :: loss =  210.11372  : micro (f1) =  0.7333403688981133  : macro (f2) =  0.7256435031322848  : Epoch_runtime =  1.7673556804656982  \n",
            "\n",
            "train_loss =  210.11372\n",
            "Validation  :: loss =  967.9801  : micro (f1) =  0.6783422661734733  : macro (f2) =  0.6674161727050676  : Epoch_runtime =  1.7673556804656982  \n",
            "\n",
            "val_loss =  967.9801\n",
            "Weight Support =  [0.55045325 0.0808715  0.23039533 0.1382799 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 53 start !\n",
            "Training  :: loss =  210.95761  : micro (f1) =  0.7330653007161863  : macro (f2) =  0.7253379854240349  : Epoch_runtime =  1.7649383544921875  \n",
            "\n",
            "train_loss =  210.95761\n",
            "Validation  :: loss =  941.8581  : micro (f1) =  0.678269405013591  : macro (f2) =  0.6673797099990953  : Epoch_runtime =  1.7649383544921875  \n",
            "\n",
            "val_loss =  941.8581\n",
            "Weight Support =  [0.5511372  0.0811226  0.2296624  0.13807774]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 54 start !\n",
            "Training  :: loss =  210.34207  : micro (f1) =  0.7347490075403443  : macro (f2) =  0.7270238747211695  : Epoch_runtime =  1.7870147228240967  \n",
            "\n",
            "train_loss =  210.34207\n",
            "Validation  :: loss =  940.9737  : micro (f1) =  0.6807036046555425  : macro (f2) =  0.6702112726191924  : Epoch_runtime =  1.7870147228240967  \n",
            "\n",
            "val_loss =  940.9737\n",
            "Weight Support =  [0.5520447  0.0813548  0.22901963 0.13758093]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 55 start !\n",
            "Training  :: loss =  210.58452  : micro (f1) =  0.734515977443609  : macro (f2) =  0.7267079010135394  : Epoch_runtime =  1.7683818340301514  \n",
            "\n",
            "train_loss =  210.58452\n",
            "Validation  :: loss =  963.14453  : micro (f1) =  0.6805716204619231  : macro (f2) =  0.6702985723310255  : Epoch_runtime =  1.7683818340301514  \n",
            "\n",
            "val_loss =  963.14453\n",
            "Weight Support =  [0.5522899  0.08122038 0.228859   0.13763076]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 56 start !\n",
            "Training  :: loss =  207.7214  : micro (f1) =  0.7350479593755876  : macro (f2) =  0.7272952295902556  : Epoch_runtime =  1.725642204284668  \n",
            "\n",
            "train_loss =  207.7214\n",
            "Validation  :: loss =  1000.5485  : micro (f1) =  0.681172920004471  : macro (f2) =  0.6706509472102213  : Epoch_runtime =  1.725642204284668  \n",
            "\n",
            "val_loss =  1000.5485\n",
            "Weight Support =  [0.55360836 0.08088135 0.22866316 0.13684708]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 57 start !\n",
            "Training  :: loss =  206.47066  : micro (f1) =  0.7340455505987322  : macro (f2) =  0.726272543575564  : Epoch_runtime =  1.7987520694732666  \n",
            "\n",
            "train_loss =  206.47066\n",
            "Validation  :: loss =  999.8284  : micro (f1) =  0.6840605455016469  : macro (f2) =  0.6735441363689747  : Epoch_runtime =  1.7987520694732666  \n",
            "\n",
            "val_loss =  999.8284\n",
            "Weight Support =  [0.5534835  0.08124983 0.22847311 0.13679355]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 58 start !\n",
            "Training  :: loss =  207.67648  : micro (f1) =  0.7347993609623155  : macro (f2) =  0.7270435694099278  : Epoch_runtime =  1.7727272510528564  \n",
            "\n",
            "train_loss =  207.67648\n",
            "Validation  :: loss =  972.393  : micro (f1) =  0.6821728468455078  : macro (f2) =  0.6717762599152112  : Epoch_runtime =  1.7727272510528564  \n",
            "\n",
            "val_loss =  972.393\n",
            "Weight Support =  [0.5543353  0.08104809 0.22838907 0.13622753]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 59 start !\n",
            "Training  :: loss =  207.35721  : micro (f1) =  0.7319774725725058  : macro (f2) =  0.7242501726924704  : Epoch_runtime =  1.8195595741271973  \n",
            "\n",
            "train_loss =  207.35721\n",
            "Validation  :: loss =  953.27386  : micro (f1) =  0.6825076589703355  : macro (f2) =  0.6716611628895733  : Epoch_runtime =  1.8195595741271973  \n",
            "\n",
            "val_loss =  953.27386\n",
            "Weight Support =  [0.5550092  0.08104888 0.22844936 0.13549255]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 60 start !\n",
            "Training  :: loss =  207.57812  : micro (f1) =  0.7345563019683374  : macro (f2) =  0.7268657177123318  : Epoch_runtime =  1.7672359943389893  \n",
            "\n",
            "train_loss =  207.57812\n",
            "Validation  :: loss =  968.03613  : micro (f1) =  0.6814419024683925  : macro (f2) =  0.6705852591879379  : Epoch_runtime =  1.7672359943389893  \n",
            "\n",
            "val_loss =  968.03613\n",
            "Weight Support =  [0.5550179  0.08115008 0.22905949 0.13477254]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 61 start !\n",
            "Training  :: loss =  207.5817  : micro (f1) =  0.7333599136920118  : macro (f2) =  0.7256210814308478  : Epoch_runtime =  1.7273094654083252  \n",
            "\n",
            "train_loss =  207.5817\n",
            "Validation  :: loss =  952.14417  : micro (f1) =  0.6783298696391461  : macro (f2) =  0.6678992252882885  : Epoch_runtime =  1.7273094654083252  \n",
            "\n",
            "val_loss =  952.14417\n",
            "Weight Support =  [0.5550421  0.08142947 0.22892179 0.13460664]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 62 start !\n",
            "Training  :: loss =  208.41412  : micro (f1) =  0.7356178559668844  : macro (f2) =  0.7278212048808003  : Epoch_runtime =  1.75950288772583  \n",
            "\n",
            "train_loss =  208.41412\n",
            "Validation  :: loss =  972.2784  : micro (f1) =  0.6746676794530954  : macro (f2) =  0.6633016662290732  : Epoch_runtime =  1.75950288772583  \n",
            "\n",
            "val_loss =  972.2784\n",
            "Weight Support =  [0.55600995 0.08192673 0.22778724 0.13427606]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 63 start !\n",
            "Training  :: loss =  208.34746  : micro (f1) =  0.7345835683399136  : macro (f2) =  0.7268349840233929  : Epoch_runtime =  1.742980718612671  \n",
            "\n",
            "train_loss =  208.34746\n",
            "Validation  :: loss =  971.0082  : micro (f1) =  0.67953493622305  : macro (f2) =  0.6686314671018815  : Epoch_runtime =  1.742980718612671  \n",
            "\n",
            "val_loss =  971.0082\n",
            "Weight Support =  [0.557264   0.08175871 0.22705829 0.133919  ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 64 start !\n",
            "Training  :: loss =  207.39474  : micro (f1) =  0.7348877679400803  : macro (f2) =  0.7270744368519738  : Epoch_runtime =  1.7684497833251953  \n",
            "\n",
            "train_loss =  207.39474\n",
            "Validation  :: loss =  1009.7799  : micro (f1) =  0.6823363410196428  : macro (f2) =  0.6721099104033238  : Epoch_runtime =  1.7684497833251953  \n",
            "\n",
            "val_loss =  1009.7799\n",
            "Weight Support =  [0.5591064  0.08122531 0.225996   0.13367228]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 65 start !\n",
            "Training  :: loss =  208.40648  : micro (f1) =  0.7340829250525772  : macro (f2) =  0.7263328445554121  : Epoch_runtime =  1.7831146717071533  \n",
            "\n",
            "train_loss =  208.40648\n",
            "Validation  :: loss =  992.95026  : micro (f1) =  0.6842202037289008  : macro (f2) =  0.6741581687058312  : Epoch_runtime =  1.7831146717071533  \n",
            "\n",
            "val_loss =  992.95026\n",
            "Weight Support =  [0.55962247 0.08124756 0.22606802 0.13306196]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 66 start !\n",
            "Training  :: loss =  208.69218  : micro (f1) =  0.735050370875407  : macro (f2) =  0.7272995863986871  : Epoch_runtime =  1.7467341423034668  \n",
            "\n",
            "train_loss =  208.69218\n",
            "Validation  :: loss =  963.91187  : micro (f1) =  0.6832619293839564  : macro (f2) =  0.6735568486564071  : Epoch_runtime =  1.7467341423034668  \n",
            "\n",
            "val_loss =  963.91187\n",
            "Weight Support =  [0.55958307 0.08134484 0.22643194 0.1326401 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 67 start !\n",
            "Training  :: loss =  207.24786  : micro (f1) =  0.7336508904897105  : macro (f2) =  0.7259127852972569  : Epoch_runtime =  1.7492403984069824  \n",
            "\n",
            "train_loss =  207.24786\n",
            "Validation  :: loss =  998.69794  : micro (f1) =  0.6795926065635609  : macro (f2) =  0.669150385189735  : Epoch_runtime =  1.7492403984069824  \n",
            "\n",
            "val_loss =  998.69794\n",
            "Weight Support =  [0.56026554 0.08147177 0.22631459 0.13194811]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 68 start !\n",
            "Training  :: loss =  206.12518  : micro (f1) =  0.7348606723892952  : macro (f2) =  0.7271445851307882  : Epoch_runtime =  1.74863600730896  \n",
            "\n",
            "train_loss =  206.12518\n",
            "Validation  :: loss =  958.4784  : micro (f1) =  0.6784388025767335  : macro (f2) =  0.6675375463099406  : Epoch_runtime =  1.74863600730896  \n",
            "\n",
            "val_loss =  958.4784\n",
            "Weight Support =  [0.56081295 0.08146638 0.22604838 0.13167219]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 69 start !\n",
            "Training  :: loss =  207.08015  : micro (f1) =  0.733882979347227  : macro (f2) =  0.726109855348696  : Epoch_runtime =  1.745551347732544  \n",
            "\n",
            "train_loss =  207.08015\n",
            "Validation  :: loss =  956.70496  : micro (f1) =  0.6768457525796748  : macro (f2) =  0.6658662349686973  : Epoch_runtime =  1.745551347732544  \n",
            "\n",
            "val_loss =  956.70496\n",
            "Weight Support =  [0.5624989  0.08117326 0.22494754 0.13138033]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 70 start !\n",
            "Training  :: loss =  206.7154  : micro (f1) =  0.7354398352456605  : macro (f2) =  0.7276446702878592  : Epoch_runtime =  1.806595802307129  \n",
            "\n",
            "train_loss =  206.7154\n",
            "Validation  :: loss =  950.74713  : micro (f1) =  0.6767444490519093  : macro (f2) =  0.6659103785784763  : Epoch_runtime =  1.806595802307129  \n",
            "\n",
            "val_loss =  950.74713\n",
            "Weight Support =  [0.56274134 0.08160499 0.22433758 0.13131605]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 71 start !\n",
            "Training  :: loss =  205.4615  : micro (f1) =  0.734788383108259  : macro (f2) =  0.7270520982231273  : Epoch_runtime =  1.786738395690918  \n",
            "\n",
            "train_loss =  205.4615\n",
            "Validation  :: loss =  967.86255  : micro (f1) =  0.6807812732965557  : macro (f2) =  0.6703090585371979  : Epoch_runtime =  1.786738395690918  \n",
            "\n",
            "val_loss =  967.86255\n",
            "Weight Support =  [0.56371707 0.08111998 0.22398295 0.13118005]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 72 start !\n",
            "Training  :: loss =  204.3836  : micro (f1) =  0.735345982930098  : macro (f2) =  0.7275837672718725  : Epoch_runtime =  1.7447960376739502  \n",
            "\n",
            "train_loss =  204.3836\n",
            "Validation  :: loss =  999.53644  : micro (f1) =  0.6839603522605845  : macro (f2) =  0.6739142760661541  : Epoch_runtime =  1.7447960376739502  \n",
            "\n",
            "val_loss =  999.53644\n",
            "Weight Support =  [0.5637083  0.08137189 0.22419952 0.13072033]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 73 start !\n",
            "Training  :: loss =  203.88475  : micro (f1) =  0.7353902593890363  : macro (f2) =  0.7276524004639451  : Epoch_runtime =  1.7517869472503662  \n",
            "\n",
            "train_loss =  203.88475\n",
            "Validation  :: loss =  990.84607  : micro (f1) =  0.684853630624977  : macro (f2) =  0.6742690428144329  : Epoch_runtime =  1.7517869472503662  \n",
            "\n",
            "val_loss =  990.84607\n",
            "Weight Support =  [0.5639907  0.08151639 0.22436567 0.13012733]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 74 start !\n",
            "Training  :: loss =  204.05345  : micro (f1) =  0.7339559330783489  : macro (f2) =  0.7261925146160783  : Epoch_runtime =  1.7767236232757568  \n",
            "\n",
            "train_loss =  204.05345\n",
            "Validation  :: loss =  953.3634  : micro (f1) =  0.6828110513628778  : macro (f2) =  0.6725199783641915  : Epoch_runtime =  1.7767236232757568  \n",
            "\n",
            "val_loss =  953.3634\n",
            "Weight Support =  [0.5647953  0.08126615 0.22427139 0.12966716]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 75 start !\n",
            "Training  :: loss =  202.76328  : micro (f1) =  0.7337376676090706  : macro (f2) =  0.7259658154414814  : Epoch_runtime =  1.7832045555114746  \n",
            "\n",
            "train_loss =  202.76328\n",
            "Validation  :: loss =  958.2031  : micro (f1) =  0.6789018427980444  : macro (f2) =  0.668922427571968  : Epoch_runtime =  1.7832045555114746  \n",
            "\n",
            "val_loss =  958.2031\n",
            "Weight Support =  [0.56525546 0.08151283 0.22383693 0.12939475]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 76 start !\n",
            "Training  :: loss =  202.60483  : micro (f1) =  0.7348602302090674  : macro (f2) =  0.7270627521988042  : Epoch_runtime =  1.7620694637298584  \n",
            "\n",
            "train_loss =  202.60483\n",
            "Validation  :: loss =  956.7963  : micro (f1) =  0.6782470481380564  : macro (f2) =  0.668298235825571  : Epoch_runtime =  1.7620694637298584  \n",
            "\n",
            "val_loss =  956.7963\n",
            "Weight Support =  [0.5653547  0.08147675 0.2239347  0.12923378]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 77 start !\n",
            "Training  :: loss =  202.73824  : micro (f1) =  0.7343518974864464  : macro (f2) =  0.7266049703408706  : Epoch_runtime =  1.7557775974273682  \n",
            "\n",
            "train_loss =  202.73824\n",
            "Validation  :: loss =  952.7161  : micro (f1) =  0.6795544647913914  : macro (f2) =  0.6688867716619075  : Epoch_runtime =  1.7557775974273682  \n",
            "\n",
            "val_loss =  952.7161\n",
            "Weight Support =  [0.56625795 0.08168377 0.22298974 0.12906855]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 78 start !\n",
            "Training  :: loss =  203.94337  : micro (f1) =  0.735155203609277  : macro (f2) =  0.7274132814537985  : Epoch_runtime =  1.7501463890075684  \n",
            "\n",
            "train_loss =  203.94337\n",
            "Validation  :: loss =  953.2016  : micro (f1) =  0.6794064748201439  : macro (f2) =  0.6686465724156403  : Epoch_runtime =  1.7501463890075684  \n",
            "\n",
            "val_loss =  953.2016\n",
            "Weight Support =  [0.56646127 0.08146939 0.22319001 0.12887932]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 79 start !\n",
            "Training  :: loss =  203.52966  : micro (f1) =  0.734959005802617  : macro (f2) =  0.7272175874570577  : Epoch_runtime =  1.7342371940612793  \n",
            "\n",
            "train_loss =  203.52966\n",
            "Validation  :: loss =  971.1492  : micro (f1) =  0.6803654670893157  : macro (f2) =  0.6693365583460706  : Epoch_runtime =  1.7342371940612793  \n",
            "\n",
            "val_loss =  971.1492\n",
            "Weight Support =  [0.5676275  0.08099273 0.22289532 0.12848446]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 80 start !\n",
            "Training  :: loss =  203.05351  : micro (f1) =  0.7360011295579428  : macro (f2) =  0.7282193545613977  : Epoch_runtime =  1.7489900588989258  \n",
            "\n",
            "train_loss =  203.05351\n",
            "Validation  :: loss =  976.851  : micro (f1) =  0.683846125365619  : macro (f2) =  0.6729113547278724  : Epoch_runtime =  1.7489900588989258  \n",
            "\n",
            "val_loss =  976.851\n",
            "Weight Support =  [0.56808084 0.08117402 0.22279464 0.1279505 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 81 start !\n",
            "Training  :: loss =  203.41367  : micro (f1) =  0.7349765975962557  : macro (f2) =  0.7272273998596757  : Epoch_runtime =  1.7186269760131836  \n",
            "\n",
            "train_loss =  203.41367\n",
            "Validation  :: loss =  995.36145  : micro (f1) =  0.6824106048791356  : macro (f2) =  0.6718697488151528  : Epoch_runtime =  1.7186269760131836  \n",
            "\n",
            "val_loss =  995.36145\n",
            "Weight Support =  [0.5684238  0.08156879 0.22248943 0.12751795]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 82 start !\n",
            "Training  :: loss =  204.16304  : micro (f1) =  0.7347442390627387  : macro (f2) =  0.7270020343518255  : Epoch_runtime =  1.719078540802002  \n",
            "\n",
            "train_loss =  204.16304\n",
            "Validation  :: loss =  978.2685  : micro (f1) =  0.6810705741626795  : macro (f2) =  0.6709530227227593  : Epoch_runtime =  1.719078540802002  \n",
            "\n",
            "val_loss =  978.2685\n",
            "Weight Support =  [0.5690756  0.08175033 0.22222684 0.1269473 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 83 start !\n",
            "Training  :: loss =  204.70459  : micro (f1) =  0.7338948059869563  : macro (f2) =  0.7261664377845919  : Epoch_runtime =  1.7613792419433594  \n",
            "\n",
            "train_loss =  204.70459\n",
            "Validation  :: loss =  983.642  : micro (f1) =  0.6781151673679822  : macro (f2) =  0.6680269611656414  : Epoch_runtime =  1.7613792419433594  \n",
            "\n",
            "val_loss =  983.642\n",
            "Weight Support =  [0.5701195  0.08114295 0.22180705 0.12693055]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 84 start !\n",
            "Training  :: loss =  205.0907  : micro (f1) =  0.7351200263071358  : macro (f2) =  0.72736894357399  : Epoch_runtime =  1.7229423522949219  \n",
            "\n",
            "train_loss =  205.0907\n",
            "Validation  :: loss =  961.90704  : micro (f1) =  0.6775010393438906  : macro (f2) =  0.6671273726355901  : Epoch_runtime =  1.7229423522949219  \n",
            "\n",
            "val_loss =  961.90704\n",
            "Weight Support =  [0.5704724  0.08119786 0.22187637 0.1264534 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 85 start !\n",
            "Training  :: loss =  206.73096  : micro (f1) =  0.7341231170233999  : macro (f2) =  0.7264358283390339  : Epoch_runtime =  1.7320666313171387  \n",
            "\n",
            "train_loss =  206.73096\n",
            "Validation  :: loss =  956.0777  : micro (f1) =  0.6759933272672126  : macro (f2) =  0.6652069149464115  : Epoch_runtime =  1.7320666313171387  \n",
            "\n",
            "val_loss =  956.0777\n",
            "Weight Support =  [0.5712374  0.08151207 0.2211762  0.12607436]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 86 start !\n",
            "Training  :: loss =  206.35414  : micro (f1) =  0.7358315195011471  : macro (f2) =  0.7280753086945246  : Epoch_runtime =  1.8174247741699219  \n",
            "\n",
            "train_loss =  206.35414\n",
            "Validation  :: loss =  949.17944  : micro (f1) =  0.6789918502867492  : macro (f2) =  0.6677573154650096  : Epoch_runtime =  1.8174247741699219  \n",
            "\n",
            "val_loss =  949.17944\n",
            "Weight Support =  [0.5713532  0.08179169 0.2212138  0.12564133]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 87 start !\n",
            "Training  :: loss =  204.09479  : micro (f1) =  0.735869488453118  : macro (f2) =  0.7280564865555677  : Epoch_runtime =  1.7625374794006348  \n",
            "\n",
            "train_loss =  204.09479\n",
            "Validation  :: loss =  994.67303  : micro (f1) =  0.6829631837829782  : macro (f2) =  0.6716290892060526  : Epoch_runtime =  1.7625374794006348  \n",
            "\n",
            "val_loss =  994.67303\n",
            "Weight Support =  [0.5719036  0.08180688 0.22080772 0.1254818 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 88 start !\n",
            "Training  :: loss =  203.01553  : micro (f1) =  0.7358766042623337  : macro (f2) =  0.7281377847664792  : Epoch_runtime =  1.7592217922210693  \n",
            "\n",
            "train_loss =  203.01553\n",
            "Validation  :: loss =  1006.51996  : micro (f1) =  0.6834711050153836  : macro (f2) =  0.6727596656498699  : Epoch_runtime =  1.7592217922210693  \n",
            "\n",
            "val_loss =  1006.51996\n",
            "Weight Support =  [0.57186896 0.0821686  0.22065222 0.12531027]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 89 start !\n",
            "Training  :: loss =  204.08662  : micro (f1) =  0.734128754961599  : macro (f2) =  0.7264698542559935  : Epoch_runtime =  1.768399953842163  \n",
            "\n",
            "train_loss =  204.08662\n",
            "Validation  :: loss =  972.94025  : micro (f1) =  0.68227350300512  : macro (f2) =  0.672158569534234  : Epoch_runtime =  1.768399953842163  \n",
            "\n",
            "val_loss =  972.94025\n",
            "Weight Support =  [0.57249045 0.08205742 0.22067611 0.12477604]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 90 start !\n",
            "Training  :: loss =  202.92365  : micro (f1) =  0.7357100038788392  : macro (f2) =  0.7279477979166012  : Epoch_runtime =  1.7504472732543945  \n",
            "\n",
            "train_loss =  202.92365\n",
            "Validation  :: loss =  978.19244  : micro (f1) =  0.6780816785312851  : macro (f2) =  0.6686621993863596  : Epoch_runtime =  1.7504472732543945  \n",
            "\n",
            "val_loss =  978.19244\n",
            "Weight Support =  [0.573624   0.08156295 0.22061591 0.1241971 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 91 start !\n",
            "Training  :: loss =  202.32973  : micro (f1) =  0.7329358432217784  : macro (f2) =  0.7252121169613  : Epoch_runtime =  1.7466762065887451  \n",
            "\n",
            "train_loss =  202.32973\n",
            "Validation  :: loss =  967.7082  : micro (f1) =  0.6750547872742386  : macro (f2) =  0.6649254732590899  : Epoch_runtime =  1.7466762065887451  \n",
            "\n",
            "val_loss =  967.7082\n",
            "Weight Support =  [0.5738035  0.08141799 0.22076787 0.12401066]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 92 start !\n",
            "Training  :: loss =  203.49478  : micro (f1) =  0.7360803878194567  : macro (f2) =  0.7283250703857171  : Epoch_runtime =  1.7312204837799072  \n",
            "\n",
            "train_loss =  203.49478\n",
            "Validation  :: loss =  967.133  : micro (f1) =  0.6758657391568783  : macro (f2) =  0.6642882865817266  : Epoch_runtime =  1.7312204837799072  \n",
            "\n",
            "val_loss =  967.133\n",
            "Weight Support =  [0.57345587 0.08199385 0.22053246 0.12401775]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 93 start !\n",
            "Training  :: loss =  203.12233  : micro (f1) =  0.7351178585697163  : macro (f2) =  0.7273906808726698  : Epoch_runtime =  1.7718825340270996  \n",
            "\n",
            "train_loss =  203.12233\n",
            "Validation  :: loss =  949.7213  : micro (f1) =  0.6807548875579161  : macro (f2) =  0.668892701802341  : Epoch_runtime =  1.7718825340270996  \n",
            "\n",
            "val_loss =  949.7213\n",
            "Weight Support =  [0.5731218  0.08226505 0.22065243 0.12396076]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 94 start !\n",
            "Training  :: loss =  201.97368  : micro (f1) =  0.736154434700724  : macro (f2) =  0.7284184310897093  : Epoch_runtime =  1.7291920185089111  \n",
            "\n",
            "train_loss =  201.97368\n",
            "Validation  :: loss =  986.914  : micro (f1) =  0.6825829074495467  : macro (f2) =  0.6718076123850775  : Epoch_runtime =  1.7291920185089111  \n",
            "\n",
            "val_loss =  986.914\n",
            "Weight Support =  [0.57330424 0.08226831 0.22041969 0.12400772]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 95 start !\n",
            "Training  :: loss =  200.91516  : micro (f1) =  0.7357487354428892  : macro (f2) =  0.7280191171692821  : Epoch_runtime =  1.7403075695037842  \n",
            "\n",
            "train_loss =  200.91516\n",
            "Validation  :: loss =  991.4058  : micro (f1) =  0.6836595020531981  : macro (f2) =  0.6736596293307592  : Epoch_runtime =  1.7403075695037842  \n",
            "\n",
            "val_loss =  991.4058\n",
            "Weight Support =  [0.5742263  0.08230042 0.22001982 0.12345343]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 96 start !\n",
            "Training  :: loss =  200.7942  : micro (f1) =  0.7357984616470257  : macro (f2) =  0.7280944888979582  : Epoch_runtime =  1.7515523433685303  \n",
            "\n",
            "train_loss =  200.7942\n",
            "Validation  :: loss =  980.74036  : micro (f1) =  0.6814303731730841  : macro (f2) =  0.6713176756722338  : Epoch_runtime =  1.7515523433685303  \n",
            "\n",
            "val_loss =  980.74036\n",
            "Weight Support =  [0.5752589  0.08189587 0.22004288 0.12280238]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 97 start !\n",
            "Training  :: loss =  200.77544  : micro (f1) =  0.734614346249105  : macro (f2) =  0.7268577436905578  : Epoch_runtime =  1.7635822296142578  \n",
            "\n",
            "train_loss =  200.77544\n",
            "Validation  :: loss =  972.7594  : micro (f1) =  0.6790151174973806  : macro (f2) =  0.6689064591637004  : Epoch_runtime =  1.7635822296142578  \n",
            "\n",
            "val_loss =  972.7594\n",
            "Weight Support =  [0.5760098  0.08160282 0.21997575 0.12241159]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 98 start !\n",
            "Training  :: loss =  200.00677  : micro (f1) =  0.7349199256872752  : macro (f2) =  0.7271746008881735  : Epoch_runtime =  1.7539551258087158  \n",
            "\n",
            "train_loss =  200.00677\n",
            "Validation  :: loss =  957.31665  : micro (f1) =  0.6794596831847086  : macro (f2) =  0.6682261613364436  : Epoch_runtime =  1.7539551258087158  \n",
            "\n",
            "val_loss =  957.31665\n",
            "Weight Support =  [0.57576007 0.08198202 0.21999143 0.12226642]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 99 start !\n",
            "Training  :: loss =  199.88177  : micro (f1) =  0.7340471694794  : macro (f2) =  0.7263403704612211  : Epoch_runtime =  1.7408342361450195  \n",
            "\n",
            "train_loss =  199.88177\n",
            "Validation  :: loss =  954.2605  : micro (f1) =  0.6811435477170589  : macro (f2) =  0.6698183208494646  : Epoch_runtime =  1.7408342361450195  \n",
            "\n",
            "val_loss =  954.2605\n",
            "Weight Support =  [0.5758904  0.08220475 0.21971498 0.12218984]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 100 start !\n",
            "Training  :: loss =  200.15883  : micro (f1) =  0.7363714450445362  : macro (f2) =  0.7285781520946352  : Epoch_runtime =  1.7565407752990723  \n",
            "\n",
            "train_loss =  200.15883\n",
            "Validation  :: loss =  955.3225  : micro (f1) =  0.6796606974552309  : macro (f2) =  0.668758203728532  : Epoch_runtime =  1.7565407752990723  \n",
            "\n",
            "val_loss =  955.3225\n",
            "Weight Support =  [0.57642245 0.08246765 0.21926028 0.1218496 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 101 start !\n",
            "Training  :: loss =  199.52544  : micro (f1) =  0.7351383097433488  : macro (f2) =  0.7273686665442717  : Epoch_runtime =  1.7423791885375977  \n",
            "\n",
            "train_loss =  199.52544\n",
            "Validation  :: loss =  967.2614  : micro (f1) =  0.6806854748185287  : macro (f2) =  0.6705288956325526  : Epoch_runtime =  1.7423791885375977  \n",
            "\n",
            "val_loss =  967.2614\n",
            "Weight Support =  [0.5771081  0.08204236 0.21902479 0.12182473]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 102 start !\n",
            "Training  :: loss =  199.07806  : micro (f1) =  0.7361382525369059  : macro (f2) =  0.7283909145283114  : Epoch_runtime =  1.815666675567627  \n",
            "\n",
            "train_loss =  199.07806\n",
            "Validation  :: loss =  995.57654  : micro (f1) =  0.6821221578243425  : macro (f2) =  0.6720507017658706  : Epoch_runtime =  1.815666675567627  \n",
            "\n",
            "val_loss =  995.57654\n",
            "Weight Support =  [0.57770526 0.08179279 0.21911904 0.12138287]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 103 start !\n",
            "Training  :: loss =  199.35918  : micro (f1) =  0.7359181031946813  : macro (f2) =  0.7281738060983756  : Epoch_runtime =  1.7950329780578613  \n",
            "\n",
            "train_loss =  199.35918\n",
            "Validation  :: loss =  982.3332  : micro (f1) =  0.6830873922852178  : macro (f2) =  0.6728523198437647  : Epoch_runtime =  1.7950329780578613  \n",
            "\n",
            "val_loss =  982.3332\n",
            "Weight Support =  [0.5778617  0.08197569 0.21909928 0.12106325]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 104 start !\n",
            "Training  :: loss =  198.93697  : micro (f1) =  0.7351759148276106  : macro (f2) =  0.727339874927615  : Epoch_runtime =  1.771515130996704  \n",
            "\n",
            "train_loss =  198.93697\n",
            "Validation  :: loss =  979.4145  : micro (f1) =  0.6816297568809053  : macro (f2) =  0.670923397170541  : Epoch_runtime =  1.771515130996704  \n",
            "\n",
            "val_loss =  979.4145\n",
            "Weight Support =  [0.5773357  0.0823175  0.21949127 0.12085555]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 105 start !\n",
            "Training  :: loss =  198.95062  : micro (f1) =  0.7345084968547555  : macro (f2) =  0.7267790751255041  : Epoch_runtime =  1.7753431797027588  \n",
            "\n",
            "train_loss =  198.95062\n",
            "Validation  :: loss =  977.8235  : micro (f1) =  0.6784678543181047  : macro (f2) =  0.6677435080957842  : Epoch_runtime =  1.7753431797027588  \n",
            "\n",
            "val_loss =  977.8235\n",
            "Weight Support =  [0.5772772  0.08246639 0.21944979 0.12080669]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 106 start !\n",
            "Training  :: loss =  198.42911  : micro (f1) =  0.7353479853479853  : macro (f2) =  0.7275882517314982  : Epoch_runtime =  1.7455010414123535  \n",
            "\n",
            "train_loss =  198.42911\n",
            "Validation  :: loss =  965.4366  : micro (f1) =  0.67713972063444  : macro (f2) =  0.6663272938710163  : Epoch_runtime =  1.7455010414123535  \n",
            "\n",
            "val_loss =  965.4366\n",
            "Weight Support =  [0.57763666 0.08272993 0.21904185 0.12059159]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 107 start !\n",
            "Training  :: loss =  198.68619  : micro (f1) =  0.7355751713113695  : macro (f2) =  0.7278380936640425  : Epoch_runtime =  1.7334487438201904  \n",
            "\n",
            "train_loss =  198.68619\n",
            "Validation  :: loss =  945.6095  : micro (f1) =  0.6783193023519197  : macro (f2) =  0.6674859005375116  : Epoch_runtime =  1.7334487438201904  \n",
            "\n",
            "val_loss =  945.6095\n",
            "Weight Support =  [0.57848704 0.082365   0.21864632 0.12050167]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 108 start !\n",
            "Training  :: loss =  198.45143  : micro (f1) =  0.7359540646436598  : macro (f2) =  0.7281584316378373  : Epoch_runtime =  1.752889633178711  \n",
            "\n",
            "train_loss =  198.45143\n",
            "Validation  :: loss =  943.8742  : micro (f1) =  0.6787883332083677  : macro (f2) =  0.6679913514881282  : Epoch_runtime =  1.752889633178711  \n",
            "\n",
            "val_loss =  943.8742\n",
            "Weight Support =  [0.578727   0.08220143 0.21871273 0.12035883]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 109 start !\n",
            "Training  :: loss =  197.35025  : micro (f1) =  0.7360937831030344  : macro (f2) =  0.7282634873114595  : Epoch_runtime =  1.7436060905456543  \n",
            "\n",
            "train_loss =  197.35025\n",
            "Validation  :: loss =  968.31635  : micro (f1) =  0.6821251020256734  : macro (f2) =  0.6715101948928256  : Epoch_runtime =  1.7436060905456543  \n",
            "\n",
            "val_loss =  968.31635\n",
            "Weight Support =  [0.57959944 0.08177543 0.21844095 0.12018414]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 110 start !\n",
            "Training  :: loss =  197.08397  : micro (f1) =  0.7361024470050965  : macro (f2) =  0.7283002366112227  : Epoch_runtime =  1.7212705612182617  \n",
            "\n",
            "train_loss =  197.08397\n",
            "Validation  :: loss =  991.5123  : micro (f1) =  0.6822980331147904  : macro (f2) =  0.6722647677614104  : Epoch_runtime =  1.7212705612182617  \n",
            "\n",
            "val_loss =  991.5123\n",
            "Weight Support =  [0.5795187  0.08234742 0.21839973 0.11973416]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 111 start !\n",
            "Training  :: loss =  198.70216  : micro (f1) =  0.7357762777242044  : macro (f2) =  0.7280026812338687  : Epoch_runtime =  1.7524948120117188  \n",
            "\n",
            "train_loss =  198.70216\n",
            "Validation  :: loss =  986.10925  : micro (f1) =  0.6817332835263354  : macro (f2) =  0.6720590692645678  : Epoch_runtime =  1.7524948120117188  \n",
            "\n",
            "val_loss =  986.10925\n",
            "Weight Support =  [0.5788293  0.08307574 0.21869494 0.11940004]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 112 start !\n",
            "Training  :: loss =  198.0666  : micro (f1) =  0.7350740571079554  : macro (f2) =  0.7273124824518542  : Epoch_runtime =  1.7351675033569336  \n",
            "\n",
            "train_loss =  198.0666\n",
            "Validation  :: loss =  961.1581  : micro (f1) =  0.6816518557239938  : macro (f2) =  0.6712120358421907  : Epoch_runtime =  1.7351675033569336  \n",
            "\n",
            "val_loss =  961.1581\n",
            "Weight Support =  [0.57915807 0.08311498 0.21861023 0.11911666]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 113 start !\n",
            "Training  :: loss =  197.82951  : micro (f1) =  0.7360662097504203  : macro (f2) =  0.7283467294204369  : Epoch_runtime =  1.7729301452636719  \n",
            "\n",
            "train_loss =  197.82951\n",
            "Validation  :: loss =  962.2639  : micro (f1) =  0.6774583963691376  : macro (f2) =  0.6661535021969756  : Epoch_runtime =  1.7729301452636719  \n",
            "\n",
            "val_loss =  962.2639\n",
            "Weight Support =  [0.5798434  0.08283613 0.2182962  0.11902436]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 114 start !\n",
            "Training  :: loss =  198.16412  : micro (f1) =  0.7349393345000529  : macro (f2) =  0.7271820095615917  : Epoch_runtime =  1.7502741813659668  \n",
            "\n",
            "train_loss =  198.16412\n",
            "Validation  :: loss =  948.81976  : micro (f1) =  0.6801089505939321  : macro (f2) =  0.6687966978947562  : Epoch_runtime =  1.7502741813659668  \n",
            "\n",
            "val_loss =  948.81976\n",
            "Weight Support =  [0.58049697 0.08242631 0.21814916 0.11892754]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 115 start !\n",
            "Training  :: loss =  198.40471  : micro (f1) =  0.7359817480477937  : macro (f2) =  0.7282043468252992  : Epoch_runtime =  1.7302119731903076  \n",
            "\n",
            "train_loss =  198.40471\n",
            "Validation  :: loss =  949.22815  : micro (f1) =  0.6800498094411531  : macro (f2) =  0.6685680596798534  : Epoch_runtime =  1.7302119731903076  \n",
            "\n",
            "val_loss =  949.22815\n",
            "Weight Support =  [0.58118045 0.08227512 0.21803086 0.11851358]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 116 start !\n",
            "Training  :: loss =  198.70189  : micro (f1) =  0.7345591258885039  : macro (f2) =  0.7268194200222241  : Epoch_runtime =  1.7232918739318848  \n",
            "\n",
            "train_loss =  198.70189\n",
            "Validation  :: loss =  966.2181  : micro (f1) =  0.6790704681556251  : macro (f2) =  0.667930627864785  : Epoch_runtime =  1.7232918739318848  \n",
            "\n",
            "val_loss =  966.2181\n",
            "Weight Support =  [0.582197   0.08218878 0.21754706 0.11806715]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 117 start !\n",
            "Training  :: loss =  199.02823  : micro (f1) =  0.7370840553060575  : macro (f2) =  0.7293333528263775  : Epoch_runtime =  1.7423434257507324  \n",
            "\n",
            "train_loss =  199.02823\n",
            "Validation  :: loss =  975.3395  : micro (f1) =  0.6821959673511983  : macro (f2) =  0.6716047296470284  : Epoch_runtime =  1.7423434257507324  \n",
            "\n",
            "val_loss =  975.3395\n",
            "Weight Support =  [0.58144635 0.08280277 0.2177138  0.11803709]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 118 start !\n",
            "Training  :: loss =  199.13081  : micro (f1) =  0.7350369081762189  : macro (f2) =  0.7273024970952684  : Epoch_runtime =  1.7801756858825684  \n",
            "\n",
            "train_loss =  199.13081\n",
            "Validation  :: loss =  987.6683  : micro (f1) =  0.6815513782599003  : macro (f2) =  0.6716835034210412  : Epoch_runtime =  1.7801756858825684  \n",
            "\n",
            "val_loss =  987.6683\n",
            "Weight Support =  [0.58205765 0.08290422 0.21724817 0.11778996]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 119 start !\n",
            "Training  :: loss =  198.98076  : micro (f1) =  0.7375322822203092  : macro (f2) =  0.7297587905132011  : Epoch_runtime =  1.8326785564422607  \n",
            "\n",
            "train_loss =  198.98076\n",
            "Validation  :: loss =  981.37036  : micro (f1) =  0.6829051273495718  : macro (f2) =  0.6725374216983666  : Epoch_runtime =  1.8326785564422607  \n",
            "\n",
            "val_loss =  981.37036\n",
            "Weight Support =  [0.5818377  0.08328271 0.21742693 0.11745264]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 120 start !\n",
            "Training  :: loss =  199.10063  : micro (f1) =  0.7337748344370861  : macro (f2) =  0.726032626581971  : Epoch_runtime =  1.744828462600708  \n",
            "\n",
            "train_loss =  199.10063\n",
            "Validation  :: loss =  955.0312  : micro (f1) =  0.6815025016802331  : macro (f2) =  0.6711522504367413  : Epoch_runtime =  1.744828462600708  \n",
            "\n",
            "val_loss =  955.0312\n",
            "Weight Support =  [0.5821855  0.08286189 0.21773413 0.11721853]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 121 start !\n",
            "Training  :: loss =  198.91266  : micro (f1) =  0.7371371371371371  : macro (f2) =  0.729388744351448  : Epoch_runtime =  1.7795288562774658  \n",
            "\n",
            "train_loss =  198.91266\n",
            "Validation  :: loss =  969.8865  : micro (f1) =  0.677331723513432  : macro (f2) =  0.6671854978508425  : Epoch_runtime =  1.7795288562774658  \n",
            "\n",
            "val_loss =  969.8865\n",
            "Weight Support =  [0.5816513  0.0830861  0.21830894 0.11695371]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 122 start !\n",
            "Training  :: loss =  198.97588  : micro (f1) =  0.7337446049915556  : macro (f2) =  0.7260456522490931  : Epoch_runtime =  1.7795264720916748  \n",
            "\n",
            "train_loss =  198.97588\n",
            "Validation  :: loss =  962.3375  : micro (f1) =  0.6769464489175845  : macro (f2) =  0.6660758605386966  : Epoch_runtime =  1.7795264720916748  \n",
            "\n",
            "val_loss =  962.3375\n",
            "Weight Support =  [0.58309746 0.08270674 0.21772024 0.11647555]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 123 start !\n",
            "Training  :: loss =  199.25447  : micro (f1) =  0.7365457779243573  : macro (f2) =  0.72876043586671  : Epoch_runtime =  1.7439968585968018  \n",
            "\n",
            "train_loss =  199.25447\n",
            "Validation  :: loss =  967.8106  : micro (f1) =  0.67713972063444  : macro (f2) =  0.6655269088295537  : Epoch_runtime =  1.7439968585968018  \n",
            "\n",
            "val_loss =  967.8106\n",
            "Weight Support =  [0.58357644 0.08303274 0.21685955 0.11653131]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 124 start !\n",
            "Training  :: loss =  199.51848  : micro (f1) =  0.7350200359589644  : macro (f2) =  0.7272986207025882  : Epoch_runtime =  1.8261618614196777  \n",
            "\n",
            "train_loss =  199.51848\n",
            "Validation  :: loss =  955.43475  : micro (f1) =  0.6815901774350528  : macro (f2) =  0.6703162390428046  : Epoch_runtime =  1.8261618614196777  \n",
            "\n",
            "val_loss =  955.43475\n",
            "Weight Support =  [0.58460224 0.08302078 0.21617551 0.11620148]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 125 start !\n",
            "Training  :: loss =  198.39493  : micro (f1) =  0.736351107812997  : macro (f2) =  0.7285607963963119  : Epoch_runtime =  1.7370445728302002  \n",
            "\n",
            "train_loss =  198.39493\n",
            "Validation  :: loss =  962.0622  : micro (f1) =  0.6834373715097373  : macro (f2) =  0.6722733399545818  : Epoch_runtime =  1.7370445728302002  \n",
            "\n",
            "val_loss =  962.0622\n",
            "Weight Support =  [0.5839854  0.08302611 0.21652746 0.11646101]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 126 start !\n",
            "Training  :: loss =  198.5145  : micro (f1) =  0.735544867648961  : macro (f2) =  0.7278022096540611  : Epoch_runtime =  1.7632145881652832  \n",
            "\n",
            "train_loss =  198.5145\n",
            "Validation  :: loss =  1001.6432  : micro (f1) =  0.6819177776952501  : macro (f2) =  0.6716250979685395  : Epoch_runtime =  1.7632145881652832  \n",
            "\n",
            "val_loss =  1001.6432\n",
            "Weight Support =  [0.5836963  0.08353639 0.21704318 0.11572409]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 127 start !\n",
            "Training  :: loss =  198.90732  : micro (f1) =  0.7358910104824762  : macro (f2) =  0.7281383712868613  : Epoch_runtime =  1.7364356517791748  \n",
            "\n",
            "train_loss =  198.90732\n",
            "Validation  :: loss =  986.56714  : micro (f1) =  0.6816287032901592  : macro (f2) =  0.6718034866558182  : Epoch_runtime =  1.7364356517791748  \n",
            "\n",
            "val_loss =  986.56714\n",
            "Weight Support =  [0.5834574  0.08362449 0.21751961 0.11539847]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 128 start !\n",
            "Training  :: loss =  199.36932  : micro (f1) =  0.7344083749970658  : macro (f2) =  0.72664730239601  : Epoch_runtime =  1.7362630367279053  \n",
            "\n",
            "train_loss =  199.36932\n",
            "Validation  :: loss =  950.6155  : micro (f1) =  0.681209680426595  : macro (f2) =  0.6711023092828248  : Epoch_runtime =  1.7362630367279053  \n",
            "\n",
            "val_loss =  950.6155\n",
            "Weight Support =  [0.5838458  0.08380644 0.21734285 0.11500487]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 129 start !\n",
            "Training  :: loss =  199.47003  : micro (f1) =  0.7350521763655166  : macro (f2) =  0.7273041579238574  : Epoch_runtime =  1.7300446033477783  \n",
            "\n",
            "train_loss =  199.47003\n",
            "Validation  :: loss =  939.7802  : micro (f1) =  0.6792481259652691  : macro (f2) =  0.6685128218577572  : Epoch_runtime =  1.7300446033477783  \n",
            "\n",
            "val_loss =  939.7802\n",
            "Weight Support =  [0.58470464 0.08361563 0.2169079  0.11477192]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 130 start !\n",
            "Training  :: loss =  198.63548  : micro (f1) =  0.7360472365000765  : macro (f2) =  0.7282945343698894  : Epoch_runtime =  1.746434211730957  \n",
            "\n",
            "train_loss =  198.63548\n",
            "Validation  :: loss =  959.05066  : micro (f1) =  0.6777272727272727  : macro (f2) =  0.6665126774266092  : Epoch_runtime =  1.746434211730957  \n",
            "\n",
            "val_loss =  959.05066\n",
            "Weight Support =  [0.585345   0.08382659 0.21651046 0.11431798]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 131 start !\n",
            "Training  :: loss =  198.92734  : micro (f1) =  0.7352263645338724  : macro (f2) =  0.7274743829021462  : Epoch_runtime =  1.6923534870147705  \n",
            "\n",
            "train_loss =  198.92734\n",
            "Validation  :: loss =  966.546  : micro (f1) =  0.679483282674772  : macro (f2) =  0.6678280896461988  : Epoch_runtime =  1.6923534870147705  \n",
            "\n",
            "val_loss =  966.546\n",
            "Weight Support =  [0.58529353 0.08409192 0.21629241 0.11432213]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 132 start !\n",
            "Training  :: loss =  198.388  : micro (f1) =  0.7357698234387682  : macro (f2) =  0.7279881496379284  : Epoch_runtime =  1.7244389057159424  \n",
            "\n",
            "train_loss =  198.388\n",
            "Validation  :: loss =  960.3122  : micro (f1) =  0.6797733838592278  : macro (f2) =  0.6688820741166177  : Epoch_runtime =  1.7244389057159424  \n",
            "\n",
            "val_loss =  960.3122\n",
            "Weight Support =  [0.58550334 0.08371992 0.21638818 0.11438851]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 133 start !\n",
            "Training  :: loss =  197.04155  : micro (f1) =  0.7369413317308258  : macro (f2) =  0.7292093302444314  : Epoch_runtime =  1.7112133502960205  \n",
            "\n",
            "train_loss =  197.04155\n",
            "Validation  :: loss =  986.0872  : micro (f1) =  0.6820379323168464  : macro (f2) =  0.6716366164819462  : Epoch_runtime =  1.7112133502960205  \n",
            "\n",
            "val_loss =  986.0872\n",
            "Weight Support =  [0.5853845  0.08390517 0.21647522 0.1142351 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 134 start !\n",
            "Training  :: loss =  197.23457  : micro (f1) =  0.7360047072668433  : macro (f2) =  0.7282588234618713  : Epoch_runtime =  1.6948456764221191  \n",
            "\n",
            "train_loss =  197.23457\n",
            "Validation  :: loss =  973.5572  : micro (f1) =  0.684391017367897  : macro (f2) =  0.674386130148916  : Epoch_runtime =  1.6948456764221191  \n",
            "\n",
            "val_loss =  973.5572\n",
            "Weight Support =  [0.58539695 0.08407456 0.21689077 0.11363768]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 135 start !\n",
            "Training  :: loss =  198.01472  : micro (f1) =  0.736469148009511  : macro (f2) =  0.7287262183935825  : Epoch_runtime =  1.7886910438537598  \n",
            "\n",
            "train_loss =  198.01472\n",
            "Validation  :: loss =  967.13995  : micro (f1) =  0.6823546878479697  : macro (f2) =  0.6718512718519708  : Epoch_runtime =  1.7886910438537598  \n",
            "\n",
            "val_loss =  967.13995\n",
            "Weight Support =  [0.5864013  0.0836838  0.21664979 0.11326519]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 136 start !\n",
            "Training  :: loss =  197.43578  : micro (f1) =  0.7349859022556391  : macro (f2) =  0.7272093001802413  : Epoch_runtime =  1.6810784339904785  \n",
            "\n",
            "train_loss =  197.43578\n",
            "Validation  :: loss =  966.34344  : micro (f1) =  0.679116511043612  : macro (f2) =  0.6684965195532506  : Epoch_runtime =  1.6810784339904785  \n",
            "\n",
            "val_loss =  966.34344\n",
            "Weight Support =  [0.5870315  0.08380675 0.21617341 0.11298839]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 137 start !\n",
            "Training  :: loss =  197.30048  : micro (f1) =  0.7357887062254521  : macro (f2) =  0.7280616655071989  : Epoch_runtime =  1.6814305782318115  \n",
            "\n",
            "train_loss =  197.30048\n",
            "Validation  :: loss =  987.1735  : micro (f1) =  0.6735183281945871  : macro (f2) =  0.6627708181219574  : Epoch_runtime =  1.6814305782318115  \n",
            "\n",
            "val_loss =  987.1735\n",
            "Weight Support =  [0.58785456 0.08371848 0.2157611  0.11266576]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 138 start !\n",
            "Training  :: loss =  197.36086  : micro (f1) =  0.7344558521560576  : macro (f2) =  0.7267275827939231  : Epoch_runtime =  1.6776864528656006  \n",
            "\n",
            "train_loss =  197.36086\n",
            "Validation  :: loss =  961.9106  : micro (f1) =  0.6749659452096262  : macro (f2) =  0.6641154137402487  : Epoch_runtime =  1.6776864528656006  \n",
            "\n",
            "val_loss =  961.9106\n",
            "Weight Support =  [0.58739436 0.08457293 0.21551457 0.11251815]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 139 start !\n",
            "Training  :: loss =  198.16791  : micro (f1) =  0.7363726228582187  : macro (f2) =  0.7285986339822237  : Epoch_runtime =  1.680922031402588  \n",
            "\n",
            "train_loss =  198.16791\n",
            "Validation  :: loss =  959.52094  : micro (f1) =  0.6794080914354919  : macro (f2) =  0.6684033478143236  : Epoch_runtime =  1.680922031402588  \n",
            "\n",
            "val_loss =  959.52094\n",
            "Weight Support =  [0.58708924 0.08450159 0.21583879 0.11257039]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 140 start !\n",
            "Training  :: loss =  197.97293  : micro (f1) =  0.7355272752934535  : macro (f2) =  0.7278443664438482  : Epoch_runtime =  1.6981420516967773  \n",
            "\n",
            "train_loss =  197.97293\n",
            "Validation  :: loss =  967.43463  : micro (f1) =  0.6813474408344862  : macro (f2) =  0.670562685783722  : Epoch_runtime =  1.6981420516967773  \n",
            "\n",
            "val_loss =  967.43463\n",
            "Weight Support =  [0.58800554 0.08365743 0.21567401 0.11266293]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 141 start !\n",
            "Training  :: loss =  197.58223  : micro (f1) =  0.7370257716158545  : macro (f2) =  0.729236650334181  : Epoch_runtime =  1.678236961364746  \n",
            "\n",
            "train_loss =  197.58223\n",
            "Validation  :: loss =  996.71014  : micro (f1) =  0.683630698468044  : macro (f2) =  0.6732040082867348  : Epoch_runtime =  1.678236961364746  \n",
            "\n",
            "val_loss =  996.71014\n",
            "Weight Support =  [0.5888207  0.08381961 0.21520403 0.11215568]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 142 start !\n",
            "Training  :: loss =  198.05894  : micro (f1) =  0.7357736008845186  : macro (f2) =  0.7280303012708342  : Epoch_runtime =  1.6737949848175049  \n",
            "\n",
            "train_loss =  198.05894\n",
            "Validation  :: loss =  993.3729  : micro (f1) =  0.685393675918701  : macro (f2) =  0.6750406497592517  : Epoch_runtime =  1.6737949848175049  \n",
            "\n",
            "val_loss =  993.3729\n",
            "Weight Support =  [0.5897386  0.08361731 0.21471544 0.11192858]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 143 start !\n",
            "Training  :: loss =  199.80746  : micro (f1) =  0.7367664557782255  : macro (f2) =  0.7290227047355257  : Epoch_runtime =  1.6535301208496094  \n",
            "\n",
            "train_loss =  199.80746\n",
            "Validation  :: loss =  983.1176  : micro (f1) =  0.6811414857314283  : macro (f2) =  0.6706333355814995  : Epoch_runtime =  1.6535301208496094  \n",
            "\n",
            "val_loss =  983.1176\n",
            "Weight Support =  [0.59071237 0.08361315 0.21429643 0.11137805]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 144 start !\n",
            "Training  :: loss =  198.51556  : micro (f1) =  0.7339068386400178  : macro (f2) =  0.7261569842104973  : Epoch_runtime =  1.6600914001464844  \n",
            "\n",
            "train_loss =  198.51556\n",
            "Validation  :: loss =  983.0631  : micro (f1) =  0.6738008790368814  : macro (f2) =  0.6634659271846968  : Epoch_runtime =  1.6600914001464844  \n",
            "\n",
            "val_loss =  983.0631\n",
            "Weight Support =  [0.59105754 0.08406213 0.21372207 0.11115834]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 145 start !\n",
            "Training  :: loss =  198.87914  : micro (f1) =  0.7358850098804931  : macro (f2) =  0.7281549576613096  : Epoch_runtime =  1.6719155311584473  \n",
            "\n",
            "train_loss =  198.87914\n",
            "Validation  :: loss =  968.71356  : micro (f1) =  0.6729839783841383  : macro (f2) =  0.6621845544298586  : Epoch_runtime =  1.6719155311584473  \n",
            "\n",
            "val_loss =  968.71356\n",
            "Weight Support =  [0.5914085  0.0840952  0.21356164 0.1109346 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 146 start !\n",
            "Training  :: loss =  200.34863  : micro (f1) =  0.7340491697471102  : macro (f2) =  0.7263401296940305  : Epoch_runtime =  1.7041780948638916  \n",
            "\n",
            "train_loss =  200.34863\n",
            "Validation  :: loss =  964.2861  : micro (f1) =  0.677649726053278  : macro (f2) =  0.6665702737394867  : Epoch_runtime =  1.7041780948638916  \n",
            "\n",
            "val_loss =  964.2861\n",
            "Weight Support =  [0.5916681  0.08456957 0.21332619 0.11043612]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 147 start !\n",
            "Training  :: loss =  200.30768  : micro (f1) =  0.7364677899843394  : macro (f2) =  0.7287295144412687  : Epoch_runtime =  1.6748547554016113  \n",
            "\n",
            "train_loss =  200.30768\n",
            "Validation  :: loss =  956.6442  : micro (f1) =  0.6823334463149172  : macro (f2) =  0.6713894407417895  : Epoch_runtime =  1.6748547554016113  \n",
            "\n",
            "val_loss =  956.6442\n",
            "Weight Support =  [0.59147334 0.08457616 0.21359944 0.11035104]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 148 start !\n",
            "Training  :: loss =  198.76704  : micro (f1) =  0.7353508276899925  : macro (f2) =  0.7276130964891488  : Epoch_runtime =  1.7054674625396729  \n",
            "\n",
            "train_loss =  198.76704\n",
            "Validation  :: loss =  988.5676  : micro (f1) =  0.6836409929980903  : macro (f2) =  0.6726901503460077  : Epoch_runtime =  1.7054674625396729  \n",
            "\n",
            "val_loss =  988.5676\n",
            "Weight Support =  [0.5934482  0.0836632  0.2123351  0.11055346]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 149 start !\n",
            "Training  :: loss =  198.5062  : micro (f1) =  0.7376850050120881  : macro (f2) =  0.7299261058941977  : Epoch_runtime =  1.7543718814849854  \n",
            "\n",
            "train_loss =  198.5062\n",
            "Validation  :: loss =  995.37756  : micro (f1) =  0.684584629053754  : macro (f2) =  0.6741580920398803  : Epoch_runtime =  1.7543718814849854  \n",
            "\n",
            "val_loss =  995.37756\n",
            "Weight Support =  [0.59315467 0.08425176 0.21206324 0.11053033]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 150 start !\n",
            "Training  :: loss =  197.85165  : micro (f1) =  0.7345955977354414  : macro (f2) =  0.7268381300439266  : Epoch_runtime =  1.747563362121582  \n",
            "\n",
            "train_loss =  197.85165\n",
            "Validation  :: loss =  973.4205  : micro (f1) =  0.6802797833935017  : macro (f2) =  0.669667077253352  : Epoch_runtime =  1.747563362121582  \n",
            "\n",
            "val_loss =  973.4205\n",
            "Weight Support =  [0.5935172  0.08425735 0.2122698  0.10995568]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 151 start !\n",
            "Training  :: loss =  196.84784  : micro (f1) =  0.7361346516007534  : macro (f2) =  0.7284048813404767  : Epoch_runtime =  2.351377248764038  \n",
            "\n",
            "train_loss =  196.84784\n",
            "Validation  :: loss =  1023.348  : micro (f1) =  0.6694882348373069  : macro (f2) =  0.6578494823283135  : Epoch_runtime =  2.351377248764038  \n",
            "\n",
            "val_loss =  1023.348\n",
            "Weight Support =  [0.5938672  0.08441235 0.21183415 0.10988625]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 152 start !\n",
            "Training  :: loss =  197.04666  : micro (f1) =  0.7340495402514543  : macro (f2) =  0.7262937670033721  : Epoch_runtime =  2.3624041080474854  \n",
            "\n",
            "train_loss =  197.04666\n",
            "Validation  :: loss =  952.00507  : micro (f1) =  0.6779278431962681  : macro (f2) =  0.6670969784763795  : Epoch_runtime =  2.3624041080474854  \n",
            "\n",
            "val_loss =  952.00507\n",
            "Weight Support =  [0.59398395 0.0851266  0.21178153 0.10910796]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 153 start !\n",
            "Training  :: loss =  197.023  : micro (f1) =  0.7359890853063288  : macro (f2) =  0.7282151596720843  : Epoch_runtime =  3.1660003662109375  \n",
            "\n",
            "train_loss =  197.023\n",
            "Validation  :: loss =  959.87775  : micro (f1) =  0.6788802913063268  : macro (f2) =  0.6676043392616923  : Epoch_runtime =  3.1660003662109375  \n",
            "\n",
            "val_loss =  959.87775\n",
            "Weight Support =  [0.5940213  0.0852802  0.21182288 0.1088756 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 154 start !\n",
            "Training  :: loss =  196.03029  : micro (f1) =  0.735304135395746  : macro (f2) =  0.7275673068718301  : Epoch_runtime =  1.6777331829071045  \n",
            "\n",
            "train_loss =  196.03029\n",
            "Validation  :: loss =  963.5373  : micro (f1) =  0.6811243766057127  : macro (f2) =  0.6697865493580688  : Epoch_runtime =  1.6777331829071045  \n",
            "\n",
            "val_loss =  963.5373\n",
            "Weight Support =  [0.5948608  0.0846973  0.21156718 0.10887477]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 155 start !\n",
            "Training  :: loss =  195.1028  : micro (f1) =  0.7373049177236081  : macro (f2) =  0.7295174045975421  : Epoch_runtime =  1.680783987045288  \n",
            "\n",
            "train_loss =  195.1028\n",
            "Validation  :: loss =  977.0852  : micro (f1) =  0.6839223865032961  : macro (f2) =  0.6730315040936067  : Epoch_runtime =  1.680783987045288  \n",
            "\n",
            "val_loss =  977.0852\n",
            "Weight Support =  [0.59502685 0.08461564 0.21117496 0.10918253]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 156 start !\n",
            "Training  :: loss =  193.7308  : micro (f1) =  0.7367070126689587  : macro (f2) =  0.7289319428099161  : Epoch_runtime =  1.674999475479126  \n",
            "\n",
            "train_loss =  193.7308\n",
            "Validation  :: loss =  979.4106  : micro (f1) =  0.6838235294117647  : macro (f2) =  0.6732691372479759  : Epoch_runtime =  1.674999475479126  \n",
            "\n",
            "val_loss =  979.4106\n",
            "Weight Support =  [0.59531987 0.08455383 0.21122901 0.10889724]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 157 start !\n",
            "Training  :: loss =  194.04808  : micro (f1) =  0.7364383045821467  : macro (f2) =  0.7287136021237425  : Epoch_runtime =  1.6777236461639404  \n",
            "\n",
            "train_loss =  194.04808\n",
            "Validation  :: loss =  954.76917  : micro (f1) =  0.6818844099077223  : macro (f2) =  0.6715495987246731  : Epoch_runtime =  1.6777236461639404  \n",
            "\n",
            "val_loss =  954.76917\n",
            "Weight Support =  [0.59528714 0.08484431 0.2112319  0.10863668]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 158 start !\n",
            "Training  :: loss =  193.55267  : micro (f1) =  0.7344716305814704  : macro (f2) =  0.7267579427229292  : Epoch_runtime =  1.6838610172271729  \n",
            "\n",
            "train_loss =  193.55267\n",
            "Validation  :: loss =  947.3793  : micro (f1) =  0.6785674020068396  : macro (f2) =  0.6680122200576137  : Epoch_runtime =  1.6838610172271729  \n",
            "\n",
            "val_loss =  947.3793\n",
            "Weight Support =  [0.5956177  0.08513158 0.2110552  0.10819551]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 159 start !\n",
            "Training  :: loss =  192.99847  : micro (f1) =  0.7355543803983783  : macro (f2) =  0.7278038434413878  : Epoch_runtime =  1.6632251739501953  \n",
            "\n",
            "train_loss =  192.99847\n",
            "Validation  :: loss =  956.7381  : micro (f1) =  0.6789978395178714  : macro (f2) =  0.667463548345204  : Epoch_runtime =  1.6632251739501953  \n",
            "\n",
            "val_loss =  956.7381\n",
            "Weight Support =  [0.59577394 0.08493649 0.21112856 0.10816102]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 160 start !\n",
            "Training  :: loss =  192.952  : micro (f1) =  0.7352626944525976  : macro (f2) =  0.7274852077131094  : Epoch_runtime =  1.6537210941314697  \n",
            "\n",
            "train_loss =  192.952\n",
            "Validation  :: loss =  967.47375  : micro (f1) =  0.6795158781348408  : macro (f2) =  0.6677772563444756  : Epoch_runtime =  1.6537210941314697  \n",
            "\n",
            "val_loss =  967.47375\n",
            "Weight Support =  [0.59663403 0.08464566 0.21060723 0.10811318]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 161 start !\n",
            "Training  :: loss =  192.43787  : micro (f1) =  0.7364491704054357  : macro (f2) =  0.7286973942880213  : Epoch_runtime =  1.6629784107208252  \n",
            "\n",
            "train_loss =  192.43787\n",
            "Validation  :: loss =  973.1724  : micro (f1) =  0.6807980990457513  : macro (f2) =  0.6695982295886169  : Epoch_runtime =  1.6629784107208252  \n",
            "\n",
            "val_loss =  973.1724\n",
            "Weight Support =  [0.596886   0.08457574 0.21045797 0.10808032]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 162 start !\n",
            "Training  :: loss =  192.08955  : micro (f1) =  0.7369301646027501  : macro (f2) =  0.7291643272022793  : Epoch_runtime =  1.6795260906219482  \n",
            "\n",
            "train_loss =  192.08955\n",
            "Validation  :: loss =  971.3294  : micro (f1) =  0.6843572304604749  : macro (f2) =  0.6738334588826991  : Epoch_runtime =  1.6795260906219482  \n",
            "\n",
            "val_loss =  971.3294\n",
            "Weight Support =  [0.5968925  0.08479039 0.2102761  0.108041  ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 163 start !\n",
            "Training  :: loss =  192.28445  : micro (f1) =  0.7368297077979438  : macro (f2) =  0.7290336623982859  : Epoch_runtime =  1.6458234786987305  \n",
            "\n",
            "train_loss =  192.28445\n",
            "Validation  :: loss =  964.46716  : micro (f1) =  0.6827350934440818  : macro (f2) =  0.6725219296299739  : Epoch_runtime =  1.6458234786987305  \n",
            "\n",
            "val_loss =  964.46716\n",
            "Weight Support =  [0.5969218  0.08496023 0.21050978 0.10760819]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 164 start !\n",
            "Training  :: loss =  192.74893  : micro (f1) =  0.735702700796841  : macro (f2) =  0.7279877250446614  : Epoch_runtime =  1.669386386871338  \n",
            "\n",
            "train_loss =  192.74893\n",
            "Validation  :: loss =  962.7437  : micro (f1) =  0.6817502986857825  : macro (f2) =  0.6712148185700718  : Epoch_runtime =  1.669386386871338  \n",
            "\n",
            "val_loss =  962.7437\n",
            "Weight Support =  [0.59723353 0.08478709 0.2105282  0.10745116]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 165 start !\n",
            "Training  :: loss =  193.1969  : micro (f1) =  0.7365473094618923  : macro (f2) =  0.7287932819688052  : Epoch_runtime =  1.6644923686981201  \n",
            "\n",
            "train_loss =  193.1969\n",
            "Validation  :: loss =  957.9573  : micro (f1) =  0.6803109392016223  : macro (f2) =  0.6691968309710639  : Epoch_runtime =  1.6644923686981201  \n",
            "\n",
            "val_loss =  957.9573\n",
            "Weight Support =  [0.5976727  0.08449164 0.21053563 0.10729995]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 166 start !\n",
            "Training  :: loss =  193.5984  : micro (f1) =  0.7358164572583773  : macro (f2) =  0.7280606647025125  : Epoch_runtime =  1.7007112503051758  \n",
            "\n",
            "train_loss =  193.5984\n",
            "Validation  :: loss =  959.33105  : micro (f1) =  0.6778688215081372  : macro (f2) =  0.6662650746916695  : Epoch_runtime =  1.7007112503051758  \n",
            "\n",
            "val_loss =  959.33105\n",
            "Weight Support =  [0.5987835  0.08406789 0.20981361 0.107335  ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 167 start !\n",
            "Training  :: loss =  193.79204  : micro (f1) =  0.7358279711559952  : macro (f2) =  0.7280688891226776  : Epoch_runtime =  1.7337474822998047  \n",
            "\n",
            "train_loss =  193.79204\n",
            "Validation  :: loss =  978.83215  : micro (f1) =  0.6761811023622047  : macro (f2) =  0.6649403666605893  : Epoch_runtime =  1.7337474822998047  \n",
            "\n",
            "val_loss =  978.83215\n",
            "Weight Support =  [0.5992871  0.08430482 0.20948787 0.10692021]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 168 start !\n",
            "Training  :: loss =  194.19682  : micro (f1) =  0.7362360476616586  : macro (f2) =  0.7284322764388166  : Epoch_runtime =  1.7224159240722656  \n",
            "\n",
            "train_loss =  194.19682\n",
            "Validation  :: loss =  963.3248  : micro (f1) =  0.6804504199148873  : macro (f2) =  0.6693991342968008  : Epoch_runtime =  1.7224159240722656  \n",
            "\n",
            "val_loss =  963.3248\n",
            "Weight Support =  [0.599012   0.08461267 0.20950495 0.10687044]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 169 start !\n",
            "Training  :: loss =  194.80223  : micro (f1) =  0.735825075905571  : macro (f2) =  0.7280217276746236  : Epoch_runtime =  1.6974656581878662  \n",
            "\n",
            "train_loss =  194.80223\n",
            "Validation  :: loss =  969.5706  : micro (f1) =  0.6813631762456417  : macro (f2) =  0.6706200145155725  : Epoch_runtime =  1.6974656581878662  \n",
            "\n",
            "val_loss =  969.5706\n",
            "Weight Support =  [0.5987729  0.08491796 0.20960872 0.10670047]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 170 start !\n",
            "Training  :: loss =  194.6299  : micro (f1) =  0.7367144639871912  : macro (f2) =  0.7289587091608463  : Epoch_runtime =  1.6798524856567383  \n",
            "\n",
            "train_loss =  194.6299\n",
            "Validation  :: loss =  980.2835  : micro (f1) =  0.6819422120883639  : macro (f2) =  0.6710318840257901  : Epoch_runtime =  1.6798524856567383  \n",
            "\n",
            "val_loss =  980.2835\n",
            "Weight Support =  [0.5982756  0.08500984 0.20980275 0.10691193]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 171 start !\n",
            "Training  :: loss =  196.48813  : micro (f1) =  0.7360869360681187  : macro (f2) =  0.7283274037877174  : Epoch_runtime =  1.6663057804107666  \n",
            "\n",
            "train_loss =  196.48813\n",
            "Validation  :: loss =  977.3856  : micro (f1) =  0.6812661160730968  : macro (f2) =  0.6704840649181072  : Epoch_runtime =  1.6663057804107666  \n",
            "\n",
            "val_loss =  977.3856\n",
            "Weight Support =  [0.5993653  0.08569978 0.20899229 0.10594261]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 172 start !\n",
            "Training  :: loss =  196.73276  : micro (f1) =  0.7365124957327338  : macro (f2) =  0.7287924644609058  : Epoch_runtime =  1.6823444366455078  \n",
            "\n",
            "train_loss =  196.73276\n",
            "Validation  :: loss =  948.49207  : micro (f1) =  0.6818554702065778  : macro (f2) =  0.6713233433239266  : Epoch_runtime =  1.6823444366455078  \n",
            "\n",
            "val_loss =  948.49207\n",
            "Weight Support =  [0.6004394  0.08551594 0.20834137 0.1057033 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 173 start !\n",
            "Training  :: loss =  196.67552  : micro (f1) =  0.7353055343174734  : macro (f2) =  0.7275252720269836  : Epoch_runtime =  1.716223955154419  \n",
            "\n",
            "train_loss =  196.67552\n",
            "Validation  :: loss =  949.24164  : micro (f1) =  0.6810057331284896  : macro (f2) =  0.6702676850141615  : Epoch_runtime =  1.716223955154419  \n",
            "\n",
            "val_loss =  949.24164\n",
            "Weight Support =  [0.60177493 0.08519831 0.20791508 0.10511162]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 174 start !\n",
            "Training  :: loss =  195.44965  : micro (f1) =  0.7360446101902286  : macro (f2) =  0.7283069167279744  : Epoch_runtime =  1.694854736328125  \n",
            "\n",
            "train_loss =  195.44965\n",
            "Validation  :: loss =  963.33954  : micro (f1) =  0.6796942655973494  : macro (f2) =  0.6685577216175063  : Epoch_runtime =  1.694854736328125  \n",
            "\n",
            "val_loss =  963.33954\n",
            "Weight Support =  [0.60216427 0.08518396 0.20715444 0.10549725]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 175 start !\n",
            "Training  :: loss =  194.66806  : micro (f1) =  0.735965562259624  : macro (f2) =  0.7282011836319013  : Epoch_runtime =  1.6966373920440674  \n",
            "\n",
            "train_loss =  194.66806\n",
            "Validation  :: loss =  972.6759  : micro (f1) =  0.6779943867101571  : macro (f2) =  0.6668246983362492  : Epoch_runtime =  1.6966373920440674  \n",
            "\n",
            "val_loss =  972.6759\n",
            "Weight Support =  [0.602154   0.08568156 0.20695075 0.10521372]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 176 start !\n",
            "Training  :: loss =  193.83345  : micro (f1) =  0.7354976676419097  : macro (f2) =  0.7277480382976461  : Epoch_runtime =  1.6689903736114502  \n",
            "\n",
            "train_loss =  193.83345\n",
            "Validation  :: loss =  973.5319  : micro (f1) =  0.6784321151297104  : macro (f2) =  0.6674814692454683  : Epoch_runtime =  1.6689903736114502  \n",
            "\n",
            "val_loss =  973.5319\n",
            "Weight Support =  [0.6013872  0.08612539 0.20715408 0.10533336]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 177 start !\n",
            "Training  :: loss =  192.83658  : micro (f1) =  0.7369487134193018  : macro (f2) =  0.7291841231016516  : Epoch_runtime =  1.6744210720062256  \n",
            "\n",
            "train_loss =  192.83658\n",
            "Validation  :: loss =  976.16016  : micro (f1) =  0.6805832925129055  : macro (f2) =  0.6698326818093756  : Epoch_runtime =  1.6744210720062256  \n",
            "\n",
            "val_loss =  976.16016\n",
            "Weight Support =  [0.6016207  0.0862043  0.20718758 0.10498742]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 178 start !\n",
            "Training  :: loss =  191.83759  : micro (f1) =  0.7359546598311503  : macro (f2) =  0.7282002499020994  : Epoch_runtime =  1.673619270324707  \n",
            "\n",
            "train_loss =  191.83759\n",
            "Validation  :: loss =  962.9778  : micro (f1) =  0.68108794739595  : macro (f2) =  0.6703768383478721  : Epoch_runtime =  1.673619270324707  \n",
            "\n",
            "val_loss =  962.9778\n",
            "Weight Support =  [0.60180336 0.08640625 0.20702192 0.10476846]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 179 start !\n",
            "Training  :: loss =  191.52356  : micro (f1) =  0.7364418651190715  : macro (f2) =  0.7286602007936717  : Epoch_runtime =  1.7155861854553223  \n",
            "\n",
            "train_loss =  191.52356\n",
            "Validation  :: loss =  946.2063  : micro (f1) =  0.684010152284264  : macro (f2) =  0.672869757459182  : Epoch_runtime =  1.7155861854553223  \n",
            "\n",
            "val_loss =  946.2063\n",
            "Weight Support =  [0.60294026 0.08577238 0.20686936 0.10441802]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 180 start !\n",
            "Training  :: loss =  191.42941  : micro (f1) =  0.7354946398344933  : macro (f2) =  0.7277766718661479  : Epoch_runtime =  1.7163810729980469  \n",
            "\n",
            "train_loss =  191.42941\n",
            "Validation  :: loss =  956.07367  : micro (f1) =  0.6839416874836882  : macro (f2) =  0.6732594656353221  : Epoch_runtime =  1.7163810729980469  \n",
            "\n",
            "val_loss =  956.07367\n",
            "Weight Support =  [0.6036596  0.08555012 0.20625953 0.10453083]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 181 start !\n",
            "Training  :: loss =  191.50328  : micro (f1) =  0.7369053368040424  : macro (f2) =  0.7291257235558012  : Epoch_runtime =  1.6666035652160645  \n",
            "\n",
            "train_loss =  191.50328\n",
            "Validation  :: loss =  983.7177  : micro (f1) =  0.6798088860464242  : macro (f2) =  0.6694406160989138  : Epoch_runtime =  1.6666035652160645  \n",
            "\n",
            "val_loss =  983.7177\n",
            "Weight Support =  [0.60504645 0.08533783 0.20566478 0.10395087]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 182 start !\n",
            "Training  :: loss =  191.87541  : micro (f1) =  0.7352395708617977  : macro (f2) =  0.7275085190056053  : Epoch_runtime =  1.6863420009613037  \n",
            "\n",
            "train_loss =  191.87541\n",
            "Validation  :: loss =  984.5175  : micro (f1) =  0.6764683534685814  : macro (f2) =  0.6655058026969792  : Epoch_runtime =  1.6863420009613037  \n",
            "\n",
            "val_loss =  984.5175\n",
            "Weight Support =  [0.60497016 0.08533619 0.20567296 0.10402066]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 183 start !\n",
            "Training  :: loss =  191.93591  : micro (f1) =  0.7369362794147076  : macro (f2) =  0.7291724933598642  : Epoch_runtime =  1.6593451499938965  \n",
            "\n",
            "train_loss =  191.93591\n",
            "Validation  :: loss =  992.50134  : micro (f1) =  0.6762672811059907  : macro (f2) =  0.6652009604530806  : Epoch_runtime =  1.6593451499938965  \n",
            "\n",
            "val_loss =  992.50134\n",
            "Weight Support =  [0.6045216  0.08564868 0.20587523 0.10395455]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 184 start !\n",
            "Training  :: loss =  193.12111  : micro (f1) =  0.7342431674431158  : macro (f2) =  0.7264917189992001  : Epoch_runtime =  1.7314021587371826  \n",
            "\n",
            "train_loss =  193.12111\n",
            "Validation  :: loss =  953.3412  : micro (f1) =  0.6805691485357223  : macro (f2) =  0.6691595052206736  : Epoch_runtime =  1.7314021587371826  \n",
            "\n",
            "val_loss =  953.3412\n",
            "Weight Support =  [0.6044217  0.08586907 0.20607266 0.10363651]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 185 start !\n",
            "Training  :: loss =  193.80426  : micro (f1) =  0.7368607117605468  : macro (f2) =  0.7290972132119825  : Epoch_runtime =  1.6934998035430908  \n",
            "\n",
            "train_loss =  193.80426\n",
            "Validation  :: loss =  953.2815  : micro (f1) =  0.6820870220380486  : macro (f2) =  0.6702849786366527  : Epoch_runtime =  1.6934998035430908  \n",
            "\n",
            "val_loss =  953.2815\n",
            "Weight Support =  [0.6040687  0.08592775 0.20658736 0.10341625]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 186 start !\n",
            "Training  :: loss =  194.31042  : micro (f1) =  0.7348898357807749  : macro (f2) =  0.727163501021266  : Epoch_runtime =  1.7172062397003174  \n",
            "\n",
            "train_loss =  194.31042\n",
            "Validation  :: loss =  950.90216  : micro (f1) =  0.6819069069069069  : macro (f2) =  0.6706416587873913  : Epoch_runtime =  1.7172062397003174  \n",
            "\n",
            "val_loss =  950.90216\n",
            "Weight Support =  [0.6053374  0.08544201 0.2061154  0.10310531]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 187 start !\n",
            "Training  :: loss =  194.50395  : micro (f1) =  0.7379195542070529  : macro (f2) =  0.7301215461361223  : Epoch_runtime =  1.6654255390167236  \n",
            "\n",
            "train_loss =  194.50395\n",
            "Validation  :: loss =  983.20996  : micro (f1) =  0.6830358806214837  : macro (f2) =  0.6721820790250119  : Epoch_runtime =  1.6654255390167236  \n",
            "\n",
            "val_loss =  983.20996\n",
            "Weight Support =  [0.6068176  0.08540847 0.20485424 0.10291968]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 188 start !\n",
            "Training  :: loss =  194.59076  : micro (f1) =  0.7359214615312177  : macro (f2) =  0.7281499910191488  : Epoch_runtime =  1.694512128829956  \n",
            "\n",
            "train_loss =  194.59076\n",
            "Validation  :: loss =  998.0509  : micro (f1) =  0.6804154413133305  : macro (f2) =  0.6700824834871477  : Epoch_runtime =  1.694512128829956  \n",
            "\n",
            "val_loss =  998.0509\n",
            "Weight Support =  [0.60715586 0.0856091  0.20473224 0.10250276]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 189 start !\n",
            "Training  :: loss =  196.2074  : micro (f1) =  0.7375492021590025  : macro (f2) =  0.7298036884121  : Epoch_runtime =  1.6819863319396973  \n",
            "\n",
            "train_loss =  196.2074\n",
            "Validation  :: loss =  995.69617  : micro (f1) =  0.6784665210514423  : macro (f2) =  0.6678613127696206  : Epoch_runtime =  1.6819863319396973  \n",
            "\n",
            "val_loss =  995.69617\n",
            "Weight Support =  [0.6069444  0.08532124 0.20512003 0.10261437]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 190 start !\n",
            "Training  :: loss =  196.34273  : micro (f1) =  0.7342960034721816  : macro (f2) =  0.7265811211931958  : Epoch_runtime =  1.7236080169677734  \n",
            "\n",
            "train_loss =  196.34273\n",
            "Validation  :: loss =  962.2702  : micro (f1) =  0.6795285639216275  : macro (f2) =  0.6690538248809458  : Epoch_runtime =  1.7236080169677734  \n",
            "\n",
            "val_loss =  962.2702\n",
            "Weight Support =  [0.60592645 0.08613814 0.20532955 0.10260593]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 191 start !\n",
            "Training  :: loss =  197.98965  : micro (f1) =  0.7360496847652207  : macro (f2) =  0.728266231614089  : Epoch_runtime =  1.7108583450317383  \n",
            "\n",
            "train_loss =  197.98965\n",
            "Validation  :: loss =  952.94  : micro (f1) =  0.6796263378843462  : macro (f2) =  0.6685171322759582  : Epoch_runtime =  1.7108583450317383  \n",
            "\n",
            "val_loss =  952.94\n",
            "Weight Support =  [0.60696733 0.08587288 0.2051875  0.1019723 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 192 start !\n",
            "Training  :: loss =  197.87076  : micro (f1) =  0.734774944176754  : macro (f2) =  0.7270028863650874  : Epoch_runtime =  1.7254610061645508  \n",
            "\n",
            "train_loss =  197.87076\n",
            "Validation  :: loss =  964.1064  : micro (f1) =  0.6766366549671905  : macro (f2) =  0.6652175992237043  : Epoch_runtime =  1.7254610061645508  \n",
            "\n",
            "val_loss =  964.1064\n",
            "Weight Support =  [0.6064439  0.08687054 0.2051559  0.10152964]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 193 start !\n",
            "Training  :: loss =  196.12129  : micro (f1) =  0.7356243460267821  : macro (f2) =  0.7278729309585978  : Epoch_runtime =  1.7221291065216064  \n",
            "\n",
            "train_loss =  196.12129\n",
            "Validation  :: loss =  944.48627  : micro (f1) =  0.6797573919636087  : macro (f2) =  0.6679042059428504  : Epoch_runtime =  1.7221291065216064  \n",
            "\n",
            "val_loss =  944.48627\n",
            "Weight Support =  [0.60667413 0.0866828  0.20492698 0.10171612]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 194 start !\n",
            "Training  :: loss =  194.95961  : micro (f1) =  0.7365915731925723  : macro (f2) =  0.7288280039185312  : Epoch_runtime =  1.694375991821289  \n",
            "\n",
            "train_loss =  194.95961\n",
            "Validation  :: loss =  967.4901  : micro (f1) =  0.6823758958388053  : macro (f2) =  0.6710490684212648  : Epoch_runtime =  1.694375991821289  \n",
            "\n",
            "val_loss =  967.4901\n",
            "Weight Support =  [0.60710883 0.08634102 0.20477076 0.10177942]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 195 start !\n",
            "Training  :: loss =  194.04025  : micro (f1) =  0.736709457072194  : macro (f2) =  0.7289243440696278  : Epoch_runtime =  1.7098827362060547  \n",
            "\n",
            "train_loss =  194.04025\n",
            "Validation  :: loss =  1001.54156  : micro (f1) =  0.6840482822655525  : macro (f2) =  0.6735451996410243  : Epoch_runtime =  1.7098827362060547  \n",
            "\n",
            "val_loss =  1001.54156\n",
            "Weight Support =  [0.60678893 0.08704583 0.204749   0.10141619]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 196 start !\n",
            "Training  :: loss =  194.0056  : micro (f1) =  0.7363132566977679  : macro (f2) =  0.7285715429615389  : Epoch_runtime =  1.679973840713501  \n",
            "\n",
            "train_loss =  194.0056\n",
            "Validation  :: loss =  972.7513  : micro (f1) =  0.6843065014327714  : macro (f2) =  0.6738708708522125  : Epoch_runtime =  1.679973840713501  \n",
            "\n",
            "val_loss =  972.7513\n",
            "Weight Support =  [0.6071856  0.0865976  0.20499456 0.10122223]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 197 start !\n",
            "Training  :: loss =  194.09686  : micro (f1) =  0.7368892134116899  : macro (f2) =  0.7291406410012746  : Epoch_runtime =  1.6895570755004883  \n",
            "\n",
            "train_loss =  194.09686\n",
            "Validation  :: loss =  964.2435  : micro (f1) =  0.6811968692656256  : macro (f2) =  0.6709110494041487  : Epoch_runtime =  1.6895570755004883  \n",
            "\n",
            "val_loss =  964.2435\n",
            "Weight Support =  [0.6082288  0.08662806 0.20429642 0.10084681]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 198 start !\n",
            "Training  :: loss =  193.8966  : micro (f1) =  0.7353926049511814  : macro (f2) =  0.7276570622519026  : Epoch_runtime =  1.6910202503204346  \n",
            "\n",
            "train_loss =  193.8966\n",
            "Validation  :: loss =  980.39343  : micro (f1) =  0.6758861745374416  : macro (f2) =  0.6652149880195732  : Epoch_runtime =  1.6910202503204346  \n",
            "\n",
            "val_loss =  980.39343\n",
            "Weight Support =  [0.6088574  0.08671353 0.20395452 0.10047469]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 199 start !\n",
            "Training  :: loss =  193.66803  : micro (f1) =  0.7355201015061444  : macro (f2) =  0.7277765706622902  : Epoch_runtime =  1.686678409576416  \n",
            "\n",
            "train_loss =  193.66803\n",
            "Validation  :: loss =  996.9971  : micro (f1) =  0.6722889710412815  : macro (f2) =  0.6608817447754376  : Epoch_runtime =  1.686678409576416  \n",
            "\n",
            "val_loss =  996.9971\n",
            "Weight Support =  [0.6085423  0.08673807 0.20440856 0.10031108]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 200 start !\n",
            "Training  :: loss =  194.22536  : micro (f1) =  0.7342217634085684  : macro (f2) =  0.7264966078259909  : Epoch_runtime =  1.6843631267547607  \n",
            "\n",
            "train_loss =  194.22536\n",
            "Validation  :: loss =  958.9114  : micro (f1) =  0.6789954337899544  : macro (f2) =  0.6670322360383537  : Epoch_runtime =  1.6843631267547607  \n",
            "\n",
            "val_loss =  958.9114\n",
            "Weight Support =  [0.60818547 0.08714189 0.20437565 0.10029695]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 201 start !\n",
            "Training  :: loss =  194.57355  : micro (f1) =  0.736528567560249  : macro (f2) =  0.7287776393717241  : Epoch_runtime =  1.7707464694976807  \n",
            "\n",
            "train_loss =  194.57355\n",
            "Validation  :: loss =  963.9807  : micro (f1) =  0.6797167846730529  : macro (f2) =  0.6676738339249357  : Epoch_runtime =  1.7707464694976807  \n",
            "\n",
            "val_loss =  963.9807\n",
            "Weight Support =  [0.60879827 0.08691037 0.2040523  0.10023903]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 202 start !\n",
            "Training  :: loss =  193.93736  : micro (f1) =  0.736911549879836  : macro (f2) =  0.7291206957692611  : Epoch_runtime =  1.7022387981414795  \n",
            "\n",
            "train_loss =  193.93736\n",
            "Validation  :: loss =  975.57947  : micro (f1) =  0.6849345710770607  : macro (f2) =  0.6738527115891052  : Epoch_runtime =  1.7022387981414795  \n",
            "\n",
            "val_loss =  975.57947\n",
            "Weight Support =  [0.60934085 0.08681867 0.20379189 0.10004856]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 203 start !\n",
            "Training  :: loss =  193.52606  : micro (f1) =  0.7375272684393609  : macro (f2) =  0.7297413170247582  : Epoch_runtime =  1.671628713607788  \n",
            "\n",
            "train_loss =  193.52606\n",
            "Validation  :: loss =  990.82837  : micro (f1) =  0.6842830027168856  : macro (f2) =  0.6737332158491776  : Epoch_runtime =  1.671628713607788  \n",
            "\n",
            "val_loss =  990.82837\n",
            "Weight Support =  [0.6101646  0.08681316 0.20343359 0.09958861]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 204 start !\n",
            "Training  :: loss =  194.39967  : micro (f1) =  0.7363327014329616  : macro (f2) =  0.728557358654984  : Epoch_runtime =  1.6850111484527588  \n",
            "\n",
            "train_loss =  194.39967\n",
            "Validation  :: loss =  990.4711  : micro (f1) =  0.6809516661663164  : macro (f2) =  0.6707241932023444  : Epoch_runtime =  1.6850111484527588  \n",
            "\n",
            "val_loss =  990.4711\n",
            "Weight Support =  [0.6104403  0.08662574 0.20360422 0.09932975]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 205 start !\n",
            "Training  :: loss =  194.87523  : micro (f1) =  0.7366228018550344  : macro (f2) =  0.7289021877376588  : Epoch_runtime =  1.6936874389648438  \n",
            "\n",
            "train_loss =  194.87523\n",
            "Validation  :: loss =  967.86237  : micro (f1) =  0.6771617957216028  : macro (f2) =  0.6666971226877796  : Epoch_runtime =  1.6936874389648438  \n",
            "\n",
            "val_loss =  967.86237\n",
            "Weight Support =  [0.6110266  0.08639243 0.20348114 0.09909982]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 206 start !\n",
            "Training  :: loss =  194.89906  : micro (f1) =  0.7346301492081753  : macro (f2) =  0.7269059287061402  : Epoch_runtime =  1.7199528217315674  \n",
            "\n",
            "train_loss =  194.89906\n",
            "Validation  :: loss =  968.6425  : micro (f1) =  0.6759072388713301  : macro (f2) =  0.664585509731286  : Epoch_runtime =  1.7199528217315674  \n",
            "\n",
            "val_loss =  968.6425\n",
            "Weight Support =  [0.61116725 0.08674679 0.20337853 0.09870742]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 207 start !\n",
            "Training  :: loss =  194.7684  : micro (f1) =  0.7356891871246753  : macro (f2) =  0.7279413046811883  : Epoch_runtime =  1.6972801685333252  \n",
            "\n",
            "train_loss =  194.7684\n",
            "Validation  :: loss =  972.19617  : micro (f1) =  0.6755436856161771  : macro (f2) =  0.6637281095469113  : Epoch_runtime =  1.6972801685333252  \n",
            "\n",
            "val_loss =  972.19617\n",
            "Weight Support =  [0.61155146 0.08698183 0.20296183 0.09850486]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 208 start !\n",
            "Training  :: loss =  196.09174  : micro (f1) =  0.7359109783209626  : macro (f2) =  0.7281869593691129  : Epoch_runtime =  1.708385944366455  \n",
            "\n",
            "train_loss =  196.09174\n",
            "Validation  :: loss =  954.1516  : micro (f1) =  0.6813627254509018  : macro (f2) =  0.6697610933253034  : Epoch_runtime =  1.708385944366455  \n",
            "\n",
            "val_loss =  954.1516\n",
            "Weight Support =  [0.6120414  0.08743162 0.20220998 0.09831699]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 209 start !\n",
            "Training  :: loss =  194.21255  : micro (f1) =  0.7366400075383692  : macro (f2) =  0.7288616281549589  : Epoch_runtime =  1.6898424625396729  \n",
            "\n",
            "train_loss =  194.21255\n",
            "Validation  :: loss =  971.2048  : micro (f1) =  0.6845075686787517  : macro (f2) =  0.6733241842097104  : Epoch_runtime =  1.6898424625396729  \n",
            "\n",
            "val_loss =  971.2048\n",
            "Weight Support =  [0.6133183  0.08662025 0.20173521 0.09832619]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 210 start !\n",
            "Training  :: loss =  192.69153  : micro (f1) =  0.7372764462468906  : macro (f2) =  0.729488782794354  : Epoch_runtime =  1.6700522899627686  \n",
            "\n",
            "train_loss =  192.69153\n",
            "Validation  :: loss =  996.73737  : micro (f1) =  0.68538741549662  : macro (f2) =  0.6747373553708887  : Epoch_runtime =  1.6700522899627686  \n",
            "\n",
            "val_loss =  996.73737\n",
            "Weight Support =  [0.6131831  0.08717932 0.20167367 0.09796393]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 211 start !\n",
            "Training  :: loss =  192.5682  : micro (f1) =  0.7363083402943633  : macro (f2) =  0.7285893722333684  : Epoch_runtime =  1.7116632461547852  \n",
            "\n",
            "train_loss =  192.5682\n",
            "Validation  :: loss =  987.39044  : micro (f1) =  0.681865828092243  : macro (f2) =  0.6713090810338205  : Epoch_runtime =  1.7116632461547852  \n",
            "\n",
            "val_loss =  987.39044\n",
            "Weight Support =  [0.613045   0.08710513 0.20210068 0.09774924]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 212 start !\n",
            "Training  :: loss =  192.6414  : micro (f1) =  0.7361119275762742  : macro (f2) =  0.7283910300542996  : Epoch_runtime =  1.7087581157684326  \n",
            "\n",
            "train_loss =  192.6414\n",
            "Validation  :: loss =  964.2712  : micro (f1) =  0.6795852968897267  : macro (f2) =  0.6690345182841247  : Epoch_runtime =  1.7087581157684326  \n",
            "\n",
            "val_loss =  964.2712\n",
            "Weight Support =  [0.61356616 0.08685239 0.20197207 0.09760937]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 213 start !\n",
            "Training  :: loss =  192.60333  : micro (f1) =  0.7355505934892467  : macro (f2) =  0.7278124863508395  : Epoch_runtime =  1.7149131298065186  \n",
            "\n",
            "train_loss =  192.60333\n",
            "Validation  :: loss =  957.3937  : micro (f1) =  0.6752110747699095  : macro (f2) =  0.6644077867817982  : Epoch_runtime =  1.7149131298065186  \n",
            "\n",
            "val_loss =  957.3937\n",
            "Weight Support =  [0.61422783 0.08662479 0.2018996  0.09724771]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 214 start !\n",
            "Training  :: loss =  192.75595  : micro (f1) =  0.7357326659381719  : macro (f2) =  0.7279645664762825  : Epoch_runtime =  1.6871027946472168  \n",
            "\n",
            "train_loss =  192.75595\n",
            "Validation  :: loss =  973.93933  : micro (f1) =  0.6773874695863747  : macro (f2) =  0.6656842110448654  : Epoch_runtime =  1.6871027946472168  \n",
            "\n",
            "val_loss =  973.93933\n",
            "Weight Support =  [0.6149203  0.08673345 0.20122266 0.09712362]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 215 start !\n",
            "Training  :: loss =  192.63374  : micro (f1) =  0.7364655770882605  : macro (f2) =  0.7287193022065568  : Epoch_runtime =  1.6721668243408203  \n",
            "\n",
            "train_loss =  192.63374\n",
            "Validation  :: loss =  971.7755  : micro (f1) =  0.6787566728504904  : macro (f2) =  0.6674955624558104  : Epoch_runtime =  1.6721668243408203  \n",
            "\n",
            "val_loss =  971.7755\n",
            "Weight Support =  [0.6147917  0.08708167 0.20109724 0.09702939]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 216 start !\n",
            "Training  :: loss =  192.32901  : micro (f1) =  0.7361209210340444  : macro (f2) =  0.7283664584744889  : Epoch_runtime =  1.6832339763641357  \n",
            "\n",
            "train_loss =  192.32901\n",
            "Validation  :: loss =  980.08453  : micro (f1) =  0.681150159744409  : macro (f2) =  0.6697816683278163  : Epoch_runtime =  1.6832339763641357  \n",
            "\n",
            "val_loss =  980.08453\n",
            "Weight Support =  [0.6150058  0.08709723 0.20076627 0.09713067]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 217 start !\n",
            "Training  :: loss =  191.5817  : micro (f1) =  0.7373135031937209  : macro (f2) =  0.7295129512581059  : Epoch_runtime =  1.6910572052001953  \n",
            "\n",
            "train_loss =  191.5817\n",
            "Validation  :: loss =  992.1804  : micro (f1) =  0.6842380738746823  : macro (f2) =  0.6733997093873614  : Epoch_runtime =  1.6910572052001953  \n",
            "\n",
            "val_loss =  992.1804\n",
            "Weight Support =  [0.6152471  0.08712993 0.20089944 0.09672368]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 218 start !\n",
            "Training  :: loss =  191.9727  : micro (f1) =  0.736631472391916  : macro (f2) =  0.7288925435871006  : Epoch_runtime =  1.7514827251434326  \n",
            "\n",
            "train_loss =  191.9727\n",
            "Validation  :: loss =  969.4872  : micro (f1) =  0.6821246628708421  : macro (f2) =  0.6719504631290538  : Epoch_runtime =  1.7514827251434326  \n",
            "\n",
            "val_loss =  969.4872\n",
            "Weight Support =  [0.6152736  0.08706838 0.20126949 0.09638856]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 219 start !\n",
            "Training  :: loss =  191.83295  : micro (f1) =  0.7363269531663589  : macro (f2) =  0.7286172573364594  : Epoch_runtime =  1.6931695938110352  \n",
            "\n",
            "train_loss =  191.83295\n",
            "Validation  :: loss =  959.5831  : micro (f1) =  0.677810975379146  : macro (f2) =  0.6676699288644883  : Epoch_runtime =  1.6931695938110352  \n",
            "\n",
            "val_loss =  959.5831\n",
            "Weight Support =  [0.61567026 0.08683369 0.20137501 0.09612107]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 220 start !\n",
            "Training  :: loss =  191.58247  : micro (f1) =  0.7353462753526171  : macro (f2) =  0.7276285426743602  : Epoch_runtime =  1.6883459091186523  \n",
            "\n",
            "train_loss =  191.58247\n",
            "Validation  :: loss =  964.65985  : micro (f1) =  0.6771868598740612  : macro (f2) =  0.6659009091834374  : Epoch_runtime =  1.6883459091186523  \n",
            "\n",
            "val_loss =  964.65985\n",
            "Weight Support =  [0.61632365 0.08655071 0.20110063 0.09602504]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 221 start !\n",
            "Training  :: loss =  191.47313  : micro (f1) =  0.7359841936280562  : macro (f2) =  0.7282223288275373  : Epoch_runtime =  1.684441328048706  \n",
            "\n",
            "train_loss =  191.47313\n",
            "Validation  :: loss =  978.495  : micro (f1) =  0.6774855579203405  : macro (f2) =  0.6659606292668736  : Epoch_runtime =  1.684441328048706  \n",
            "\n",
            "val_loss =  978.495\n",
            "Weight Support =  [0.61656827 0.0867272  0.20075352 0.09595098]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 222 start !\n",
            "Training  :: loss =  191.59198  : micro (f1) =  0.7356813693219224  : macro (f2) =  0.7279311502501911  : Epoch_runtime =  1.706855297088623  \n",
            "\n",
            "train_loss =  191.59198\n",
            "Validation  :: loss =  984.5147  : micro (f1) =  0.67832751555623  : macro (f2) =  0.6663418163255683  : Epoch_runtime =  1.706855297088623  \n",
            "\n",
            "val_loss =  984.5147\n",
            "Weight Support =  [0.61615616 0.0872486  0.20061904 0.09597616]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 223 start !\n",
            "Training  :: loss =  191.46684  : micro (f1) =  0.7368123520474862  : macro (f2) =  0.7290474698093367  : Epoch_runtime =  1.6946742534637451  \n",
            "\n",
            "train_loss =  191.46684\n",
            "Validation  :: loss =  985.5764  : micro (f1) =  0.6823317443038925  : macro (f2) =  0.671448143080847  : Epoch_runtime =  1.6946742534637451  \n",
            "\n",
            "val_loss =  985.5764\n",
            "Weight Support =  [0.6164442  0.08727647 0.20043387 0.09584552]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 224 start !\n",
            "Training  :: loss =  190.80096  : micro (f1) =  0.7371322879728568  : macro (f2) =  0.7293988025074479  : Epoch_runtime =  1.6809356212615967  \n",
            "\n",
            "train_loss =  190.80096\n",
            "Validation  :: loss =  974.3127  : micro (f1) =  0.6829268292682926  : macro (f2) =  0.6727469802698001  : Epoch_runtime =  1.6809356212615967  \n",
            "\n",
            "val_loss =  974.3127\n",
            "Weight Support =  [0.6166501  0.08745331 0.20046191 0.09543472]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 225 start !\n",
            "Training  :: loss =  191.19989  : micro (f1) =  0.7364567392583873  : macro (f2) =  0.7286918237901919  : Epoch_runtime =  1.6692490577697754  \n",
            "\n",
            "train_loss =  191.19989\n",
            "Validation  :: loss =  964.984  : micro (f1) =  0.6819030547464335  : macro (f2) =  0.6714189791780947  : Epoch_runtime =  1.6692490577697754  \n",
            "\n",
            "val_loss =  964.984\n",
            "Weight Support =  [0.6170702  0.0872828  0.2004234  0.09522368]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 226 start !\n",
            "Training  :: loss =  191.56229  : micro (f1) =  0.7365349772935835  : macro (f2) =  0.7288477166716232  : Epoch_runtime =  1.7073009014129639  \n",
            "\n",
            "train_loss =  191.56229\n",
            "Validation  :: loss =  963.61176  : micro (f1) =  0.6801592069690597  : macro (f2) =  0.6698513650105113  : Epoch_runtime =  1.7073009014129639  \n",
            "\n",
            "val_loss =  963.61176\n",
            "Weight Support =  [0.6178556  0.08675531 0.20029673 0.09509237]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 227 start !\n",
            "Training  :: loss =  191.93901  : micro (f1) =  0.7352806455022727  : macro (f2) =  0.7275262220022085  : Epoch_runtime =  1.6887826919555664  \n",
            "\n",
            "train_loss =  191.93901\n",
            "Validation  :: loss =  971.4573  : micro (f1) =  0.6783630162940508  : macro (f2) =  0.6668068189352397  : Epoch_runtime =  1.6887826919555664  \n",
            "\n",
            "val_loss =  971.4573\n",
            "Weight Support =  [0.61838835 0.08684838 0.19993518 0.09482811]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 228 start !\n",
            "Training  :: loss =  191.48055  : micro (f1) =  0.7359501704078035  : macro (f2) =  0.7282156688285167  : Epoch_runtime =  1.6933577060699463  \n",
            "\n",
            "train_loss =  191.48055\n",
            "Validation  :: loss =  982.14844  : micro (f1) =  0.6788521712584904  : macro (f2) =  0.6671159599195868  : Epoch_runtime =  1.6933577060699463  \n",
            "\n",
            "val_loss =  982.14844\n",
            "Weight Support =  [0.61798    0.08701158 0.20015478 0.09485354]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 229 start !\n",
            "Training  :: loss =  191.2588  : micro (f1) =  0.735662197130087  : macro (f2) =  0.7278982914338479  : Epoch_runtime =  1.7171382904052734  \n",
            "\n",
            "train_loss =  191.2588\n",
            "Validation  :: loss =  969.2315  : micro (f1) =  0.6785089387599849  : macro (f2) =  0.6671269244054931  : Epoch_runtime =  1.7171382904052734  \n",
            "\n",
            "val_loss =  969.2315\n",
            "Weight Support =  [0.61717814 0.08772522 0.20016159 0.09493513]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 230 start !\n",
            "Training  :: loss =  191.17218  : micro (f1) =  0.736983429708754  : macro (f2) =  0.7292465750364892  : Epoch_runtime =  1.695544958114624  \n",
            "\n",
            "train_loss =  191.17218\n",
            "Validation  :: loss =  959.52747  : micro (f1) =  0.6803155522163787  : macro (f2) =  0.6695005216529808  : Epoch_runtime =  1.695544958114624  \n",
            "\n",
            "val_loss =  959.52747\n",
            "Weight Support =  [0.6175439  0.08769544 0.19996095 0.09479971]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 231 start !\n",
            "Training  :: loss =  190.4349  : micro (f1) =  0.7368892345033167  : macro (f2) =  0.7291189831893478  : Epoch_runtime =  1.666851282119751  \n",
            "\n",
            "train_loss =  190.4349\n",
            "Validation  :: loss =  969.3971  : micro (f1) =  0.6841146511022715  : macro (f2) =  0.6735403163048217  : Epoch_runtime =  1.666851282119751  \n",
            "\n",
            "val_loss =  969.3971\n",
            "Weight Support =  [0.6185032  0.08753332 0.19957642 0.09438708]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 232 start !\n",
            "Training  :: loss =  190.61273  : micro (f1) =  0.736370056497175  : macro (f2) =  0.7286568533769547  : Epoch_runtime =  1.6716382503509521  \n",
            "\n",
            "train_loss =  190.61273\n",
            "Validation  :: loss =  982.201  : micro (f1) =  0.6833960997800067  : macro (f2) =  0.6727960123894514  : Epoch_runtime =  1.6716382503509521  \n",
            "\n",
            "val_loss =  982.201\n",
            "Weight Support =  [0.6188473  0.08749215 0.19947399 0.09418645]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 233 start !\n",
            "Training  :: loss =  191.3627  : micro (f1) =  0.7355800312672646  : macro (f2) =  0.727829613553993  : Epoch_runtime =  1.6843769550323486  \n",
            "\n",
            "train_loss =  191.3627\n",
            "Validation  :: loss =  977.39453  : micro (f1) =  0.6810762129911065  : macro (f2) =  0.6701643834737195  : Epoch_runtime =  1.6843769550323486  \n",
            "\n",
            "val_loss =  977.39453\n",
            "Weight Support =  [0.61836827 0.08756505 0.19980288 0.09426392]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 234 start !\n",
            "Training  :: loss =  190.86536  : micro (f1) =  0.7364546213644757  : macro (f2) =  0.7287220586622598  : Epoch_runtime =  1.7187016010284424  \n",
            "\n",
            "train_loss =  190.86536\n",
            "Validation  :: loss =  988.8172  : micro (f1) =  0.6754100867969257  : macro (f2) =  0.6641432656133632  : Epoch_runtime =  1.7187016010284424  \n",
            "\n",
            "val_loss =  988.8172\n",
            "Weight Support =  [0.6184829  0.08734236 0.2000447  0.09413003]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 235 start !\n",
            "Training  :: loss =  191.16643  : micro (f1) =  0.7344092963202066  : macro (f2) =  0.7266893094521316  : Epoch_runtime =  1.7631714344024658  \n",
            "\n",
            "train_loss =  191.16643\n",
            "Validation  :: loss =  989.15845  : micro (f1) =  0.673643054277829  : macro (f2) =  0.6619956301637662  : Epoch_runtime =  1.7631714344024658  \n",
            "\n",
            "val_loss =  989.15845\n",
            "Weight Support =  [0.6183729  0.08749928 0.2000001  0.09412763]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 236 start !\n",
            "Training  :: loss =  192.40675  : micro (f1) =  0.7364927682911042  : macro (f2) =  0.7287658837919075  : Epoch_runtime =  1.705796718597412  \n",
            "\n",
            "train_loss =  192.40675\n",
            "Validation  :: loss =  961.9602  : micro (f1) =  0.678246359223301  : macro (f2) =  0.6668401659715051  : Epoch_runtime =  1.705796718597412  \n",
            "\n",
            "val_loss =  961.9602\n",
            "Weight Support =  [0.6190016  0.08791427 0.1993252  0.09375889]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 237 start !\n",
            "Training  :: loss =  193.26564  : micro (f1) =  0.7360769692193693  : macro (f2) =  0.7283413844158495  : Epoch_runtime =  1.7177841663360596  \n",
            "\n",
            "train_loss =  193.26564\n",
            "Validation  :: loss =  954.85254  : micro (f1) =  0.6802237677653463  : macro (f2) =  0.6692185934390447  : Epoch_runtime =  1.7177841663360596  \n",
            "\n",
            "val_loss =  954.85254\n",
            "Weight Support =  [0.6190173  0.0880488  0.19917592 0.09375795]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 238 start !\n",
            "Training  :: loss =  192.24507  : micro (f1) =  0.737433987174651  : macro (f2) =  0.7296248031666724  : Epoch_runtime =  1.6898181438446045  \n",
            "\n",
            "train_loss =  192.24507\n",
            "Validation  :: loss =  998.1308  : micro (f1) =  0.6839037513077267  : macro (f2) =  0.6729140775195299  : Epoch_runtime =  1.6898181438446045  \n",
            "\n",
            "val_loss =  998.1308\n",
            "Weight Support =  [0.6199446  0.08772907 0.19875115 0.09357523]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 239 start !\n",
            "Training  :: loss =  191.96436  : micro (f1) =  0.7373694509276941  : macro (f2) =  0.7295835449689869  : Epoch_runtime =  1.6848680973052979  \n",
            "\n",
            "train_loss =  191.96436\n",
            "Validation  :: loss =  1021.4519  : micro (f1) =  0.6855121042830541  : macro (f2) =  0.674965846168285  : Epoch_runtime =  1.6848680973052979  \n",
            "\n",
            "val_loss =  1021.4519\n",
            "Weight Support =  [0.62012154 0.0881793  0.19848704 0.09321214]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 240 start !\n",
            "Training  :: loss =  193.2359  : micro (f1) =  0.7361091507880498  : macro (f2) =  0.7283885808108779  : Epoch_runtime =  1.7159366607666016  \n",
            "\n",
            "train_loss =  193.2359\n",
            "Validation  :: loss =  978.8754  : micro (f1) =  0.6811164118553021  : macro (f2) =  0.6706354454886936  : Epoch_runtime =  1.7159366607666016  \n",
            "\n",
            "val_loss =  978.8754\n",
            "Weight Support =  [0.6202056  0.08775174 0.1988501  0.09319248]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 241 start !\n",
            "Training  :: loss =  192.65875  : micro (f1) =  0.7361414896869635  : macro (f2) =  0.7284122059735905  : Epoch_runtime =  1.6991629600524902  \n",
            "\n",
            "train_loss =  192.65875\n",
            "Validation  :: loss =  987.5601  : micro (f1) =  0.6746327726615419  : macro (f2) =  0.6637491365948056  : Epoch_runtime =  1.6991629600524902  \n",
            "\n",
            "val_loss =  987.5601\n",
            "Weight Support =  [0.61993515 0.08772579 0.19915776 0.0931813 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 242 start !\n",
            "Training  :: loss =  192.79689  : micro (f1) =  0.7352198376619562  : macro (f2) =  0.7274749541815456  : Epoch_runtime =  1.7003047466278076  \n",
            "\n",
            "train_loss =  192.79689\n",
            "Validation  :: loss =  953.6711  : micro (f1) =  0.6785077232532545  : macro (f2) =  0.667179428484549  : Epoch_runtime =  1.7003047466278076  \n",
            "\n",
            "val_loss =  953.6711\n",
            "Weight Support =  [0.62013954 0.08790085 0.19915459 0.09280501]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 243 start !\n",
            "Training  :: loss =  194.11545  : micro (f1) =  0.7362367248050619  : macro (f2) =  0.7284703787906168  : Epoch_runtime =  1.7358460426330566  \n",
            "\n",
            "train_loss =  194.11545\n",
            "Validation  :: loss =  974.4807  : micro (f1) =  0.6756138477962483  : macro (f2) =  0.6638278899291934  : Epoch_runtime =  1.7358460426330566  \n",
            "\n",
            "val_loss =  974.4807\n",
            "Weight Support =  [0.6198865  0.088496   0.19878902 0.0928285 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 244 start !\n",
            "Training  :: loss =  193.36461  : micro (f1) =  0.7359358276190588  : macro (f2) =  0.7282037911248302  : Epoch_runtime =  1.6987013816833496  \n",
            "\n",
            "train_loss =  193.36461\n",
            "Validation  :: loss =  975.13885  : micro (f1) =  0.6802669208770258  : macro (f2) =  0.6687962663245066  : Epoch_runtime =  1.6987013816833496  \n",
            "\n",
            "val_loss =  975.13885\n",
            "Weight Support =  [0.62006015 0.08858901 0.19836023 0.09299064]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 245 start !\n",
            "Training  :: loss =  192.37366  : micro (f1) =  0.7370493039921736  : macro (f2) =  0.7292762535475591  : Epoch_runtime =  1.6797168254852295  \n",
            "\n",
            "train_loss =  192.37366\n",
            "Validation  :: loss =  994.6469  : micro (f1) =  0.6831346462872304  : macro (f2) =  0.6726115734283307  : Epoch_runtime =  1.6797168254852295  \n",
            "\n",
            "val_loss =  994.6469\n",
            "Weight Support =  [0.6206807  0.08837888 0.19813214 0.09280837]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 246 start !\n",
            "Training  :: loss =  192.15518  : micro (f1) =  0.7367677195957882  : macro (f2) =  0.7290096540345241  : Epoch_runtime =  1.732893705368042  \n",
            "\n",
            "train_loss =  192.15518\n",
            "Validation  :: loss =  1009.7189  : micro (f1) =  0.6833953834698436  : macro (f2) =  0.6734978926092141  : Epoch_runtime =  1.732893705368042  \n",
            "\n",
            "val_loss =  1009.7189\n",
            "Weight Support =  [0.6208567  0.08857878 0.19826153 0.09230297]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 247 start !\n",
            "Training  :: loss =  192.45084  : micro (f1) =  0.7368867224753629  : macro (f2) =  0.7291476006408003  : Epoch_runtime =  1.6878314018249512  \n",
            "\n",
            "train_loss =  192.45084\n",
            "Validation  :: loss =  954.421  : micro (f1) =  0.6813934364954773  : macro (f2) =  0.6710407644861315  : Epoch_runtime =  1.6878314018249512  \n",
            "\n",
            "val_loss =  954.421\n",
            "Weight Support =  [0.62112033 0.08824749 0.19841975 0.09221236]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 248 start !\n",
            "Training  :: loss =  191.42377  : micro (f1) =  0.7359578942421786  : macro (f2) =  0.7282103351563275  : Epoch_runtime =  1.683781385421753  \n",
            "\n",
            "train_loss =  191.42377\n",
            "Validation  :: loss =  962.3717  : micro (f1) =  0.6775254147613469  : macro (f2) =  0.6665878259262608  : Epoch_runtime =  1.683781385421753  \n",
            "\n",
            "val_loss =  962.3717\n",
            "Weight Support =  [0.62144136 0.08819009 0.1983424  0.09202604]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 249 start !\n",
            "Training  :: loss =  191.28183  : micro (f1) =  0.7353895340636608  : macro (f2) =  0.7276380384362573  : Epoch_runtime =  1.7086336612701416  \n",
            "\n",
            "train_loss =  191.28183\n",
            "Validation  :: loss =  978.6212  : micro (f1) =  0.6769207282164802  : macro (f2) =  0.6647382042425498  : Epoch_runtime =  1.7086336612701416  \n",
            "\n",
            "val_loss =  978.6212\n",
            "Weight Support =  [0.6212615  0.08834113 0.19844155 0.09195585]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 250 start !\n",
            "Training  :: loss =  191.29567  : micro (f1) =  0.735487359104853  : macro (f2) =  0.7277428734094935  : Epoch_runtime =  1.7198600769042969  \n",
            "\n",
            "train_loss =  191.29567\n",
            "Validation  :: loss =  972.6684  : micro (f1) =  0.6770722027171424  : macro (f2) =  0.6645627192056531  : Epoch_runtime =  1.7198600769042969  \n",
            "\n",
            "val_loss =  972.6684\n",
            "Weight Support =  [0.6208549  0.08904946 0.19811064 0.09198497]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 251 start !\n",
            "Training  :: loss =  190.62248  : micro (f1) =  0.7367281116839701  : macro (f2) =  0.728959992608366  : Epoch_runtime =  1.7283039093017578  \n",
            "\n",
            "train_loss =  190.62248\n",
            "Validation  :: loss =  983.765  : micro (f1) =  0.6782450419020893  : macro (f2) =  0.6672061074867891  : Epoch_runtime =  1.7283039093017578  \n",
            "\n",
            "val_loss =  983.765\n",
            "Weight Support =  [0.6214676  0.08878666 0.19775926 0.09198646]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 252 start !\n",
            "Training  :: loss =  189.9702  : micro (f1) =  0.7378993280679005  : macro (f2) =  0.7301770869396874  : Epoch_runtime =  1.749582052230835  \n",
            "\n",
            "train_loss =  189.9702\n",
            "Validation  :: loss =  982.55084  : micro (f1) =  0.6811897466557058  : macro (f2) =  0.6710934270734271  : Epoch_runtime =  1.749582052230835  \n",
            "\n",
            "val_loss =  982.55084\n",
            "Weight Support =  [0.6214236  0.08891381 0.19779138 0.09187118]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 253 start !\n",
            "Training  :: loss =  189.72443  : micro (f1) =  0.7372053039402717  : macro (f2) =  0.7294195685880155  : Epoch_runtime =  1.6766114234924316  \n",
            "\n",
            "train_loss =  189.72443\n",
            "Validation  :: loss =  976.07904  : micro (f1) =  0.6827524575513853  : macro (f2) =  0.6728053864650968  : Epoch_runtime =  1.6766114234924316  \n",
            "\n",
            "val_loss =  976.07904\n",
            "Weight Support =  [0.622479   0.08851672 0.19745423 0.09155001]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 254 start !\n",
            "Training  :: loss =  189.96754  : micro (f1) =  0.7366265797462992  : macro (f2) =  0.7288880300783529  : Epoch_runtime =  1.6815471649169922  \n",
            "\n",
            "train_loss =  189.96754\n",
            "Validation  :: loss =  957.51935  : micro (f1) =  0.6824963541861422  : macro (f2) =  0.6716482660182017  : Epoch_runtime =  1.6815471649169922  \n",
            "\n",
            "val_loss =  957.51935\n",
            "Weight Support =  [0.6228947  0.08825389 0.1975089  0.09134255]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 255 start !\n",
            "Training  :: loss =  189.7525  : micro (f1) =  0.7358137894551225  : macro (f2) =  0.7281036973656572  : Epoch_runtime =  1.7047719955444336  \n",
            "\n",
            "train_loss =  189.7525\n",
            "Validation  :: loss =  974.12274  : micro (f1) =  0.6798305341201392  : macro (f2) =  0.6684083513035682  : Epoch_runtime =  1.7047719955444336  \n",
            "\n",
            "val_loss =  974.12274\n",
            "Weight Support =  [0.6227709  0.08853658 0.19749928 0.09119318]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 256 start !\n",
            "Training  :: loss =  189.59898  : micro (f1) =  0.7355592379743563  : macro (f2) =  0.7278024482908843  : Epoch_runtime =  1.6859314441680908  \n",
            "\n",
            "train_loss =  189.59898\n",
            "Validation  :: loss =  988.5979  : micro (f1) =  0.6766085663068766  : macro (f2) =  0.665103779488941  : Epoch_runtime =  1.6859314441680908  \n",
            "\n",
            "val_loss =  988.5979\n",
            "Weight Support =  [0.62229455 0.08876237 0.19776125 0.09118184]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 257 start !\n",
            "Training  :: loss =  190.08257  : micro (f1) =  0.736224177685756  : macro (f2) =  0.7284631393358904  : Epoch_runtime =  1.715656042098999  \n",
            "\n",
            "train_loss =  190.08257\n",
            "Validation  :: loss =  990.7226  : micro (f1) =  0.6769395072087878  : macro (f2) =  0.6659209781189552  : Epoch_runtime =  1.715656042098999  \n",
            "\n",
            "val_loss =  990.7226\n",
            "Weight Support =  [0.6219306  0.08927479 0.19761488 0.09117972]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 258 start !\n",
            "Training  :: loss =  190.30684  : micro (f1) =  0.7363020146864997  : macro (f2) =  0.7285106638971605  : Epoch_runtime =  1.6813938617706299  \n",
            "\n",
            "train_loss =  190.30684\n",
            "Validation  :: loss =  972.5838  : micro (f1) =  0.6793164585612435  : macro (f2) =  0.6691756468803053  : Epoch_runtime =  1.6813938617706299  \n",
            "\n",
            "val_loss =  972.5838\n",
            "Weight Support =  [0.6222176  0.0890277  0.19751866 0.09123606]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 259 start !\n",
            "Training  :: loss =  189.84998  : micro (f1) =  0.7369561377686192  : macro (f2) =  0.7292176855246653  : Epoch_runtime =  1.6653199195861816  \n",
            "\n",
            "train_loss =  189.84998\n",
            "Validation  :: loss =  976.1221  : micro (f1) =  0.6822094154629144  : macro (f2) =  0.6712557505058497  : Epoch_runtime =  1.6653199195861816  \n",
            "\n",
            "val_loss =  976.1221\n",
            "Weight Support =  [0.6229885  0.08884804 0.19710086 0.09106264]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 260 start !\n",
            "Training  :: loss =  190.06952  : micro (f1) =  0.7365149624467308  : macro (f2) =  0.7287559377169811  : Epoch_runtime =  1.688154697418213  \n",
            "\n",
            "train_loss =  190.06952\n",
            "Validation  :: loss =  973.5304  : micro (f1) =  0.6838960262922019  : macro (f2) =  0.6726151363258447  : Epoch_runtime =  1.688154697418213  \n",
            "\n",
            "val_loss =  973.5304\n",
            "Weight Support =  [0.624467   0.08829379 0.19662623 0.0906129 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 261 start !\n",
            "Training  :: loss =  190.41756  : micro (f1) =  0.7369759474446367  : macro (f2) =  0.7292462605747971  : Epoch_runtime =  1.700659990310669  \n",
            "\n",
            "train_loss =  190.41756\n",
            "Validation  :: loss =  972.6827  : micro (f1) =  0.6830072708192788  : macro (f2) =  0.671890027750869  : Epoch_runtime =  1.700659990310669  \n",
            "\n",
            "val_loss =  972.6827\n",
            "Weight Support =  [0.62407804 0.08858648 0.1969206  0.09041489]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 262 start !\n",
            "Training  :: loss =  190.46777  : micro (f1) =  0.7360129789210097  : macro (f2) =  0.728299120594802  : Epoch_runtime =  1.7066144943237305  \n",
            "\n",
            "train_loss =  190.46777\n",
            "Validation  :: loss =  984.6644  : micro (f1) =  0.67590618336887  : macro (f2) =  0.6649501371965156  : Epoch_runtime =  1.7066144943237305  \n",
            "\n",
            "val_loss =  984.6644\n",
            "Weight Support =  [0.6242306  0.08874938 0.19694969 0.09007026]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 263 start !\n",
            "Training  :: loss =  190.5208  : micro (f1) =  0.7366860424069847  : macro (f2) =  0.7289585907748708  : Epoch_runtime =  1.7454354763031006  \n",
            "\n",
            "train_loss =  190.5208\n",
            "Validation  :: loss =  1022.2859  : micro (f1) =  0.6713508087866271  : macro (f2) =  0.660079473001344  : Epoch_runtime =  1.7454354763031006  \n",
            "\n",
            "val_loss =  1022.2859\n",
            "Weight Support =  [0.62341875 0.08926444 0.19707136 0.09024545]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 264 start !\n",
            "Training  :: loss =  191.41774  : micro (f1) =  0.7359076264609741  : macro (f2) =  0.7281285433892319  : Epoch_runtime =  1.7070236206054688  \n",
            "\n",
            "train_loss =  191.41774\n",
            "Validation  :: loss =  988.2508  : micro (f1) =  0.6778645635853492  : macro (f2) =  0.6665920820417741  : Epoch_runtime =  1.7070236206054688  \n",
            "\n",
            "val_loss =  988.2508\n",
            "Weight Support =  [0.62375355 0.08928663 0.19686568 0.09009422]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 265 start !\n",
            "Training  :: loss =  191.8925  : micro (f1) =  0.7362935952478975  : macro (f2) =  0.7285152800370172  : Epoch_runtime =  1.726283073425293  \n",
            "\n",
            "train_loss =  191.8925\n",
            "Validation  :: loss =  966.31476  : micro (f1) =  0.680145113748016  : macro (f2) =  0.6687124363805784  : Epoch_runtime =  1.726283073425293  \n",
            "\n",
            "val_loss =  966.31476\n",
            "Weight Support =  [0.62491447 0.08887221 0.19616875 0.0900446 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 266 start !\n",
            "Training  :: loss =  191.27603  : micro (f1) =  0.735687325831599  : macro (f2) =  0.7279501354437633  : Epoch_runtime =  1.6926052570343018  \n",
            "\n",
            "train_loss =  191.27603\n",
            "Validation  :: loss =  970.6083  : micro (f1) =  0.6832265557678618  : macro (f2) =  0.6721179885199033  : Epoch_runtime =  1.6926052570343018  \n",
            "\n",
            "val_loss =  970.6083\n",
            "Weight Support =  [0.6262409  0.08864003 0.1956542  0.08946482]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 267 start !\n",
            "Training  :: loss =  190.79633  : micro (f1) =  0.7372502799222111  : macro (f2) =  0.7295129331162152  : Epoch_runtime =  1.7215454578399658  \n",
            "\n",
            "train_loss =  190.79633\n",
            "Validation  :: loss =  965.6932  : micro (f1) =  0.6828647373741156  : macro (f2) =  0.6720831567427652  : Epoch_runtime =  1.7215454578399658  \n",
            "\n",
            "val_loss =  965.6932\n",
            "Weight Support =  [0.6262701  0.08882689 0.19567105 0.08923193]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 268 start !\n",
            "Training  :: loss =  191.13902  : micro (f1) =  0.7364149611856033  : macro (f2) =  0.7286797446386124  : Epoch_runtime =  1.7678821086883545  \n",
            "\n",
            "train_loss =  191.13902\n",
            "Validation  :: loss =  975.0258  : micro (f1) =  0.6790484766621643  : macro (f2) =  0.6683141051541086  : Epoch_runtime =  1.7678821086883545  \n",
            "\n",
            "val_loss =  975.0258\n",
            "Weight Support =  [0.6257596  0.08942545 0.19573063 0.0890843 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 269 start !\n",
            "Training  :: loss =  190.9162  : micro (f1) =  0.7365670499888182  : macro (f2) =  0.7288103875003371  : Epoch_runtime =  1.722193956375122  \n",
            "\n",
            "train_loss =  190.9162\n",
            "Validation  :: loss =  992.42444  : micro (f1) =  0.6777542051826034  : macro (f2) =  0.66693288721374  : Epoch_runtime =  1.722193956375122  \n",
            "\n",
            "val_loss =  992.42444\n",
            "Weight Support =  [0.62582844 0.08912044 0.19594005 0.08911102]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 270 start !\n",
            "Training  :: loss =  190.85336  : micro (f1) =  0.7356521739130435  : macro (f2) =  0.7278642883021338  : Epoch_runtime =  1.708893060684204  \n",
            "\n",
            "train_loss =  190.85336\n",
            "Validation  :: loss =  985.43445  : micro (f1) =  0.6764470525473459  : macro (f2) =  0.6648732137866882  : Epoch_runtime =  1.708893060684204  \n",
            "\n",
            "val_loss =  985.43445\n",
            "Weight Support =  [0.6256939  0.08957087 0.1956768  0.08905846]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 271 start !\n",
            "Training  :: loss =  190.72906  : micro (f1) =  0.7369437477184677  : macro (f2) =  0.7291636695046145  : Epoch_runtime =  1.6988937854766846  \n",
            "\n",
            "train_loss =  190.72906\n",
            "Validation  :: loss =  971.7521  : micro (f1) =  0.6784779392237945  : macro (f2) =  0.6669556044084318  : Epoch_runtime =  1.6988937854766846  \n",
            "\n",
            "val_loss =  971.7521\n",
            "Weight Support =  [0.62669265 0.0895804  0.19501777 0.08870925]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 272 start !\n",
            "Training  :: loss =  190.05878  : micro (f1) =  0.7361651598339936  : macro (f2) =  0.7284204252641757  : Epoch_runtime =  1.717454195022583  \n",
            "\n",
            "train_loss =  190.05878\n",
            "Validation  :: loss =  971.0861  : micro (f1) =  0.680246866835788  : macro (f2) =  0.6692195614911505  : Epoch_runtime =  1.717454195022583  \n",
            "\n",
            "val_loss =  971.0861\n",
            "Weight Support =  [0.6263766  0.08984864 0.19529308 0.08848161]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 273 start !\n",
            "Training  :: loss =  189.73007  : micro (f1) =  0.7369473659443307  : macro (f2) =  0.7291814546787282  : Epoch_runtime =  1.6976771354675293  \n",
            "\n",
            "train_loss =  189.73007\n",
            "Validation  :: loss =  955.66675  : micro (f1) =  0.679457218243498  : macro (f2) =  0.6685935273243068  : Epoch_runtime =  1.6976771354675293  \n",
            "\n",
            "val_loss =  955.66675\n",
            "Weight Support =  [0.62665004 0.08967713 0.19511275 0.08856013]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 274 start !\n",
            "Training  :: loss =  189.1511  : micro (f1) =  0.7367962616382405  : macro (f2) =  0.7290565657131461  : Epoch_runtime =  1.7390594482421875  \n",
            "\n",
            "train_loss =  189.1511\n",
            "Validation  :: loss =  968.7701  : micro (f1) =  0.6820978391356542  : macro (f2) =  0.6708822435122104  : Epoch_runtime =  1.7390594482421875  \n",
            "\n",
            "val_loss =  968.7701\n",
            "Weight Support =  [0.62649715 0.08956516 0.19543786 0.08849987]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 275 start !\n",
            "Training  :: loss =  189.027  : micro (f1) =  0.7369598756344878  : macro (f2) =  0.7292096711612723  : Epoch_runtime =  1.6813170909881592  \n",
            "\n",
            "train_loss =  189.027\n",
            "Validation  :: loss =  974.64417  : micro (f1) =  0.6825772073999024  : macro (f2) =  0.6714410053265278  : Epoch_runtime =  1.6813170909881592  \n",
            "\n",
            "val_loss =  974.64417\n",
            "Weight Support =  [0.6266234  0.08954276 0.19536859 0.0884653 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 276 start !\n",
            "Training  :: loss =  189.35469  : micro (f1) =  0.7370453662384759  : macro (f2) =  0.7292872483891707  : Epoch_runtime =  1.6867287158966064  \n",
            "\n",
            "train_loss =  189.35469\n",
            "Validation  :: loss =  979.73267  : micro (f1) =  0.6803105683702698  : macro (f2) =  0.6695621826121726  : Epoch_runtime =  1.6867287158966064  \n",
            "\n",
            "val_loss =  979.73267\n",
            "Weight Support =  [0.62677944 0.08989792 0.1949571  0.08836557]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 277 start !\n",
            "Training  :: loss =  189.09712  : micro (f1) =  0.7362766532665679  : macro (f2) =  0.728511930049319  : Epoch_runtime =  1.6910467147827148  \n",
            "\n",
            "train_loss =  189.09712\n",
            "Validation  :: loss =  973.4597  : micro (f1) =  0.6786321556773096  : macro (f2) =  0.6677957622898716  : Epoch_runtime =  1.6910467147827148  \n",
            "\n",
            "val_loss =  973.4597\n",
            "Weight Support =  [0.62758046 0.08951093 0.19475374 0.08815483]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 278 start !\n",
            "Training  :: loss =  188.98006  : micro (f1) =  0.7365683759678064  : macro (f2) =  0.7288138981045951  : Epoch_runtime =  1.6933934688568115  \n",
            "\n",
            "train_loss =  188.98006\n",
            "Validation  :: loss =  973.6611  : micro (f1) =  0.6784070594500019  : macro (f2) =  0.667577841454765  : Epoch_runtime =  1.6933934688568115  \n",
            "\n",
            "val_loss =  973.6611\n",
            "Weight Support =  [0.6265551  0.08994529 0.19543022 0.08806945]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 279 start !\n",
            "Training  :: loss =  189.05803  : micro (f1) =  0.734533513164839  : macro (f2) =  0.7268292436934919  : Epoch_runtime =  1.6941702365875244  \n",
            "\n",
            "train_loss =  189.05803\n",
            "Validation  :: loss =  976.2657  : micro (f1) =  0.6765967223158217  : macro (f2) =  0.6652398126480369  : Epoch_runtime =  1.6941702365875244  \n",
            "\n",
            "val_loss =  976.2657\n",
            "Weight Support =  [0.6267327  0.08956669 0.19554693 0.08815368]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 280 start !\n",
            "Training  :: loss =  189.28342  : micro (f1) =  0.7367950287166933  : macro (f2) =  0.7290529443719936  : Epoch_runtime =  1.6863949298858643  \n",
            "\n",
            "train_loss =  189.28342\n",
            "Validation  :: loss =  952.9171  : micro (f1) =  0.6800015181994156  : macro (f2) =  0.6685031775645462  : Epoch_runtime =  1.6863949298858643  \n",
            "\n",
            "val_loss =  952.9171\n",
            "Weight Support =  [0.62694395 0.0899065  0.19512129 0.08802827]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 281 start !\n",
            "Training  :: loss =  189.7355  : micro (f1) =  0.7361217266559269  : macro (f2) =  0.7283585692022413  : Epoch_runtime =  1.6762702465057373  \n",
            "\n",
            "train_loss =  189.7355\n",
            "Validation  :: loss =  976.0969  : micro (f1) =  0.6812615396209352  : macro (f2) =  0.6697687086005082  : Epoch_runtime =  1.6762702465057373  \n",
            "\n",
            "val_loss =  976.0969\n",
            "Weight Support =  [0.6283969  0.0895603  0.1943725  0.08767033]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 282 start !\n",
            "Training  :: loss =  189.5687  : micro (f1) =  0.7387353364647012  : macro (f2) =  0.730946495319014  : Epoch_runtime =  1.6798603534698486  \n",
            "\n",
            "train_loss =  189.5687\n",
            "Validation  :: loss =  986.42346  : micro (f1) =  0.6837338124111086  : macro (f2) =  0.6728039983594387  : Epoch_runtime =  1.6798603534698486  \n",
            "\n",
            "val_loss =  986.42346\n",
            "Weight Support =  [0.6284741  0.08986364 0.19407864 0.08758358]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 283 start !\n",
            "Training  :: loss =  190.29922  : micro (f1) =  0.736692205392555  : macro (f2) =  0.7289085759937979  : Epoch_runtime =  1.6899957656860352  \n",
            "\n",
            "train_loss =  190.29922\n",
            "Validation  :: loss =  985.56  : micro (f1) =  0.6826535186015239  : macro (f2) =  0.6723828096160464  : Epoch_runtime =  1.6899957656860352  \n",
            "\n",
            "val_loss =  985.56\n",
            "Weight Support =  [0.6284424  0.08971797 0.19443856 0.08740099]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 284 start !\n",
            "Training  :: loss =  191.41972  : micro (f1) =  0.7379405487625134  : macro (f2) =  0.7302516780774799  : Epoch_runtime =  1.7106328010559082  \n",
            "\n",
            "train_loss =  191.41972\n",
            "Validation  :: loss =  968.008  : micro (f1) =  0.6806675417213576  : macro (f2) =  0.6703289278124924  : Epoch_runtime =  1.7106328010559082  \n",
            "\n",
            "val_loss =  968.008\n",
            "Weight Support =  [0.62795025 0.08971689 0.19492906 0.08740373]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 285 start !\n",
            "Training  :: loss =  192.12251  : micro (f1) =  0.7342478602415289  : macro (f2) =  0.7265218199399857  : Epoch_runtime =  1.7838189601898193  \n",
            "\n",
            "train_loss =  192.12251\n",
            "Validation  :: loss =  979.4621  : micro (f1) =  0.677599786918306  : macro (f2) =  0.6668998804224551  : Epoch_runtime =  1.7838189601898193  \n",
            "\n",
            "val_loss =  979.4621\n",
            "Weight Support =  [0.6289071  0.08865621 0.19526497 0.08717176]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 286 start !\n",
            "Training  :: loss =  192.38748  : micro (f1) =  0.7370255251012527  : macro (f2) =  0.7292700478965678  : Epoch_runtime =  1.7001898288726807  \n",
            "\n",
            "train_loss =  192.38748\n",
            "Validation  :: loss =  989.2807  : micro (f1) =  0.6745209042573538  : macro (f2) =  0.6632211688621616  : Epoch_runtime =  1.7001898288726807  \n",
            "\n",
            "val_loss =  989.2807\n",
            "Weight Support =  [0.6285135  0.08951334 0.19502352 0.08694955]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 287 start !\n",
            "Training  :: loss =  193.21577  : micro (f1) =  0.7349790952224362  : macro (f2) =  0.727240315463294  : Epoch_runtime =  1.694953441619873  \n",
            "\n",
            "train_loss =  193.21577\n",
            "Validation  :: loss =  989.1793  : micro (f1) =  0.6750747527409338  : macro (f2) =  0.6630481537249673  : Epoch_runtime =  1.694953441619873  \n",
            "\n",
            "val_loss =  989.1793\n",
            "Weight Support =  [0.6288572  0.09025628 0.19416551 0.086721  ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 288 start !\n",
            "Training  :: loss =  193.38562  : micro (f1) =  0.7363068121623053  : macro (f2) =  0.7285759953871734  : Epoch_runtime =  1.720036268234253  \n",
            "\n",
            "train_loss =  193.38562\n",
            "Validation  :: loss =  970.2245  : micro (f1) =  0.679077542646556  : macro (f2) =  0.6674642780280275  : Epoch_runtime =  1.720036268234253  \n",
            "\n",
            "val_loss =  970.2245\n",
            "Weight Support =  [0.62904197 0.09049423 0.19380188 0.08666194]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 289 start !\n",
            "Training  :: loss =  192.76836  : micro (f1) =  0.7369811009325742  : macro (f2) =  0.7292024953423394  : Epoch_runtime =  1.6947073936462402  \n",
            "\n",
            "train_loss =  192.76836\n",
            "Validation  :: loss =  968.50397  : micro (f1) =  0.6834057862389447  : macro (f2) =  0.6721524577230176  : Epoch_runtime =  1.6947073936462402  \n",
            "\n",
            "val_loss =  968.50397\n",
            "Weight Support =  [0.630166   0.09007053 0.19353312 0.0862303 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 290 start !\n",
            "Training  :: loss =  192.14949  : micro (f1) =  0.7375767793353062  : macro (f2) =  0.7298012752869395  : Epoch_runtime =  1.656334638595581  \n",
            "\n",
            "train_loss =  192.14949\n",
            "Validation  :: loss =  1003.34174  : micro (f1) =  0.6843008506193106  : macro (f2) =  0.6736821727000454  : Epoch_runtime =  1.656334638595581  \n",
            "\n",
            "val_loss =  1003.34174\n",
            "Weight Support =  [0.63093424 0.09006752 0.19326325 0.08573505]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 291 start !\n",
            "Training  :: loss =  192.91219  : micro (f1) =  0.737253793233437  : macro (f2) =  0.729516474234872  : Epoch_runtime =  1.7145373821258545  \n",
            "\n",
            "train_loss =  192.91219\n",
            "Validation  :: loss =  988.4899  : micro (f1) =  0.6782105698080607  : macro (f2) =  0.6677893153399497  : Epoch_runtime =  1.7145373821258545  \n",
            "\n",
            "val_loss =  988.4899\n",
            "Weight Support =  [0.6314759  0.08955153 0.19359772 0.08537491]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 292 start !\n",
            "Training  :: loss =  192.13406  : micro (f1) =  0.7368297161150605  : macro (f2) =  0.7290694086500107  : Epoch_runtime =  1.6974449157714844  \n",
            "\n",
            "train_loss =  192.13406\n",
            "Validation  :: loss =  987.9953  : micro (f1) =  0.6772851089588378  : macro (f2) =  0.6665319050401997  : Epoch_runtime =  1.6974449157714844  \n",
            "\n",
            "val_loss =  987.9953\n",
            "Weight Support =  [0.63078773 0.08978094 0.19386558 0.08556578]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 293 start !\n",
            "Training  :: loss =  191.46407  : micro (f1) =  0.7353397237101775  : macro (f2) =  0.7276048760989527  : Epoch_runtime =  1.7088680267333984  \n",
            "\n",
            "train_loss =  191.46407\n",
            "Validation  :: loss =  980.9974  : micro (f1) =  0.6783131613001445  : macro (f2) =  0.6670808352273169  : Epoch_runtime =  1.7088680267333984  \n",
            "\n",
            "val_loss =  980.9974\n",
            "Weight Support =  [0.63037866 0.09000974 0.1940509  0.08556074]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 294 start !\n",
            "Training  :: loss =  191.35835  : micro (f1) =  0.7356721982429111  : macro (f2) =  0.7279451373155953  : Epoch_runtime =  1.6977767944335938  \n",
            "\n",
            "train_loss =  191.35835\n",
            "Validation  :: loss =  966.3919  : micro (f1) =  0.6790334205834377  : macro (f2) =  0.667142290311141  : Epoch_runtime =  1.6977767944335938  \n",
            "\n",
            "val_loss =  966.3919\n",
            "Weight Support =  [0.6295613  0.09067649 0.19434947 0.08541273]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 295 start !\n",
            "Training  :: loss =  191.34918  : micro (f1) =  0.7363207491676569  : macro (f2) =  0.7285342271066523  : Epoch_runtime =  1.6903376579284668  \n",
            "\n",
            "train_loss =  191.34918\n",
            "Validation  :: loss =  955.6764  : micro (f1) =  0.6806700345652751  : macro (f2) =  0.6692162675871522  : Epoch_runtime =  1.6903376579284668  \n",
            "\n",
            "val_loss =  955.6764\n",
            "Weight Support =  [0.63002837 0.09031841 0.19418606 0.08546714]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 296 start !\n",
            "Training  :: loss =  190.0408  : micro (f1) =  0.7372969154697434  : macro (f2) =  0.7295606071214729  : Epoch_runtime =  1.6991205215454102  \n",
            "\n",
            "train_loss =  190.0408\n",
            "Validation  :: loss =  994.9025  : micro (f1) =  0.68120590121873  : macro (f2) =  0.670157726424517  : Epoch_runtime =  1.6991205215454102  \n",
            "\n",
            "val_loss =  994.9025\n",
            "Weight Support =  [0.6311121  0.09030543 0.19342753 0.08515493]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 297 start !\n",
            "Training  :: loss =  189.21771  : micro (f1) =  0.7382640601219901  : macro (f2) =  0.7305239618836539  : Epoch_runtime =  1.6450862884521484  \n",
            "\n",
            "train_loss =  189.21771\n",
            "Validation  :: loss =  993.621  : micro (f1) =  0.684147706079821  : macro (f2) =  0.673267043959578  : Epoch_runtime =  1.6450862884521484  \n",
            "\n",
            "val_loss =  993.621\n",
            "Weight Support =  [0.6313495  0.09038849 0.19335283 0.08490912]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 298 start !\n",
            "Training  :: loss =  189.70767  : micro (f1) =  0.7376441079146009  : macro (f2) =  0.7298718626931179  : Epoch_runtime =  1.6671299934387207  \n",
            "\n",
            "train_loss =  189.70767\n",
            "Validation  :: loss =  991.5585  : micro (f1) =  0.6808399146610772  : macro (f2) =  0.6705193481484697  : Epoch_runtime =  1.6671299934387207  \n",
            "\n",
            "val_loss =  991.5585\n",
            "Weight Support =  [0.63101125 0.09043354 0.19353543 0.08501972]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 299 start !\n",
            "Training  :: loss =  189.23903  : micro (f1) =  0.7364237164561724  : macro (f2) =  0.7286736650276127  : Epoch_runtime =  1.6983237266540527  \n",
            "\n",
            "train_loss =  189.23903\n",
            "Validation  :: loss =  973.4045  : micro (f1) =  0.6801445674271515  : macro (f2) =  0.6696683106800831  : Epoch_runtime =  1.6983237266540527  \n",
            "\n",
            "val_loss =  973.4045\n",
            "Weight Support =  [0.63095933 0.09051111 0.19375066 0.08477888]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 300 start !\n",
            "Training  :: loss =  188.70802  : micro (f1) =  0.735678368344377  : macro (f2) =  0.7279288539185408  : Epoch_runtime =  1.7066926956176758  \n",
            "\n",
            "train_loss =  188.70802\n",
            "Validation  :: loss =  986.4608  : micro (f1) =  0.6743999081199036  : macro (f2) =  0.6630059444412708  : Epoch_runtime =  1.7066926956176758  \n",
            "\n",
            "val_loss =  986.4608\n",
            "Weight Support =  [0.6302868  0.09045377 0.1944582  0.08480121]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 301 start !\n",
            "Training  :: loss =  189.27834  : micro (f1) =  0.7351762867582269  : macro (f2) =  0.7274004573950659  : Epoch_runtime =  1.6846873760223389  \n",
            "\n",
            "train_loss =  189.27834\n",
            "Validation  :: loss =  989.6582  : micro (f1) =  0.677819462522012  : macro (f2) =  0.6654662847785392  : Epoch_runtime =  1.6846873760223389  \n",
            "\n",
            "val_loss =  989.6582\n",
            "Weight Support =  [0.63049984 0.09062038 0.19425663 0.08462318]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 302 start !\n",
            "Training  :: loss =  189.93791  : micro (f1) =  0.7361846284141434  : macro (f2) =  0.7284169283080528  : Epoch_runtime =  1.755401849746704  \n",
            "\n",
            "train_loss =  189.93791\n",
            "Validation  :: loss =  979.2599  : micro (f1) =  0.6770285714285714  : macro (f2) =  0.6654757013207112  : Epoch_runtime =  1.755401849746704  \n",
            "\n",
            "val_loss =  979.2599\n",
            "Weight Support =  [0.6311259  0.09056408 0.19352394 0.08478612]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 303 start !\n",
            "Training  :: loss =  189.68506  : micro (f1) =  0.7376483203129603  : macro (f2) =  0.7298947808884692  : Epoch_runtime =  1.6539311408996582  \n",
            "\n",
            "train_loss =  189.68506\n",
            "Validation  :: loss =  985.26184  : micro (f1) =  0.6825462320417118  : macro (f2) =  0.6714993894509654  : Epoch_runtime =  1.6539311408996582  \n",
            "\n",
            "val_loss =  985.26184\n",
            "Weight Support =  [0.63232696 0.09015854 0.19301997 0.08449458]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 304 start !\n",
            "Training  :: loss =  189.1397  : micro (f1) =  0.7382946102134685  : macro (f2) =  0.7305389180216767  : Epoch_runtime =  1.6671156883239746  \n",
            "\n",
            "train_loss =  189.1397\n",
            "Validation  :: loss =  992.4435  : micro (f1) =  0.6853099981381492  : macro (f2) =  0.6750043614385342  : Epoch_runtime =  1.6671156883239746  \n",
            "\n",
            "val_loss =  992.4435\n",
            "Weight Support =  [0.6325995  0.09016893 0.19287716 0.08435448]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 305 start !\n",
            "Training  :: loss =  190.11584  : micro (f1) =  0.7368978669303574  : macro (f2) =  0.7291347282006431  : Epoch_runtime =  1.735724925994873  \n",
            "\n",
            "train_loss =  190.11584\n",
            "Validation  :: loss =  985.874  : micro (f1) =  0.683132800239145  : macro (f2) =  0.6729789240454154  : Epoch_runtime =  1.735724925994873  \n",
            "\n",
            "val_loss =  985.874\n",
            "Weight Support =  [0.6323776  0.09057925 0.19297978 0.08406334]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 306 start !\n",
            "Training  :: loss =  190.02386  : micro (f1) =  0.7357445958177095  : macro (f2) =  0.7280056924693827  : Epoch_runtime =  1.7891311645507812  \n",
            "\n",
            "train_loss =  190.02386\n",
            "Validation  :: loss =  998.0972  : micro (f1) =  0.6757990867579908  : macro (f2) =  0.6646983583909261  : Epoch_runtime =  1.7891311645507812  \n",
            "\n",
            "val_loss =  998.0972\n",
            "Weight Support =  [0.6317985  0.09032148 0.19365746 0.08422255]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 307 start !\n",
            "Training  :: loss =  189.86174  : micro (f1) =  0.7348759670810879  : macro (f2) =  0.7271427440724587  : Epoch_runtime =  1.7843987941741943  \n",
            "\n",
            "train_loss =  189.86174\n",
            "Validation  :: loss =  1000.7012  : micro (f1) =  0.6734142007262612  : macro (f2) =  0.6613437294791416  : Epoch_runtime =  1.7843987941741943  \n",
            "\n",
            "val_loss =  1000.7012\n",
            "Weight Support =  [0.631267   0.09054017 0.19413081 0.08406208]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 308 start !\n",
            "Training  :: loss =  191.15776  : micro (f1) =  0.7354198975515768  : macro (f2) =  0.7276822744961652  : Epoch_runtime =  1.7557837963104248  \n",
            "\n",
            "train_loss =  191.15776\n",
            "Validation  :: loss =  980.3868  : micro (f1) =  0.6758057125400947  : macro (f2) =  0.6635930014345531  : Epoch_runtime =  1.7557837963104248  \n",
            "\n",
            "val_loss =  980.3868\n",
            "Weight Support =  [0.63194627 0.09070833 0.19343789 0.08390751]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 309 start !\n",
            "Training  :: loss =  191.52953  : micro (f1) =  0.7360656894807425  : macro (f2) =  0.7283123432432921  : Epoch_runtime =  1.7144834995269775  \n",
            "\n",
            "train_loss =  191.52953\n",
            "Validation  :: loss =  987.1888  : micro (f1) =  0.6788928123932112  : macro (f2) =  0.6679976213616171  : Epoch_runtime =  1.7144834995269775  \n",
            "\n",
            "val_loss =  987.1888\n",
            "Weight Support =  [0.6333537  0.0905234  0.19248846 0.08363447]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 310 start !\n",
            "Training  :: loss =  190.4058  : micro (f1) =  0.7379517717115737  : macro (f2) =  0.7301617454129697  : Epoch_runtime =  1.683122158050537  \n",
            "\n",
            "train_loss =  190.4058\n",
            "Validation  :: loss =  1015.2175  : micro (f1) =  0.684068406840684  : macro (f2) =  0.6735903227472256  : Epoch_runtime =  1.683122158050537  \n",
            "\n",
            "val_loss =  1015.2175\n",
            "Weight Support =  [0.6341804  0.09024466 0.19199617 0.08357876]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 311 start !\n",
            "Training  :: loss =  190.26779  : micro (f1) =  0.7376678445229683  : macro (f2) =  0.729879948000238  : Epoch_runtime =  1.6779212951660156  \n",
            "\n",
            "train_loss =  190.26779\n",
            "Validation  :: loss =  979.5617  : micro (f1) =  0.6845902127026626  : macro (f2) =  0.6745231888446308  : Epoch_runtime =  1.6779212951660156  \n",
            "\n",
            "val_loss =  979.5617\n",
            "Weight Support =  [0.6334281  0.09081364 0.19230959 0.08344871]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 312 start !\n",
            "Training  :: loss =  190.05934  : micro (f1) =  0.736600522561966  : macro (f2) =  0.7288654040809643  : Epoch_runtime =  1.6768035888671875  \n",
            "\n",
            "train_loss =  190.05934\n",
            "Validation  :: loss =  974.6204  : micro (f1) =  0.6788565346308827  : macro (f2) =  0.6677869819454108  : Epoch_runtime =  1.6768035888671875  \n",
            "\n",
            "val_loss =  974.6204\n",
            "Weight Support =  [0.6328994  0.09071593 0.19286034 0.08352427]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 313 start !\n",
            "Training  :: loss =  189.00842  : micro (f1) =  0.7354118959544361  : macro (f2) =  0.7276804886715668  : Epoch_runtime =  1.7203335762023926  \n",
            "\n",
            "train_loss =  189.00842\n",
            "Validation  :: loss =  987.47876  : micro (f1) =  0.6768594091274859  : macro (f2) =  0.6652341096442882  : Epoch_runtime =  1.7203335762023926  \n",
            "\n",
            "val_loss =  987.47876\n",
            "Weight Support =  [0.63244516 0.09102947 0.19311792 0.08340754]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 314 start !\n",
            "Training  :: loss =  189.48848  : micro (f1) =  0.7355868081256681  : macro (f2) =  0.7278603412557751  : Epoch_runtime =  1.7128725051879883  \n",
            "\n",
            "train_loss =  189.48848\n",
            "Validation  :: loss =  981.068  : micro (f1) =  0.6755528396287668  : macro (f2) =  0.6639939325285363  : Epoch_runtime =  1.7128725051879883  \n",
            "\n",
            "val_loss =  981.068\n",
            "Weight Support =  [0.63309616 0.09106048 0.19269845 0.08314493]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 315 start !\n",
            "Training  :: loss =  189.47592  : micro (f1) =  0.7371519397186084  : macro (f2) =  0.7293679532051033  : Epoch_runtime =  1.698310375213623  \n",
            "\n",
            "train_loss =  189.47592\n",
            "Validation  :: loss =  985.4662  : micro (f1) =  0.677154582763338  : macro (f2) =  0.6664786847434154  : Epoch_runtime =  1.698310375213623  \n",
            "\n",
            "val_loss =  985.4662\n",
            "Weight Support =  [0.6335437  0.09136313 0.19210674 0.08298639]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 316 start !\n",
            "Training  :: loss =  188.70947  : micro (f1) =  0.7371544294258936  : macro (f2) =  0.7293878804469648  : Epoch_runtime =  1.7178595066070557  \n",
            "\n",
            "train_loss =  188.70947\n",
            "Validation  :: loss =  996.9194  : micro (f1) =  0.6811129147948206  : macro (f2) =  0.670225525470932  : Epoch_runtime =  1.7178595066070557  \n",
            "\n",
            "val_loss =  996.9194\n",
            "Weight Support =  [0.63410693 0.09110077 0.1916997  0.08309269]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 317 start !\n",
            "Training  :: loss =  188.44261  : micro (f1) =  0.7381165707818541  : macro (f2) =  0.7303651076188649  : Epoch_runtime =  1.6949539184570312  \n",
            "\n",
            "train_loss =  188.44261\n",
            "Validation  :: loss =  979.7705  : micro (f1) =  0.6842479332660009  : macro (f2) =  0.6736493753693821  : Epoch_runtime =  1.6949539184570312  \n",
            "\n",
            "val_loss =  979.7705\n",
            "Weight Support =  [0.6336932  0.09110752 0.19189644 0.08330288]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 318 start !\n",
            "Training  :: loss =  188.49762  : micro (f1) =  0.7365361761210538  : macro (f2) =  0.7287874626406645  : Epoch_runtime =  1.7364411354064941  \n",
            "\n",
            "train_loss =  188.49762\n",
            "Validation  :: loss =  975.63776  : micro (f1) =  0.6804255961364323  : macro (f2) =  0.6695295292746469  : Epoch_runtime =  1.7364411354064941  \n",
            "\n",
            "val_loss =  975.63776\n",
            "Weight Support =  [0.6334542  0.09120944 0.19224751 0.08308887]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 319 start !\n",
            "Training  :: loss =  188.13713  : micro (f1) =  0.7361964631925269  : macro (f2) =  0.7284776149631987  : Epoch_runtime =  1.746509313583374  \n",
            "\n",
            "train_loss =  188.13713\n",
            "Validation  :: loss =  969.4357  : micro (f1) =  0.6788094516813087  : macro (f2) =  0.6677658358781033  : Epoch_runtime =  1.746509313583374  \n",
            "\n",
            "val_loss =  969.4357\n",
            "Weight Support =  [0.63356495 0.09114556 0.1923859  0.08290358]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 320 start !\n",
            "Training  :: loss =  188.31342  : micro (f1) =  0.73511038046031  : macro (f2) =  0.7273581831635644  : Epoch_runtime =  1.696549892425537  \n",
            "\n",
            "train_loss =  188.31342\n",
            "Validation  :: loss =  979.42255  : micro (f1) =  0.6751160843419349  : macro (f2) =  0.6640292345771013  : Epoch_runtime =  1.696549892425537  \n",
            "\n",
            "val_loss =  979.42255\n",
            "Weight Support =  [0.63398844 0.09100512 0.1924624  0.08254407]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 321 start !\n",
            "Training  :: loss =  188.53943  : micro (f1) =  0.7367121354656632  : macro (f2) =  0.7289441883337937  : Epoch_runtime =  1.6763191223144531  \n",
            "\n",
            "train_loss =  188.53943\n",
            "Validation  :: loss =  1005.1031  : micro (f1) =  0.673586640110311  : macro (f2) =  0.6626679410825553  : Epoch_runtime =  1.6763191223144531  \n",
            "\n",
            "val_loss =  1005.1031\n",
            "Weight Support =  [0.6341001  0.09128244 0.191988   0.08262948]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 322 start !\n",
            "Training  :: loss =  188.55511  : micro (f1) =  0.7368953986718786  : macro (f2) =  0.7291016324072062  : Epoch_runtime =  1.6963024139404297  \n",
            "\n",
            "train_loss =  188.55511\n",
            "Validation  :: loss =  989.6775  : micro (f1) =  0.677962238346693  : macro (f2) =  0.6668358021282554  : Epoch_runtime =  1.6963024139404297  \n",
            "\n",
            "val_loss =  989.6775\n",
            "Weight Support =  [0.63385683 0.0914799  0.19174038 0.08292293]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 323 start !\n",
            "Training  :: loss =  188.48544  : micro (f1) =  0.7379501385041551  : macro (f2) =  0.7302074931126215  : Epoch_runtime =  1.6878759860992432  \n",
            "\n",
            "train_loss =  188.48544\n",
            "Validation  :: loss =  977.009  : micro (f1) =  0.681284790130886  : macro (f2) =  0.670601301734558  : Epoch_runtime =  1.6878759860992432  \n",
            "\n",
            "val_loss =  977.009\n",
            "Weight Support =  [0.63382256 0.09149618 0.19169578 0.08298551]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 324 start !\n",
            "Training  :: loss =  188.36125  : micro (f1) =  0.7365002412247156  : macro (f2) =  0.7287409475672806  : Epoch_runtime =  1.7131199836730957  \n",
            "\n",
            "train_loss =  188.36125\n",
            "Validation  :: loss =  981.38354  : micro (f1) =  0.681773621898801  : macro (f2) =  0.6706326370721081  : Epoch_runtime =  1.7131199836730957  \n",
            "\n",
            "val_loss =  981.38354\n",
            "Weight Support =  [0.6343528  0.09116535 0.19183502 0.08264679]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 325 start !\n",
            "Training  :: loss =  188.78922  : micro (f1) =  0.7372709716923366  : macro (f2) =  0.729504247796173  : Epoch_runtime =  1.6971116065979004  \n",
            "\n",
            "train_loss =  188.78922\n",
            "Validation  :: loss =  975.2031  : micro (f1) =  0.6799531951836334  : macro (f2) =  0.6691093562654491  : Epoch_runtime =  1.6971116065979004  \n",
            "\n",
            "val_loss =  975.2031\n",
            "Weight Support =  [0.6348368  0.09121846 0.19170158 0.08224319]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 326 start !\n",
            "Training  :: loss =  188.90326  : micro (f1) =  0.73589827959011  : macro (f2) =  0.7281333645666128  : Epoch_runtime =  1.682450771331787  \n",
            "\n",
            "train_loss =  188.90326\n",
            "Validation  :: loss =  980.1479  : micro (f1) =  0.677829974238521  : macro (f2) =  0.6671072690081467  : Epoch_runtime =  1.682450771331787  \n",
            "\n",
            "val_loss =  980.1479\n",
            "Weight Support =  [0.6352085  0.09093236 0.19166723 0.08219189]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 327 start !\n",
            "Training  :: loss =  188.62611  : micro (f1) =  0.73668113538248  : macro (f2) =  0.728907335201842  : Epoch_runtime =  1.6827874183654785  \n",
            "\n",
            "train_loss =  188.62611\n",
            "Validation  :: loss =  976.54175  : micro (f1) =  0.6757601158271737  : macro (f2) =  0.6647316079527962  : Epoch_runtime =  1.6827874183654785  \n",
            "\n",
            "val_loss =  976.54175\n",
            "Weight Support =  [0.6346051  0.09109088 0.19199874 0.0823053 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 328 start !\n",
            "Training  :: loss =  188.97935  : micro (f1) =  0.7355184449589264  : macro (f2) =  0.7277719566583589  : Epoch_runtime =  1.6983983516693115  \n",
            "\n",
            "train_loss =  188.97935\n",
            "Validation  :: loss =  974.34894  : micro (f1) =  0.6780952380952382  : macro (f2) =  0.666205034764005  : Epoch_runtime =  1.6983983516693115  \n",
            "\n",
            "val_loss =  974.34894\n",
            "Weight Support =  [0.6335824  0.09178322 0.19200395 0.08263049]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 329 start !\n",
            "Training  :: loss =  188.70352  : micro (f1) =  0.7372439061745266  : macro (f2) =  0.7294889425107925  : Epoch_runtime =  1.6950359344482422  \n",
            "\n",
            "train_loss =  188.70352\n",
            "Validation  :: loss =  972.6123  : micro (f1) =  0.6791665085208941  : macro (f2) =  0.6677587424648214  : Epoch_runtime =  1.6950359344482422  \n",
            "\n",
            "val_loss =  972.6123\n",
            "Weight Support =  [0.63409024 0.09156758 0.19189356 0.0824487 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 330 start !\n",
            "Training  :: loss =  188.0708  : micro (f1) =  0.7367752259036144  : macro (f2) =  0.7290084240455523  : Epoch_runtime =  1.707331657409668  \n",
            "\n",
            "train_loss =  188.0708\n",
            "Validation  :: loss =  969.98987  : micro (f1) =  0.6837729034199932  : macro (f2) =  0.6726911465348643  : Epoch_runtime =  1.707331657409668  \n",
            "\n",
            "val_loss =  969.98987\n",
            "Weight Support =  [0.63458014 0.0914958  0.19167034 0.0822538 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 331 start !\n",
            "Training  :: loss =  188.53119  : micro (f1) =  0.7375701022668363  : macro (f2) =  0.7297755152364095  : Epoch_runtime =  1.6914029121398926  \n",
            "\n",
            "train_loss =  188.53119\n",
            "Validation  :: loss =  986.9118  : micro (f1) =  0.6812069160357103  : macro (f2) =  0.670329025334189  : Epoch_runtime =  1.6914029121398926  \n",
            "\n",
            "val_loss =  986.9118\n",
            "Weight Support =  [0.63548356 0.09131975 0.19141327 0.08178345]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 332 start !\n",
            "Training  :: loss =  188.2496  : micro (f1) =  0.7361764152717698  : macro (f2) =  0.7284330088408264  : Epoch_runtime =  1.6758642196655273  \n",
            "\n",
            "train_loss =  188.2496\n",
            "Validation  :: loss =  976.1887  : micro (f1) =  0.6809263792129542  : macro (f2) =  0.6702395852631036  : Epoch_runtime =  1.6758642196655273  \n",
            "\n",
            "val_loss =  976.1887\n",
            "Weight Support =  [0.63541013 0.09133672 0.19130668 0.08194651]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 333 start !\n",
            "Training  :: loss =  187.79626  : micro (f1) =  0.736533778776365  : macro (f2) =  0.728797679431384  : Epoch_runtime =  1.6441760063171387  \n",
            "\n",
            "train_loss =  187.79626\n",
            "Validation  :: loss =  961.90234  : micro (f1) =  0.6786902601330913  : macro (f2) =  0.6672795897155729  : Epoch_runtime =  1.6441760063171387  \n",
            "\n",
            "val_loss =  961.90234\n",
            "Weight Support =  [0.63458025 0.09139138 0.19188598 0.08214242]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 334 start !\n",
            "Training  :: loss =  187.88147  : micro (f1) =  0.7353545795204924  : macro (f2) =  0.7276185189632165  : Epoch_runtime =  1.6620769500732422  \n",
            "\n",
            "train_loss =  187.88147\n",
            "Validation  :: loss =  955.7511  : micro (f1) =  0.6802659069325735  : macro (f2) =  0.6688012878642335  : Epoch_runtime =  1.6620769500732422  \n",
            "\n",
            "val_loss =  955.7511\n",
            "Weight Support =  [0.6343593  0.0915721  0.19198199 0.08208662]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 335 start !\n",
            "Training  :: loss =  187.56085  : micro (f1) =  0.7365065173403604  : macro (f2) =  0.7287533150699401  : Epoch_runtime =  1.6986064910888672  \n",
            "\n",
            "train_loss =  187.56085\n",
            "Validation  :: loss =  970.7091  : micro (f1) =  0.6784479154790408  : macro (f2) =  0.6673143271058876  : Epoch_runtime =  1.6986064910888672  \n",
            "\n",
            "val_loss =  970.7091\n",
            "Weight Support =  [0.6343625  0.09161542 0.1921127  0.08190937]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 336 start !\n",
            "Training  :: loss =  187.78676  : micro (f1) =  0.7365660319663166  : macro (f2) =  0.7287855410898113  : Epoch_runtime =  1.7184100151062012  \n",
            "\n",
            "train_loss =  187.78676\n",
            "Validation  :: loss =  976.1769  : micro (f1) =  0.6784656285605772  : macro (f2) =  0.6673087778892473  : Epoch_runtime =  1.7184100151062012  \n",
            "\n",
            "val_loss =  976.1769\n",
            "Weight Support =  [0.63505965 0.09169082 0.1915044  0.0817451 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 337 start !\n",
            "Training  :: loss =  187.65485  : micro (f1) =  0.7381858902575589  : macro (f2) =  0.7304002220324342  : Epoch_runtime =  1.697157859802246  \n",
            "\n",
            "train_loss =  187.65485\n",
            "Validation  :: loss =  991.1643  : micro (f1) =  0.6820835373432262  : macro (f2) =  0.6710972641857532  : Epoch_runtime =  1.697157859802246  \n",
            "\n",
            "val_loss =  991.1643\n",
            "Weight Support =  [0.6352648  0.09177941 0.19115537 0.08180036]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 338 start !\n",
            "Training  :: loss =  187.56943  : micro (f1) =  0.7366252722029311  : macro (f2) =  0.728892070816278  : Epoch_runtime =  1.7025022506713867  \n",
            "\n",
            "train_loss =  187.56943\n",
            "Validation  :: loss =  975.1311  : micro (f1) =  0.6829725073039179  : macro (f2) =  0.6720572846655695  : Epoch_runtime =  1.7025022506713867  \n",
            "\n",
            "val_loss =  975.1311\n",
            "Weight Support =  [0.6347178  0.09222308 0.1912162  0.08184284]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 339 start !\n",
            "Training  :: loss =  187.81206  : micro (f1) =  0.7372732729905013  : macro (f2) =  0.7295754948379611  : Epoch_runtime =  1.712038516998291  \n",
            "\n",
            "train_loss =  187.81206\n",
            "Validation  :: loss =  982.3518  : micro (f1) =  0.6795171633345907  : macro (f2) =  0.6683002585488184  : Epoch_runtime =  1.712038516998291  \n",
            "\n",
            "val_loss =  982.3518\n",
            "Weight Support =  [0.63478565 0.09176946 0.1916219  0.08182302]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 340 start !\n",
            "Training  :: loss =  187.79596  : micro (f1) =  0.7356027402089234  : macro (f2) =  0.7278591588568888  : Epoch_runtime =  1.680701732635498  \n",
            "\n",
            "train_loss =  187.79596\n",
            "Validation  :: loss =  973.56903  : micro (f1) =  0.6784714339765417  : macro (f2) =  0.6673684930450423  : Epoch_runtime =  1.680701732635498  \n",
            "\n",
            "val_loss =  973.56903\n",
            "Weight Support =  [0.63450325 0.0917624  0.19210912 0.08162527]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 341 start !\n",
            "Training  :: loss =  187.83733  : micro (f1) =  0.7361220484009355  : macro (f2) =  0.7283364721271185  : Epoch_runtime =  1.6965456008911133  \n",
            "\n",
            "train_loss =  187.83733\n",
            "Validation  :: loss =  989.96515  : micro (f1) =  0.675073560319462  : macro (f2) =  0.6636620602984615  : Epoch_runtime =  1.6965456008911133  \n",
            "\n",
            "val_loss =  989.96515\n",
            "Weight Support =  [0.63510895 0.0915985  0.191875   0.08141748]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 342 start !\n",
            "Training  :: loss =  188.17668  : micro (f1) =  0.7362824780343218  : macro (f2) =  0.7285293538191463  : Epoch_runtime =  1.7019624710083008  \n",
            "\n",
            "train_loss =  188.17668\n",
            "Validation  :: loss =  974.7949  : micro (f1) =  0.6734935615757901  : macro (f2) =  0.6618802723932524  : Epoch_runtime =  1.7019624710083008  \n",
            "\n",
            "val_loss =  974.7949\n",
            "Weight Support =  [0.63459426 0.09242035 0.19147399 0.08151139]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 343 start !\n",
            "Training  :: loss =  188.62346  : micro (f1) =  0.7364273616861121  : macro (f2) =  0.7286274678718831  : Epoch_runtime =  1.7025752067565918  \n",
            "\n",
            "train_loss =  188.62346\n",
            "Validation  :: loss =  969.1662  : micro (f1) =  0.6776689408087815  : macro (f2) =  0.666032482329584  : Epoch_runtime =  1.7025752067565918  \n",
            "\n",
            "val_loss =  969.1662\n",
            "Weight Support =  [0.63524467 0.09243629 0.1908624  0.08145656]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 344 start !\n",
            "Training  :: loss =  188.31238  : micro (f1) =  0.7378359264497878  : macro (f2) =  0.7301087485273124  : Epoch_runtime =  1.6866183280944824  \n",
            "\n",
            "train_loss =  188.31238\n",
            "Validation  :: loss =  958.4847  : micro (f1) =  0.6831631120456594  : macro (f2) =  0.6720721384626545  : Epoch_runtime =  1.6866183280944824  \n",
            "\n",
            "val_loss =  958.4847\n",
            "Weight Support =  [0.63528925 0.09241597 0.19080555 0.08148915]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 345 start !\n",
            "Training  :: loss =  188.41579  : micro (f1) =  0.7372836422335074  : macro (f2) =  0.7295150406935909  : Epoch_runtime =  1.7046003341674805  \n",
            "\n",
            "train_loss =  188.41579\n",
            "Validation  :: loss =  987.3392  : micro (f1) =  0.685090692390778  : macro (f2) =  0.6747002197733144  : Epoch_runtime =  1.7046003341674805  \n",
            "\n",
            "val_loss =  987.3392\n",
            "Weight Support =  [0.6356016  0.09222116 0.19115674 0.0810205 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 346 start !\n",
            "Training  :: loss =  189.35678  : micro (f1) =  0.7373613694694954  : macro (f2) =  0.7296222230939746  : Epoch_runtime =  1.6976556777954102  \n",
            "\n",
            "train_loss =  189.35678\n",
            "Validation  :: loss =  985.0058  : micro (f1) =  0.6804363309217679  : macro (f2) =  0.6697284962656964  : Epoch_runtime =  1.6976556777954102  \n",
            "\n",
            "val_loss =  985.0058\n",
            "Weight Support =  [0.6353082  0.09242946 0.1911307  0.08113169]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 347 start !\n",
            "Training  :: loss =  189.80278  : micro (f1) =  0.7351099940099366  : macro (f2) =  0.7273725105585805  : Epoch_runtime =  1.722611427307129  \n",
            "\n",
            "train_loss =  189.80278\n",
            "Validation  :: loss =  981.2935  : micro (f1) =  0.6794891172914147  : macro (f2) =  0.6686008710096155  : Epoch_runtime =  1.722611427307129  \n",
            "\n",
            "val_loss =  981.2935\n",
            "Weight Support =  [0.6355385  0.09240605 0.19117364 0.08088177]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 348 start !\n",
            "Training  :: loss =  189.542  : micro (f1) =  0.7361408126336569  : macro (f2) =  0.7283702641366678  : Epoch_runtime =  1.6786048412322998  \n",
            "\n",
            "train_loss =  189.542\n",
            "Validation  :: loss =  975.72186  : micro (f1) =  0.6760111522743765  : macro (f2) =  0.6647066727320428  : Epoch_runtime =  1.6786048412322998  \n",
            "\n",
            "val_loss =  975.72186\n",
            "Weight Support =  [0.6359382  0.09200399 0.19114535 0.0809124 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 349 start !\n",
            "Training  :: loss =  189.79128  : micro (f1) =  0.7360340449543874  : macro (f2) =  0.7282858634703107  : Epoch_runtime =  1.6811740398406982  \n",
            "\n",
            "train_loss =  189.79128\n",
            "Validation  :: loss =  981.8028  : micro (f1) =  0.676657723265619  : macro (f2) =  0.6646292823427642  : Epoch_runtime =  1.6811740398406982  \n",
            "\n",
            "val_loss =  981.8028\n",
            "Weight Support =  [0.63532704 0.09289935 0.19129717 0.08047649]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 350 start !\n",
            "Training  :: loss =  189.7772  : micro (f1) =  0.7365202563650263  : macro (f2) =  0.7288075750483819  : Epoch_runtime =  1.6801061630249023  \n",
            "\n",
            "train_loss =  189.7772\n",
            "Validation  :: loss =  955.08276  : micro (f1) =  0.6769172216936251  : macro (f2) =  0.6652881239288113  : Epoch_runtime =  1.6801061630249023  \n",
            "\n",
            "val_loss =  955.08276\n",
            "Weight Support =  [0.6353334  0.09312618 0.19097888 0.08056153]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 351 start !\n",
            "Training  :: loss =  190.13747  : micro (f1) =  0.737253793233437  : macro (f2) =  0.7294655580255625  : Epoch_runtime =  1.6824092864990234  \n",
            "\n",
            "train_loss =  190.13747\n",
            "Validation  :: loss =  979.2017  : micro (f1) =  0.6779828358775728  : macro (f2) =  0.666887444838096  : Epoch_runtime =  1.6824092864990234  \n",
            "\n",
            "val_loss =  979.2017\n",
            "Weight Support =  [0.63618463 0.09241558 0.19088233 0.08051743]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 352 start !\n",
            "Training  :: loss =  189.1981  : micro (f1) =  0.7384343106601381  : macro (f2) =  0.7306301553885004  : Epoch_runtime =  1.7928214073181152  \n",
            "\n",
            "train_loss =  189.1981\n",
            "Validation  :: loss =  997.04016  : micro (f1) =  0.6841634357291977  : macro (f2) =  0.6734607629567353  : Epoch_runtime =  1.7928214073181152  \n",
            "\n",
            "val_loss =  997.04016\n",
            "Weight Support =  [0.636261   0.09283996 0.19059376 0.08030526]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 353 start !\n",
            "Training  :: loss =  189.82959  : micro (f1) =  0.7369623323560234  : macro (f2) =  0.7292100054890804  : Epoch_runtime =  1.7540109157562256  \n",
            "\n",
            "train_loss =  189.82959\n",
            "Validation  :: loss =  1003.87213  : micro (f1) =  0.6836325475057158  : macro (f2) =  0.6727995793129329  : Epoch_runtime =  1.7540109157562256  \n",
            "\n",
            "val_loss =  1003.87213\n",
            "Weight Support =  [0.63605106 0.09298813 0.19063231 0.08032845]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 354 start !\n",
            "Training  :: loss =  190.92119  : micro (f1) =  0.7364211615817083  : macro (f2) =  0.7287068671984105  : Epoch_runtime =  1.7167019844055176  \n",
            "\n",
            "train_loss =  190.92119\n",
            "Validation  :: loss =  976.58624  : micro (f1) =  0.6785930408472012  : macro (f2) =  0.6675562585733943  : Epoch_runtime =  1.7167019844055176  \n",
            "\n",
            "val_loss =  976.58624\n",
            "Weight Support =  [0.6361339  0.09213731 0.19127002 0.08045875]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 355 start !\n",
            "Training  :: loss =  190.2926  : micro (f1) =  0.7352630218734583  : macro (f2) =  0.7275157804215675  : Epoch_runtime =  1.727492094039917  \n",
            "\n",
            "train_loss =  190.2926\n",
            "Validation  :: loss =  971.9669  : micro (f1) =  0.6760606291415949  : macro (f2) =  0.6653081227659801  : Epoch_runtime =  1.727492094039917  \n",
            "\n",
            "val_loss =  971.9669\n",
            "Weight Support =  [0.6362026  0.09240904 0.19129294 0.08009538]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 356 start !\n",
            "Training  :: loss =  191.0927  : micro (f1) =  0.7365671114876792  : macro (f2) =  0.7288070569497541  : Epoch_runtime =  1.7607216835021973  \n",
            "\n",
            "train_loss =  191.0927\n",
            "Validation  :: loss =  979.8012  : micro (f1) =  0.6754510284598  : macro (f2) =  0.6640781194697382  : Epoch_runtime =  1.7607216835021973  \n",
            "\n",
            "val_loss =  979.8012\n",
            "Weight Support =  [0.63586736 0.09328705 0.19127011 0.0795755 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 357 start !\n",
            "Training  :: loss =  191.5014  : micro (f1) =  0.7351804653457206  : macro (f2) =  0.727444477707379  : Epoch_runtime =  1.7103567123413086  \n",
            "\n",
            "train_loss =  191.5014\n",
            "Validation  :: loss =  987.5782  : micro (f1) =  0.6750324204744832  : macro (f2) =  0.6631400032437782  : Epoch_runtime =  1.7103567123413086  \n",
            "\n",
            "val_loss =  987.5782\n",
            "Weight Support =  [0.6362758  0.0934212  0.19057919 0.0797238 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 358 start !\n",
            "Training  :: loss =  190.80687  : micro (f1) =  0.7367516538361938  : macro (f2) =  0.7290211764257357  : Epoch_runtime =  1.758436679840088  \n",
            "\n",
            "train_loss =  190.80687\n",
            "Validation  :: loss =  975.45593  : micro (f1) =  0.6795369707024622  : macro (f2) =  0.6684497932869611  : Epoch_runtime =  1.758436679840088  \n",
            "\n",
            "val_loss =  975.45593\n",
            "Weight Support =  [0.6376779  0.09268051 0.19003348 0.07960813]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 359 start !\n",
            "Training  :: loss =  189.4032  : micro (f1) =  0.7384651684067718  : macro (f2) =  0.7307097491911917  : Epoch_runtime =  1.7430636882781982  \n",
            "\n",
            "train_loss =  189.4032\n",
            "Validation  :: loss =  977.2948  : micro (f1) =  0.6849386778342806  : macro (f2) =  0.6738192270630232  : Epoch_runtime =  1.7430636882781982  \n",
            "\n",
            "val_loss =  977.2948\n",
            "Weight Support =  [0.63790345 0.09273204 0.1900702  0.07929437]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 360 start !\n",
            "Training  :: loss =  189.28287  : micro (f1) =  0.7379532976743091  : macro (f2) =  0.7302332162009814  : Epoch_runtime =  1.7276833057403564  \n",
            "\n",
            "train_loss =  189.28287\n",
            "Validation  :: loss =  982.5254  : micro (f1) =  0.6821460775473399  : macro (f2) =  0.6716108215791258  : Epoch_runtime =  1.7276833057403564  \n",
            "\n",
            "val_loss =  982.5254\n",
            "Weight Support =  [0.63791394 0.09300324 0.19011155 0.07897126]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 361 start !\n",
            "Training  :: loss =  189.05818  : micro (f1) =  0.736671215472213  : macro (f2) =  0.7288789792029831  : Epoch_runtime =  1.753544569015503  \n",
            "\n",
            "train_loss =  189.05818\n",
            "Validation  :: loss =  978.71204  : micro (f1) =  0.6767332727823192  : macro (f2) =  0.6658891900193704  : Epoch_runtime =  1.753544569015503  \n",
            "\n",
            "val_loss =  978.71204\n",
            "Weight Support =  [0.63755715 0.09308637 0.19026624 0.07909018]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 362 start !\n",
            "Training  :: loss =  188.35744  : micro (f1) =  0.7352568018420187  : macro (f2) =  0.727506174344136  : Epoch_runtime =  1.796748161315918  \n",
            "\n",
            "train_loss =  188.35744\n",
            "Validation  :: loss =  971.00653  : micro (f1) =  0.6763821772383091  : macro (f2) =  0.6650437969496124  : Epoch_runtime =  1.796748161315918  \n",
            "\n",
            "val_loss =  971.00653\n",
            "Weight Support =  [0.63706243 0.09348654 0.19030248 0.0791486 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 363 start !\n",
            "Training  :: loss =  188.1924  : micro (f1) =  0.7357113474760535  : macro (f2) =  0.7279460287261172  : Epoch_runtime =  1.7451274394989014  \n",
            "\n",
            "train_loss =  188.1924\n",
            "Validation  :: loss =  966.3605  : micro (f1) =  0.676003520453067  : macro (f2) =  0.6641780294377496  : Epoch_runtime =  1.7451274394989014  \n",
            "\n",
            "val_loss =  966.3605\n",
            "Weight Support =  [0.63687927 0.09357424 0.1904231  0.07912343]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 364 start !\n",
            "Training  :: loss =  188.32808  : micro (f1) =  0.7362019479602878  : macro (f2) =  0.7284608006234083  : Epoch_runtime =  1.7715744972229004  \n",
            "\n",
            "train_loss =  188.32808\n",
            "Validation  :: loss =  963.14453  : micro (f1) =  0.6779609626681826  : macro (f2) =  0.6662525823282004  : Epoch_runtime =  1.7715744972229004  \n",
            "\n",
            "val_loss =  963.14453\n",
            "Weight Support =  [0.63821894 0.0931692  0.1897143  0.07889754]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 365 start !\n",
            "Training  :: loss =  187.47237  : micro (f1) =  0.7377574586416552  : macro (f2) =  0.7299677329367101  : Epoch_runtime =  1.7553200721740723  \n",
            "\n",
            "train_loss =  187.47237\n",
            "Validation  :: loss =  970.4802  : micro (f1) =  0.6818729298404095  : macro (f2) =  0.6708079279252053  : Epoch_runtime =  1.7553200721740723  \n",
            "\n",
            "val_loss =  970.4802\n",
            "Weight Support =  [0.6393661  0.092666   0.18939407 0.0785738 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 366 start !\n",
            "Training  :: loss =  186.99889  : micro (f1) =  0.7383091616554652  : macro (f2) =  0.7305628263667006  : Epoch_runtime =  1.7459323406219482  \n",
            "\n",
            "train_loss =  186.99889\n",
            "Validation  :: loss =  977.8988  : micro (f1) =  0.6835776024051109  : macro (f2) =  0.6726937210734227  : Epoch_runtime =  1.7459323406219482  \n",
            "\n",
            "val_loss =  977.8988\n",
            "Weight Support =  [0.6388725  0.09314589 0.18950932 0.07847238]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 367 start !\n",
            "Training  :: loss =  187.30908  : micro (f1) =  0.7372125320716522  : macro (f2) =  0.7294867796432634  : Epoch_runtime =  1.7702603340148926  \n",
            "\n",
            "train_loss =  187.30908\n",
            "Validation  :: loss =  985.19806  : micro (f1) =  0.6784337440471692  : macro (f2) =  0.6674180238509868  : Epoch_runtime =  1.7702603340148926  \n",
            "\n",
            "val_loss =  985.19806\n",
            "Weight Support =  [0.63821673 0.093334   0.18987384 0.07857538]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 368 start !\n",
            "Training  :: loss =  187.08824  : micro (f1) =  0.7359435626102294  : macro (f2) =  0.7281906510467567  : Epoch_runtime =  1.770472764968872  \n",
            "\n",
            "train_loss =  187.08824\n",
            "Validation  :: loss =  993.83124  : micro (f1) =  0.6747843346820368  : macro (f2) =  0.6639098398017447  : Epoch_runtime =  1.770472764968872  \n",
            "\n",
            "val_loss =  993.83124\n",
            "Weight Support =  [0.63806283 0.09345673 0.1898376  0.0786428 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 369 start !\n",
            "Training  :: loss =  187.14879  : micro (f1) =  0.735543888086261  : macro (f2) =  0.7278057312646801  : Epoch_runtime =  1.8071422576904297  \n",
            "\n",
            "train_loss =  187.14879\n",
            "Validation  :: loss =  976.8662  : micro (f1) =  0.6773258032511639  : macro (f2) =  0.6660618965075713  : Epoch_runtime =  1.8071422576904297  \n",
            "\n",
            "val_loss =  976.8662\n",
            "Weight Support =  [0.638031   0.09352509 0.18992531 0.07851864]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 370 start !\n",
            "Training  :: loss =  187.18416  : micro (f1) =  0.7361107845672772  : macro (f2) =  0.728362301670965  : Epoch_runtime =  1.7246127128601074  \n",
            "\n",
            "train_loss =  187.18416\n",
            "Validation  :: loss =  963.0646  : micro (f1) =  0.6784642843580299  : macro (f2) =  0.6670626671657999  : Epoch_runtime =  1.7246127128601074  \n",
            "\n",
            "val_loss =  963.0646\n",
            "Weight Support =  [0.6389494  0.09313808 0.18954428 0.07836825]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 371 start !\n",
            "Training  :: loss =  187.27313  : micro (f1) =  0.7375961731056992  : macro (f2) =  0.7297963957528302  : Epoch_runtime =  1.7101733684539795  \n",
            "\n",
            "train_loss =  187.27313\n",
            "Validation  :: loss =  960.51935  : micro (f1) =  0.6807854348480289  : macro (f2) =  0.669583041002327  : Epoch_runtime =  1.7101733684539795  \n",
            "\n",
            "val_loss =  960.51935\n",
            "Weight Support =  [0.6390294  0.0931728  0.18952699 0.07827082]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 372 start !\n",
            "Training  :: loss =  186.80225  : micro (f1) =  0.7379192455054525  : macro (f2) =  0.730166266925436  : Epoch_runtime =  1.735215663909912  \n",
            "\n",
            "train_loss =  186.80225\n",
            "Validation  :: loss =  975.5313  : micro (f1) =  0.6824932493249325  : macro (f2) =  0.6713142179919768  : Epoch_runtime =  1.735215663909912  \n",
            "\n",
            "val_loss =  975.5313\n",
            "Weight Support =  [0.6390221  0.09314016 0.18956916 0.07826862]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 373 start !\n",
            "Training  :: loss =  186.95816  : micro (f1) =  0.7369474365766083  : macro (f2) =  0.7291960091147768  : Epoch_runtime =  1.6880054473876953  \n",
            "\n",
            "train_loss =  186.95816\n",
            "Validation  :: loss =  997.2785  : micro (f1) =  0.6798610061942891  : macro (f2) =  0.6687236744882252  : Epoch_runtime =  1.6880054473876953  \n",
            "\n",
            "val_loss =  997.2785\n",
            "Weight Support =  [0.63866776 0.09323821 0.19003612 0.07805792]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 374 start !\n",
            "Training  :: loss =  187.10512  : micro (f1) =  0.7365537028762259  : macro (f2) =  0.728814459430453  : Epoch_runtime =  1.6870253086090088  \n",
            "\n",
            "train_loss =  187.10512\n",
            "Validation  :: loss =  1002.4835  : micro (f1) =  0.6754921410041203  : macro (f2) =  0.6644940117901563  : Epoch_runtime =  1.6870253086090088  \n",
            "\n",
            "val_loss =  1002.4835\n",
            "Weight Support =  [0.63908756 0.0932385  0.18971948 0.0779545 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 375 start !\n",
            "Training  :: loss =  187.02965  : micro (f1) =  0.7364720269030877  : macro (f2) =  0.728693226774253  : Epoch_runtime =  1.714921236038208  \n",
            "\n",
            "train_loss =  187.02965\n",
            "Validation  :: loss =  983.4601  : micro (f1) =  0.677311180550263  : macro (f2) =  0.6660789056031116  : Epoch_runtime =  1.714921236038208  \n",
            "\n",
            "val_loss =  983.4601\n",
            "Weight Support =  [0.63874364 0.09367786 0.18972266 0.07785584]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 376 start !\n",
            "Training  :: loss =  187.202  : micro (f1) =  0.7361661589847931  : macro (f2) =  0.7284024186740511  : Epoch_runtime =  1.711353063583374  \n",
            "\n",
            "train_loss =  187.202\n",
            "Validation  :: loss =  963.1798  : micro (f1) =  0.678907435508346  : macro (f2) =  0.6677064070289077  : Epoch_runtime =  1.711353063583374  \n",
            "\n",
            "val_loss =  963.1798\n",
            "Weight Support =  [0.6390115  0.09360277 0.18947446 0.07791126]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 377 start !\n",
            "Training  :: loss =  187.34494  : micro (f1) =  0.7373144876325088  : macro (f2) =  0.7295274882130106  : Epoch_runtime =  1.7310969829559326  \n",
            "\n",
            "train_loss =  187.34494\n",
            "Validation  :: loss =  953.98083  : micro (f1) =  0.6788942253148281  : macro (f2) =  0.6676660958590209  : Epoch_runtime =  1.7310969829559326  \n",
            "\n",
            "val_loss =  953.98083\n",
            "Weight Support =  [0.6382248  0.09368689 0.19013584 0.07795256]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 378 start !\n",
            "Training  :: loss =  187.05511  : micro (f1) =  0.7368867224753629  : macro (f2) =  0.7291138594448605  : Epoch_runtime =  1.7481052875518799  \n",
            "\n",
            "train_loss =  187.05511\n",
            "Validation  :: loss =  967.7361  : micro (f1) =  0.6801521829208573  : macro (f2) =  0.6690188178217711  : Epoch_runtime =  1.7481052875518799  \n",
            "\n",
            "val_loss =  967.7361\n",
            "Weight Support =  [0.6388425  0.0933509  0.18997814 0.07782839]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 379 start !\n",
            "Training  :: loss =  187.18877  : micro (f1) =  0.7375522458350504  : macro (f2) =  0.7297940214634648  : Epoch_runtime =  1.7116687297821045  \n",
            "\n",
            "train_loss =  187.18877\n",
            "Validation  :: loss =  979.8602  : micro (f1) =  0.6815486307837582  : macro (f2) =  0.670432664872397  : Epoch_runtime =  1.7116687297821045  \n",
            "\n",
            "val_loss =  979.8602\n",
            "Weight Support =  [0.63925713 0.09310879 0.19003397 0.0776    ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 380 start !\n",
            "Training  :: loss =  187.62656  : micro (f1) =  0.7360869360681187  : macro (f2) =  0.7283643000061378  : Epoch_runtime =  1.7470602989196777  \n",
            "\n",
            "train_loss =  187.62656\n",
            "Validation  :: loss =  985.8908  : micro (f1) =  0.6773642881529532  : macro (f2) =  0.665614201213796  : Epoch_runtime =  1.7470602989196777  \n",
            "\n",
            "val_loss =  985.8908\n",
            "Weight Support =  [0.64005226 0.09334407 0.18930815 0.0772955 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 381 start !\n",
            "Training  :: loss =  187.31871  : micro (f1) =  0.736928796649254  : macro (f2) =  0.7291826250564828  : Epoch_runtime =  1.7442200183868408  \n",
            "\n",
            "train_loss =  187.31871\n",
            "Validation  :: loss =  991.4762  : micro (f1) =  0.6775839744077995  : macro (f2) =  0.6669187067815925  : Epoch_runtime =  1.7442200183868408  \n",
            "\n",
            "val_loss =  991.4762\n",
            "Weight Support =  [0.639797   0.09328385 0.18954007 0.0773791 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 382 start !\n",
            "Training  :: loss =  187.3457  : micro (f1) =  0.736542510026227  : macro (f2) =  0.7287894386153515  : Epoch_runtime =  1.7264344692230225  \n",
            "\n",
            "train_loss =  187.3457\n",
            "Validation  :: loss =  969.16174  : micro (f1) =  0.6790747141717628  : macro (f2) =  0.6679492458898907  : Epoch_runtime =  1.7264344692230225  \n",
            "\n",
            "val_loss =  969.16174\n",
            "Weight Support =  [0.6390233  0.09390678 0.18973252 0.07733736]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 383 start !\n",
            "Training  :: loss =  187.19781  : micro (f1) =  0.7371098211447209  : macro (f2) =  0.7293377869277481  : Epoch_runtime =  1.7121517658233643  \n",
            "\n",
            "train_loss =  187.19781\n",
            "Validation  :: loss =  966.80194  : micro (f1) =  0.6785092038481934  : macro (f2) =  0.6674988098722877  : Epoch_runtime =  1.7121517658233643  \n",
            "\n",
            "val_loss =  966.80194\n",
            "Weight Support =  [0.63783544 0.09404559 0.19060217 0.0775168 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 384 start !\n",
            "Training  :: loss =  186.91988  : micro (f1) =  0.7356637771043296  : macro (f2) =  0.7278914484644595  : Epoch_runtime =  1.7092382907867432  \n",
            "\n",
            "train_loss =  186.91988\n",
            "Validation  :: loss =  957.1785  : micro (f1) =  0.6809185677594802  : macro (f2) =  0.6698692182656619  : Epoch_runtime =  1.7092382907867432  \n",
            "\n",
            "val_loss =  957.1785\n",
            "Weight Support =  [0.63848317 0.09379704 0.19025539 0.0774644 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 385 start !\n",
            "Training  :: loss =  186.88205  : micro (f1) =  0.7371754125762177  : macro (f2) =  0.7293982204160926  : Epoch_runtime =  1.7444586753845215  \n",
            "\n",
            "train_loss =  186.88205\n",
            "Validation  :: loss =  970.95123  : micro (f1) =  0.6820739398058987  : macro (f2) =  0.6705604647893325  : Epoch_runtime =  1.7444586753845215  \n",
            "\n",
            "val_loss =  970.95123\n",
            "Weight Support =  [0.63947064 0.09347658 0.18973738 0.07731546]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 386 start !\n",
            "Training  :: loss =  186.92995  : micro (f1) =  0.736242928883087  : macro (f2) =  0.7285348798364235  : Epoch_runtime =  1.7242751121520996  \n",
            "\n",
            "train_loss =  186.92995\n",
            "Validation  :: loss =  976.0264  : micro (f1) =  0.6822732404968009  : macro (f2) =  0.6710690077796907  : Epoch_runtime =  1.7242751121520996  \n",
            "\n",
            "val_loss =  976.0264\n",
            "Weight Support =  [0.64045876 0.09327611 0.18931027 0.07695486]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 387 start !\n",
            "Training  :: loss =  186.9568  : micro (f1) =  0.7373392377632261  : macro (f2) =  0.7295598831800317  : Epoch_runtime =  1.7153196334838867  \n",
            "\n",
            "train_loss =  186.9568\n",
            "Validation  :: loss =  985.9268  : micro (f1) =  0.6791597400634728  : macro (f2) =  0.6681116615223854  : Epoch_runtime =  1.7153196334838867  \n",
            "\n",
            "val_loss =  985.9268\n",
            "Weight Support =  [0.6408509  0.09309552 0.18920217 0.07685144]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 388 start !\n",
            "Training  :: loss =  186.95108  : micro (f1) =  0.7362824780343218  : macro (f2) =  0.7284862967029184  : Epoch_runtime =  1.7211155891418457  \n",
            "\n",
            "train_loss =  186.95108\n",
            "Validation  :: loss =  977.05316  : micro (f1) =  0.6768494243702269  : macro (f2) =  0.6661318270499617  : Epoch_runtime =  1.7211155891418457  \n",
            "\n",
            "val_loss =  977.05316\n",
            "Weight Support =  [0.6388498  0.09425529 0.18992221 0.07697266]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 389 start !\n",
            "Training  :: loss =  186.86598  : micro (f1) =  0.7372796533330194  : macro (f2) =  0.7295147463572158  : Epoch_runtime =  1.7024757862091064  \n",
            "\n",
            "train_loss =  186.86598\n",
            "Validation  :: loss =  984.6932  : micro (f1) =  0.6763450693491846  : macro (f2) =  0.6652714458754531  : Epoch_runtime =  1.7024757862091064  \n",
            "\n",
            "val_loss =  984.6932\n",
            "Weight Support =  [0.6383719  0.09452561 0.19011986 0.0769827 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 390 start !\n",
            "Training  :: loss =  187.28865  : micro (f1) =  0.7352664944822729  : macro (f2) =  0.7274928931904707  : Epoch_runtime =  1.709096908569336  \n",
            "\n",
            "train_loss =  187.28865\n",
            "Validation  :: loss =  963.2158  : micro (f1) =  0.6783919597989949  : macro (f2) =  0.6671957150763377  : Epoch_runtime =  1.709096908569336  \n",
            "\n",
            "val_loss =  963.2158\n",
            "Weight Support =  [0.6388121  0.09400769 0.19022933 0.07695084]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 391 start !\n",
            "Training  :: loss =  187.19055  : micro (f1) =  0.7372633015970155  : macro (f2) =  0.729492017858647  : Epoch_runtime =  1.74436616897583  \n",
            "\n",
            "train_loss =  187.19055\n",
            "Validation  :: loss =  956.5578  : micro (f1) =  0.678714859437751  : macro (f2) =  0.6671331939434801  : Epoch_runtime =  1.74436616897583  \n",
            "\n",
            "val_loss =  956.5578\n",
            "Weight Support =  [0.63967294 0.09405296 0.18950851 0.07676566]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 392 start !\n",
            "Training  :: loss =  187.04327  : micro (f1) =  0.7366092961424892  : macro (f2) =  0.728855525879038  : Epoch_runtime =  1.7333474159240723  \n",
            "\n",
            "train_loss =  187.04327\n",
            "Validation  :: loss =  968.8779  : micro (f1) =  0.6807389848365129  : macro (f2) =  0.6694664122434868  : Epoch_runtime =  1.7333474159240723  \n",
            "\n",
            "val_loss =  968.8779\n",
            "Weight Support =  [0.64072967 0.09350462 0.18934664 0.0764191 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 393 start !\n",
            "Training  :: loss =  187.13516  : micro (f1) =  0.7378959035917271  : macro (f2) =  0.7301328263587007  : Epoch_runtime =  1.701073169708252  \n",
            "\n",
            "train_loss =  187.13516\n",
            "Validation  :: loss =  979.6849  : micro (f1) =  0.683734374413454  : macro (f2) =  0.672751474358478  : Epoch_runtime =  1.701073169708252  \n",
            "\n",
            "val_loss =  979.6849\n",
            "Weight Support =  [0.64084923 0.09383743 0.18892711 0.07638624]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 394 start !\n",
            "Training  :: loss =  187.145  : micro (f1) =  0.7368185775121756  : macro (f2) =  0.7290721770091839  : Epoch_runtime =  1.7196447849273682  \n",
            "\n",
            "train_loss =  187.145\n",
            "Validation  :: loss =  995.9609  : micro (f1) =  0.6775513296461854  : macro (f2) =  0.6665604821969924  : Epoch_runtime =  1.7196447849273682  \n",
            "\n",
            "val_loss =  995.9609\n",
            "Weight Support =  [0.63859445 0.09484699 0.19013742 0.07642109]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 395 start !\n",
            "Training  :: loss =  188.16185  : micro (f1) =  0.7371222022064451  : macro (f2) =  0.7293441387723663  : Epoch_runtime =  1.6942949295043945  \n",
            "\n",
            "train_loss =  188.16185\n",
            "Validation  :: loss =  998.9349  : micro (f1) =  0.6763380926929832  : macro (f2) =  0.6650417851392526  : Epoch_runtime =  1.6942949295043945  \n",
            "\n",
            "val_loss =  998.9349\n",
            "Weight Support =  [0.6392278  0.09414931 0.19004835 0.0765746 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 396 start !\n",
            "Training  :: loss =  188.45642  : micro (f1) =  0.7364758434230633  : macro (f2) =  0.7287261591007573  : Epoch_runtime =  1.7070879936218262  \n",
            "\n",
            "train_loss =  188.45642\n",
            "Validation  :: loss =  973.3856  : micro (f1) =  0.6784819734345352  : macro (f2) =  0.667625230867688  : Epoch_runtime =  1.7070879936218262  \n",
            "\n",
            "val_loss =  973.3856\n",
            "Weight Support =  [0.6388507  0.09409562 0.19061148 0.07644226]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 397 start !\n",
            "Training  :: loss =  188.054  : micro (f1) =  0.7369783698543084  : macro (f2) =  0.729218396862021  : Epoch_runtime =  1.740891695022583  \n",
            "\n",
            "train_loss =  188.054\n",
            "Validation  :: loss =  976.5752  : micro (f1) =  0.6775756188732501  : macro (f2) =  0.6663427979213493  : Epoch_runtime =  1.740891695022583  \n",
            "\n",
            "val_loss =  976.5752\n",
            "Weight Support =  [0.6394849  0.09430496 0.18998896 0.07622121]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 398 start !\n",
            "Training  :: loss =  187.62395  : micro (f1) =  0.7360193695493759  : macro (f2) =  0.7283052624286134  : Epoch_runtime =  1.701270341873169  \n",
            "\n",
            "train_loss =  187.62395\n",
            "Validation  :: loss =  965.2748  : micro (f1) =  0.679252450049381  : macro (f2) =  0.6678344487383163  : Epoch_runtime =  1.701270341873169  \n",
            "\n",
            "val_loss =  965.2748\n",
            "Weight Support =  [0.6395112  0.09415729 0.19036578 0.07596572]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 399 start !\n",
            "Training  :: loss =  187.65749  : micro (f1) =  0.7364332874342406  : macro (f2) =  0.728662455199939  : Epoch_runtime =  1.7170886993408203  \n",
            "\n",
            "train_loss =  187.65749\n",
            "Validation  :: loss =  962.82806  : micro (f1) =  0.6782569085939872  : macro (f2) =  0.6668741571435649  : Epoch_runtime =  1.7170886993408203  \n",
            "\n",
            "val_loss =  962.82806\n",
            "Weight Support =  [0.63970643 0.09461872 0.18964235 0.07603247]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 400 start !\n",
            "Training  :: loss =  187.50288  : micro (f1) =  0.7374618763321205  : macro (f2) =  0.72967548316374  : Epoch_runtime =  1.7279953956604004  \n",
            "\n",
            "train_loss =  187.50288\n",
            "Validation  :: loss =  970.1155  : micro (f1) =  0.678788795221714  : macro (f2) =  0.6674171011781631  : Epoch_runtime =  1.7279953956604004  \n",
            "\n",
            "val_loss =  970.1155\n",
            "Weight Support =  [0.6394782  0.09481172 0.18961203 0.07609805]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 401 start !\n",
            "Training  :: loss =  187.2583  : micro (f1) =  0.7372364236070208  : macro (f2) =  0.7294721077114481  : Epoch_runtime =  1.719508171081543  \n",
            "\n",
            "train_loss =  187.2583\n",
            "Validation  :: loss =  992.37964  : micro (f1) =  0.6798320281466349  : macro (f2) =  0.6688456288235075  : Epoch_runtime =  1.719508171081543  \n",
            "\n",
            "val_loss =  992.37964\n",
            "Weight Support =  [0.6394613  0.09478746 0.18958233 0.07616893]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 402 start !\n",
            "Training  :: loss =  187.65312  : micro (f1) =  0.7377906565835041  : macro (f2) =  0.7299831876699056  : Epoch_runtime =  1.748164415359497  \n",
            "\n",
            "train_loss =  187.65312\n",
            "Validation  :: loss =  972.6258  : micro (f1) =  0.682102722678935  : macro (f2) =  0.6712618530352895  : Epoch_runtime =  1.748164415359497  \n",
            "\n",
            "val_loss =  972.6258\n",
            "Weight Support =  [0.63905513 0.09472798 0.19011682 0.07610003]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 403 start !\n",
            "Training  :: loss =  187.71785  : micro (f1) =  0.7367777006436127  : macro (f2) =  0.7290331077374876  : Epoch_runtime =  1.6786186695098877  \n",
            "\n",
            "train_loss =  187.71785\n",
            "Validation  :: loss =  990.6746  : micro (f1) =  0.6797932933499302  : macro (f2) =  0.669166899778374  : Epoch_runtime =  1.6786186695098877  \n",
            "\n",
            "val_loss =  990.6746\n",
            "Weight Support =  [0.63975203 0.09469327 0.18983316 0.07572147]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 404 start !\n",
            "Training  :: loss =  187.7088  : micro (f1) =  0.7366379006278364  : macro (f2) =  0.7288625365603425  : Epoch_runtime =  1.6707713603973389  \n",
            "\n",
            "train_loss =  187.7088\n",
            "Validation  :: loss =  980.06335  : micro (f1) =  0.6796167973039494  : macro (f2) =  0.66892945659526  : Epoch_runtime =  1.6707713603973389  \n",
            "\n",
            "val_loss =  980.06335\n",
            "Weight Support =  [0.6401005  0.09429611 0.19015688 0.07544648]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 405 start !\n",
            "Training  :: loss =  188.46597  : micro (f1) =  0.7352229763050984  : macro (f2) =  0.72750583624545  : Epoch_runtime =  1.6880240440368652  \n",
            "\n",
            "train_loss =  188.46597\n",
            "Validation  :: loss =  974.5197  : micro (f1) =  0.6762452837379473  : macro (f2) =  0.6652988400025572  : Epoch_runtime =  1.6880240440368652  \n",
            "\n",
            "val_loss =  974.5197\n",
            "Weight Support =  [0.6383557  0.09491649 0.1912801  0.07544778]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 406 start !\n",
            "Training  :: loss =  189.24107  : micro (f1) =  0.7357051349858373  : macro (f2) =  0.7279314661860662  : Epoch_runtime =  1.7098300457000732  \n",
            "\n",
            "train_loss =  189.24107\n",
            "Validation  :: loss =  958.09235  : micro (f1) =  0.6789924973204715  : macro (f2) =  0.6672367225311755  : Epoch_runtime =  1.7098300457000732  \n",
            "\n",
            "val_loss =  958.09235\n",
            "Weight Support =  [0.6373391  0.09570631 0.1913639  0.07559073]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 407 start !\n",
            "Training  :: loss =  189.84164  : micro (f1) =  0.7361052284813404  : macro (f2) =  0.7283407031574756  : Epoch_runtime =  1.6992487907409668  \n",
            "\n",
            "train_loss =  189.84164\n",
            "Validation  :: loss =  968.08264  : micro (f1) =  0.6781766902965165  : macro (f2) =  0.6663321815337575  : Epoch_runtime =  1.6992487907409668  \n",
            "\n",
            "val_loss =  968.08264\n",
            "Weight Support =  [0.63806677 0.09523924 0.1908732  0.07582074]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 408 start !\n",
            "Training  :: loss =  188.98712  : micro (f1) =  0.7385276174967844  : macro (f2) =  0.7307364330997601  : Epoch_runtime =  1.7361183166503906  \n",
            "\n",
            "train_loss =  188.98712\n",
            "Validation  :: loss =  983.17426  : micro (f1) =  0.6835727951382375  : macro (f2) =  0.6723139137360643  : Epoch_runtime =  1.7361183166503906  \n",
            "\n",
            "val_loss =  983.17426\n",
            "Weight Support =  [0.63927025 0.09485029 0.19034366 0.0755358 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 409 start !\n",
            "Training  :: loss =  188.49155  : micro (f1) =  0.7373208898944194  : macro (f2) =  0.7295137968235242  : Epoch_runtime =  1.7118232250213623  \n",
            "\n",
            "train_loss =  188.49155\n",
            "Validation  :: loss =  1000.70807  : micro (f1) =  0.6848498397794172  : macro (f2) =  0.6742707962009198  : Epoch_runtime =  1.7118232250213623  \n",
            "\n",
            "val_loss =  1000.70807\n",
            "Weight Support =  [0.6393498  0.0954148  0.19017054 0.07506477]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 410 start !\n",
            "Training  :: loss =  189.77847  : micro (f1) =  0.737813937035727  : macro (f2) =  0.7300409461273892  : Epoch_runtime =  1.7017474174499512  \n",
            "\n",
            "train_loss =  189.77847\n",
            "Validation  :: loss =  992.39825  : micro (f1) =  0.6811933534743202  : macro (f2) =  0.6701743943216899  : Epoch_runtime =  1.7017474174499512  \n",
            "\n",
            "val_loss =  992.39825\n",
            "Weight Support =  [0.6387214  0.09537835 0.19086151 0.07503866]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 411 start !\n",
            "Training  :: loss =  189.4447  : micro (f1) =  0.7349253310791772  : macro (f2) =  0.7271566068005  : Epoch_runtime =  1.7332189083099365  \n",
            "\n",
            "train_loss =  189.4447\n",
            "Validation  :: loss =  989.1356  : micro (f1) =  0.6769978729869341  : macro (f2) =  0.6663006847149121  : Epoch_runtime =  1.7332189083099365  \n",
            "\n",
            "val_loss =  989.1356\n",
            "Weight Support =  [0.6387851  0.09522897 0.19105734 0.0749286 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 412 start !\n",
            "Training  :: loss =  188.77676  : micro (f1) =  0.7365139859317289  : macro (f2) =  0.728796823944543  : Epoch_runtime =  1.7341911792755127  \n",
            "\n",
            "train_loss =  188.77676\n",
            "Validation  :: loss =  974.9251  : micro (f1) =  0.6770323960880196  : macro (f2) =  0.6660222811302083  : Epoch_runtime =  1.7341911792755127  \n",
            "\n",
            "val_loss =  974.9251\n",
            "Weight Support =  [0.6387495  0.09483121 0.19140658 0.07501278]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 413 start !\n",
            "Training  :: loss =  189.3792  : micro (f1) =  0.7357765158195477  : macro (f2) =  0.7280476499285955  : Epoch_runtime =  1.6932108402252197  \n",
            "\n",
            "train_loss =  189.3792\n",
            "Validation  :: loss =  956.62555  : micro (f1) =  0.6763921688123715  : macro (f2) =  0.6646266987191589  : Epoch_runtime =  1.6932108402252197  \n",
            "\n",
            "val_loss =  956.62555\n",
            "Weight Support =  [0.6388019  0.09558862 0.19076318 0.07484634]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 414 start !\n",
            "Training  :: loss =  189.31453  : micro (f1) =  0.7359966130796279  : macro (f2) =  0.7282185316372416  : Epoch_runtime =  1.7129790782928467  \n",
            "\n",
            "train_loss =  189.31453\n",
            "Validation  :: loss =  964.5743  : micro (f1) =  0.6782694797775916  : macro (f2) =  0.6666700256883116  : Epoch_runtime =  1.7129790782928467  \n",
            "\n",
            "val_loss =  964.5743\n",
            "Weight Support =  [0.6407242  0.09498887 0.18961632 0.07467063]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 415 start !\n",
            "Training  :: loss =  188.56474  : micro (f1) =  0.7377800827430133  : macro (f2) =  0.7299708090680339  : Epoch_runtime =  1.709298849105835  \n",
            "\n",
            "train_loss =  188.56474\n",
            "Validation  :: loss =  994.53894  : micro (f1) =  0.6816572504708097  : macro (f2) =  0.670579285393782  : Epoch_runtime =  1.709298849105835  \n",
            "\n",
            "val_loss =  994.53894\n",
            "Weight Support =  [0.64097214 0.09536122 0.18924682 0.07441983]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 416 start !\n",
            "Training  :: loss =  188.10138  : micro (f1) =  0.7386564734196926  : macro (f2) =  0.7308990165384881  : Epoch_runtime =  1.7147150039672852  \n",
            "\n",
            "train_loss =  188.10138\n",
            "Validation  :: loss =  1006.3316  : micro (f1) =  0.6819093765378355  : macro (f2) =  0.6710596557212124  : Epoch_runtime =  1.7147150039672852  \n",
            "\n",
            "val_loss =  1006.3316\n",
            "Weight Support =  [0.6407317  0.09530754 0.18970619 0.07425454]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 417 start !\n",
            "Training  :: loss =  188.2212  : micro (f1) =  0.7361565049730773  : macro (f2) =  0.7283769673038426  : Epoch_runtime =  1.6949095726013184  \n",
            "\n",
            "train_loss =  188.2212\n",
            "Validation  :: loss =  985.6363  : micro (f1) =  0.6794628632121995  : macro (f2) =  0.668515848405295  : Epoch_runtime =  1.6949095726013184  \n",
            "\n",
            "val_loss =  985.6363\n",
            "Weight Support =  [0.64012295 0.09517718 0.19027694 0.07442293]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 418 start !\n",
            "Training  :: loss =  187.8294  : micro (f1) =  0.7370886493102962  : macro (f2) =  0.7293639045705278  : Epoch_runtime =  1.7523481845855713  \n",
            "\n",
            "train_loss =  187.8294\n",
            "Validation  :: loss =  997.4818  : micro (f1) =  0.6745146002140345  : macro (f2) =  0.6635140160219927  : Epoch_runtime =  1.7523481845855713  \n",
            "\n",
            "val_loss =  997.4818\n",
            "Weight Support =  [0.6404486  0.09488843 0.19031791 0.07434504]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 419 start !\n",
            "Training  :: loss =  187.80788  : micro (f1) =  0.7344756007458748  : macro (f2) =  0.7267401914610434  : Epoch_runtime =  1.7286181449890137  \n",
            "\n",
            "train_loss =  187.80788\n",
            "Validation  :: loss =  992.89465  : micro (f1) =  0.6742957746478873  : macro (f2) =  0.6628787142786842  : Epoch_runtime =  1.7286181449890137  \n",
            "\n",
            "val_loss =  992.89465\n",
            "Weight Support =  [0.6405677  0.09501062 0.19020785 0.0742138 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 420 start !\n",
            "Training  :: loss =  188.04033  : micro (f1) =  0.7363048335881452  : macro (f2) =  0.7285174623587738  : Epoch_runtime =  1.7158918380737305  \n",
            "\n",
            "train_loss =  188.04033\n",
            "Validation  :: loss =  983.03766  : micro (f1) =  0.6752928565959727  : macro (f2) =  0.6636671602289028  : Epoch_runtime =  1.7158918380737305  \n",
            "\n",
            "val_loss =  983.03766\n",
            "Weight Support =  [0.6402699  0.09575927 0.18993069 0.07404018]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 421 start !\n",
            "Training  :: loss =  188.04388  : micro (f1) =  0.736145244170596  : macro (f2) =  0.7283707006905573  : Epoch_runtime =  1.7281136512756348  \n",
            "\n",
            "train_loss =  188.04388\n",
            "Validation  :: loss =  1003.0804  : micro (f1) =  0.6771293615724516  : macro (f2) =  0.6660931568530131  : Epoch_runtime =  1.7281136512756348  \n",
            "\n",
            "val_loss =  1003.0804\n",
            "Weight Support =  [0.64132506 0.09537365 0.18938082 0.07392047]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 422 start !\n",
            "Training  :: loss =  187.16617  : micro (f1) =  0.7386539255588982  : macro (f2) =  0.7308647466150751  : Epoch_runtime =  1.7346844673156738  \n",
            "\n",
            "train_loss =  187.16617\n",
            "Validation  :: loss =  997.0614  : micro (f1) =  0.6802084119912406  : macro (f2) =  0.6696863959756674  : Epoch_runtime =  1.7346844673156738  \n",
            "\n",
            "val_loss =  997.0614\n",
            "Weight Support =  [0.64129996 0.0952739  0.18953918 0.07388695]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 423 start !\n",
            "Training  :: loss =  186.91388  : micro (f1) =  0.7376950709616631  : macro (f2) =  0.7299306544706398  : Epoch_runtime =  1.7292215824127197  \n",
            "\n",
            "train_loss =  186.91388\n",
            "Validation  :: loss =  976.53503  : micro (f1) =  0.6817223727792833  : macro (f2) =  0.6711155490774953  : Epoch_runtime =  1.7292215824127197  \n",
            "\n",
            "val_loss =  976.53503\n",
            "Weight Support =  [0.6413907  0.09490098 0.18980722 0.07390108]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 424 start !\n",
            "Training  :: loss =  187.11043  : micro (f1) =  0.7368879267747476  : macro (f2) =  0.7291316065719178  : Epoch_runtime =  1.6926441192626953  \n",
            "\n",
            "train_loss =  187.11043\n",
            "Validation  :: loss =  993.8136  : micro (f1) =  0.6764839836040686  : macro (f2) =  0.6658036179662117  : Epoch_runtime =  1.6926441192626953  \n",
            "\n",
            "val_loss =  993.8136\n",
            "Weight Support =  [0.6410451  0.09500021 0.1900807  0.07387396]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 425 start !\n",
            "Training  :: loss =  186.85416  : micro (f1) =  0.734987727107238  : macro (f2) =  0.7272753863828363  : Epoch_runtime =  1.7237353324890137  \n",
            "\n",
            "train_loss =  186.85416\n",
            "Validation  :: loss =  991.9716  : micro (f1) =  0.6739719751059524  : macro (f2) =  0.663074707898121  : Epoch_runtime =  1.7237353324890137  \n",
            "\n",
            "val_loss =  991.9716\n",
            "Weight Support =  [0.64095974 0.09525515 0.1900087  0.07377637]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 426 start !\n",
            "Training  :: loss =  186.84084  : micro (f1) =  0.7359231058469636  : macro (f2) =  0.7281574864877024  : Epoch_runtime =  1.6793625354766846  \n",
            "\n",
            "train_loss =  186.84084\n",
            "Validation  :: loss =  998.9314  : micro (f1) =  0.6732956743790142  : macro (f2) =  0.6612106743086105  : Epoch_runtime =  1.6793625354766846  \n",
            "\n",
            "val_loss =  998.9314\n",
            "Weight Support =  [0.64102703 0.09537411 0.19002657 0.07357226]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 427 start !\n",
            "Training  :: loss =  186.94203  : micro (f1) =  0.7360588677825841  : macro (f2) =  0.7283266986858408  : Epoch_runtime =  1.6845991611480713  \n",
            "\n",
            "train_loss =  186.94203\n",
            "Validation  :: loss =  979.365  : micro (f1) =  0.6787047619047619  : macro (f2) =  0.6676210876377733  : Epoch_runtime =  1.6845991611480713  \n",
            "\n",
            "val_loss =  979.365\n",
            "Weight Support =  [0.641357   0.09552379 0.18962784 0.07349135]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 428 start !\n",
            "Training  :: loss =  186.66766  : micro (f1) =  0.7372772562921784  : macro (f2) =  0.7295080105339169  : Epoch_runtime =  1.710688829421997  \n",
            "\n",
            "train_loss =  186.66766\n",
            "Validation  :: loss =  970.1068  : micro (f1) =  0.6813701423769779  : macro (f2) =  0.6703867793598339  : Epoch_runtime =  1.710688829421997  \n",
            "\n",
            "val_loss =  970.1068\n",
            "Weight Support =  [0.6419395  0.0950971  0.18938799 0.07357539]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 429 start !\n",
            "Training  :: loss =  186.11208  : micro (f1) =  0.7377993474138033  : macro (f2) =  0.7300297972975469  : Epoch_runtime =  1.714592456817627  \n",
            "\n",
            "train_loss =  186.11208\n",
            "Validation  :: loss =  966.95984  : micro (f1) =  0.6821914726065924  : macro (f2) =  0.6711747522304434  : Epoch_runtime =  1.714592456817627  \n",
            "\n",
            "val_loss =  966.95984\n",
            "Weight Support =  [0.6416582  0.0950954  0.18963295 0.07361348]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 430 start !\n",
            "Training  :: loss =  186.45096  : micro (f1) =  0.7374222724703222  : macro (f2) =  0.7296384269066996  : Epoch_runtime =  1.7273972034454346  \n",
            "\n",
            "train_loss =  186.45096\n",
            "Validation  :: loss =  977.06  : micro (f1) =  0.6793264874660224  : macro (f2) =  0.6681334047608194  : Epoch_runtime =  1.7273972034454346  \n",
            "\n",
            "val_loss =  977.06\n",
            "Weight Support =  [0.6417787  0.09489093 0.18982796 0.07350241]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 431 start !\n",
            "Training  :: loss =  186.29039  : micro (f1) =  0.7357488603787772  : macro (f2) =  0.7280160093718752  : Epoch_runtime =  1.732421875  \n",
            "\n",
            "train_loss =  186.29039\n",
            "Validation  :: loss =  982.8953  : micro (f1) =  0.6767403798576486  : macro (f2) =  0.6656325947963841  : Epoch_runtime =  1.732421875  \n",
            "\n",
            "val_loss =  982.8953\n",
            "Weight Support =  [0.64136004 0.09545536 0.18984698 0.07333767]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 432 start !\n",
            "Training  :: loss =  185.92845  : micro (f1) =  0.7362084371913654  : macro (f2) =  0.7284469607538318  : Epoch_runtime =  1.7143614292144775  \n",
            "\n",
            "train_loss =  185.92845\n",
            "Validation  :: loss =  976.8811  : micro (f1) =  0.6767661271764976  : macro (f2) =  0.6659955991504194  : Epoch_runtime =  1.7143614292144775  \n",
            "\n",
            "val_loss =  976.8811\n",
            "Weight Support =  [0.64158773 0.0953488  0.18989974 0.07316373]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 433 start !\n",
            "Training  :: loss =  186.21925  : micro (f1) =  0.7357575045526641  : macro (f2) =  0.7280243403580262  : Epoch_runtime =  1.6777372360229492  \n",
            "\n",
            "train_loss =  186.21925\n",
            "Validation  :: loss =  969.2688  : micro (f1) =  0.6758647047582649  : macro (f2) =  0.6647133296999399  : Epoch_runtime =  1.6777372360229492  \n",
            "\n",
            "val_loss =  969.2688\n",
            "Weight Support =  [0.64144427 0.0957085  0.18975592 0.07309131]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 434 start !\n",
            "Training  :: loss =  186.32443  : micro (f1) =  0.7372212935276529  : macro (f2) =  0.7294614364439916  : Epoch_runtime =  1.7214219570159912  \n",
            "\n",
            "train_loss =  186.32443\n",
            "Validation  :: loss =  968.76056  : micro (f1) =  0.6796973038749667  : macro (f2) =  0.668206903973538  : Epoch_runtime =  1.7214219570159912  \n",
            "\n",
            "val_loss =  968.76056\n",
            "Weight Support =  [0.6419888  0.09518278 0.18952085 0.07330755]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 435 start !\n",
            "Training  :: loss =  185.82082  : micro (f1) =  0.7377748450498431  : macro (f2) =  0.729999872912588  : Epoch_runtime =  1.7597055435180664  \n",
            "\n",
            "train_loss =  185.82082\n",
            "Validation  :: loss =  973.43945  : micro (f1) =  0.6834235046588518  : macro (f2) =  0.6721317544526907  : Epoch_runtime =  1.7597055435180664  \n",
            "\n",
            "val_loss =  973.43945\n",
            "Weight Support =  [0.64213824 0.09509816 0.18945946 0.07330416]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 436 start !\n",
            "Training  :: loss =  186.12675  : micro (f1) =  0.7374745249567072  : macro (f2) =  0.7297016320394009  : Epoch_runtime =  1.725370168685913  \n",
            "\n",
            "train_loss =  186.12675\n",
            "Validation  :: loss =  980.83264  : micro (f1) =  0.6812151364390171  : macro (f2) =  0.6699343617034664  : Epoch_runtime =  1.725370168685913  \n",
            "\n",
            "val_loss =  980.83264\n",
            "Weight Support =  [0.64209425 0.09549319 0.18935886 0.07305375]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 437 start !\n",
            "Training  :: loss =  186.42719  : micro (f1) =  0.7364794148429508  : macro (f2) =  0.7287234413771959  : Epoch_runtime =  1.6838550567626953  \n",
            "\n",
            "train_loss =  186.42719\n",
            "Validation  :: loss =  989.281  : micro (f1) =  0.6779609511509535  : macro (f2) =  0.6672701459142834  : Epoch_runtime =  1.6838550567626953  \n",
            "\n",
            "val_loss =  989.281\n",
            "Weight Support =  [0.6419195  0.0955168  0.18958342 0.07298028]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 438 start !\n",
            "Training  :: loss =  185.98466  : micro (f1) =  0.7368062008209737  : macro (f2) =  0.729060266666647  : Epoch_runtime =  1.7086970806121826  \n",
            "\n",
            "train_loss =  185.98466\n",
            "Validation  :: loss =  994.63275  : micro (f1) =  0.6749173649012222  : macro (f2) =  0.6644500503047236  : Epoch_runtime =  1.7086970806121826  \n",
            "\n",
            "val_loss =  994.63275\n",
            "Weight Support =  [0.6416156  0.09562968 0.18986994 0.07288472]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 439 start !\n",
            "Training  :: loss =  186.19875  : micro (f1) =  0.736038163723313  : macro (f2) =  0.728313739796489  : Epoch_runtime =  1.7037429809570312  \n",
            "\n",
            "train_loss =  186.19875\n",
            "Validation  :: loss =  984.0003  : micro (f1) =  0.675072596668195  : macro (f2) =  0.6641117709040508  : Epoch_runtime =  1.7037429809570312  \n",
            "\n",
            "val_loss =  984.0003\n",
            "Weight Support =  [0.6417549  0.09549478 0.18976775 0.07298266]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 440 start !\n",
            "Training  :: loss =  186.52177  : micro (f1) =  0.7365102518556859  : macro (f2) =  0.7287632685269473  : Epoch_runtime =  1.6941215991973877  \n",
            "\n",
            "train_loss =  186.52177\n",
            "Validation  :: loss =  961.17365  : micro (f1) =  0.6787137921870847  : macro (f2) =  0.6668558947535065  : Epoch_runtime =  1.6941215991973877  \n",
            "\n",
            "val_loss =  961.17365\n",
            "Weight Support =  [0.641572   0.09582739 0.18950012 0.07310046]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 441 start !\n",
            "Training  :: loss =  186.31122  : micro (f1) =  0.7371135234417312  : macro (f2) =  0.7293510520887847  : Epoch_runtime =  1.7485957145690918  \n",
            "\n",
            "train_loss =  186.31122\n",
            "Validation  :: loss =  983.223  : micro (f1) =  0.6810761789600968  : macro (f2) =  0.6692757816470472  : Epoch_runtime =  1.7485957145690918  \n",
            "\n",
            "val_loss =  983.223\n",
            "Weight Support =  [0.64223397 0.09544617 0.1892514  0.07306849]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 442 start !\n",
            "Training  :: loss =  186.11115  : micro (f1) =  0.7379613584656552  : macro (f2) =  0.7301892125071575  : Epoch_runtime =  1.7173302173614502  \n",
            "\n",
            "train_loss =  186.11115\n",
            "Validation  :: loss =  984.46655  : micro (f1) =  0.6828589690566605  : macro (f2) =  0.6717623285118083  : Epoch_runtime =  1.7173302173614502  \n",
            "\n",
            "val_loss =  984.46655\n",
            "Weight Support =  [0.6420822  0.09615865 0.18906398 0.07269513]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 443 start !\n",
            "Training  :: loss =  186.6222  : micro (f1) =  0.7365561044460128  : macro (f2) =  0.7287824163239764  : Epoch_runtime =  1.7099354267120361  \n",
            "\n",
            "train_loss =  186.6222\n",
            "Validation  :: loss =  988.7084  : micro (f1) =  0.6789517408050751  : macro (f2) =  0.6686573891218948  : Epoch_runtime =  1.7099354267120361  \n",
            "\n",
            "val_loss =  988.7084\n",
            "Weight Support =  [0.6421049  0.09580702 0.18939878 0.07268931]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 444 start !\n",
            "Training  :: loss =  186.65146  : micro (f1) =  0.7369164156626505  : macro (f2) =  0.7291540606410364  : Epoch_runtime =  1.7028157711029053  \n",
            "\n",
            "train_loss =  186.65146\n",
            "Validation  :: loss =  982.776  : micro (f1) =  0.6765759585334248  : macro (f2) =  0.6661720801096834  : Epoch_runtime =  1.7028157711029053  \n",
            "\n",
            "val_loss =  982.776\n",
            "Weight Support =  [0.6411948  0.09580866 0.19002269 0.07297389]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 445 start !\n",
            "Training  :: loss =  186.81705  : micro (f1) =  0.735439173320808  : macro (f2) =  0.7276969575425118  : Epoch_runtime =  1.7217590808868408  \n",
            "\n",
            "train_loss =  186.81705\n",
            "Validation  :: loss =  980.28577  : micro (f1) =  0.6760423812790609  : macro (f2) =  0.6651599747824825  : Epoch_runtime =  1.7217590808868408  \n",
            "\n",
            "val_loss =  980.28577\n",
            "Weight Support =  [0.6410552  0.09571977 0.19024795 0.0729771 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 446 start !\n",
            "Training  :: loss =  187.24115  : micro (f1) =  0.7365263826483803  : macro (f2) =  0.7287490763145033  : Epoch_runtime =  1.6922736167907715  \n",
            "\n",
            "train_loss =  187.24115\n",
            "Validation  :: loss =  970.50055  : micro (f1) =  0.6782668553290756  : macro (f2) =  0.6660681026085383  : Epoch_runtime =  1.6922736167907715  \n",
            "\n",
            "val_loss =  970.50055\n",
            "Weight Support =  [0.6414388  0.09599349 0.18959267 0.07297503]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 447 start !\n",
            "Training  :: loss =  187.49516  : micro (f1) =  0.7362205650568115  : macro (f2) =  0.7284695819149662  : Epoch_runtime =  1.7217624187469482  \n",
            "\n",
            "train_loss =  187.49516\n",
            "Validation  :: loss =  977.7591  : micro (f1) =  0.6770689918923053  : macro (f2) =  0.6653454850941974  : Epoch_runtime =  1.7217624187469482  \n",
            "\n",
            "val_loss =  977.7591\n",
            "Weight Support =  [0.6418976  0.0965176  0.18899599 0.07258882]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 448 start !\n",
            "Training  :: loss =  187.03381  : micro (f1) =  0.7372675638094789  : macro (f2) =  0.7294863083951454  : Epoch_runtime =  1.6918652057647705  \n",
            "\n",
            "train_loss =  187.03381\n",
            "Validation  :: loss =  997.6158  : micro (f1) =  0.6798263823362899  : macro (f2) =  0.6687562098933977  : Epoch_runtime =  1.6918652057647705  \n",
            "\n",
            "val_loss =  997.6158\n",
            "Weight Support =  [0.6420955  0.09646165 0.1889545  0.07248827]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 449 start !\n",
            "Training  :: loss =  186.80356  : micro (f1) =  0.7376855998114541  : macro (f2) =  0.7299087413771679  : Epoch_runtime =  1.7061035633087158  \n",
            "\n",
            "train_loss =  186.80356\n",
            "Validation  :: loss =  983.7418  : micro (f1) =  0.6821095243449904  : macro (f2) =  0.6717615822088076  : Epoch_runtime =  1.7061035633087158  \n",
            "\n",
            "val_loss =  983.7418\n",
            "Weight Support =  [0.641612   0.0966176  0.18941404 0.07235641]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 450 start !\n",
            "Training  :: loss =  187.21054  : micro (f1) =  0.7373526883999105  : macro (f2) =  0.7295887539251746  : Epoch_runtime =  1.6808197498321533  \n",
            "\n",
            "train_loss =  187.21054\n",
            "Validation  :: loss =  994.50824  : micro (f1) =  0.6788089480048368  : macro (f2) =  0.6684487670464265  : Epoch_runtime =  1.6808197498321533  \n",
            "\n",
            "val_loss =  994.50824\n",
            "Weight Support =  [0.6413217  0.09553194 0.19028996 0.0728564 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 451 start !\n",
            "Training  :: loss =  187.3771  : micro (f1) =  0.7356308342629015  : macro (f2) =  0.7279041199992319  : Epoch_runtime =  1.7293624877929688  \n",
            "\n",
            "train_loss =  187.3771\n",
            "Validation  :: loss =  970.0868  : micro (f1) =  0.6770821454365759  : macro (f2) =  0.6662099648956382  : Epoch_runtime =  1.7293624877929688  \n",
            "\n",
            "val_loss =  970.0868\n",
            "Weight Support =  [0.6411524  0.095976   0.19020174 0.07266986]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 452 start !\n",
            "Training  :: loss =  187.53723  : micro (f1) =  0.736370478385625  : macro (f2) =  0.728631997447612  : Epoch_runtime =  1.7614169120788574  \n",
            "\n",
            "train_loss =  187.53723\n",
            "Validation  :: loss =  982.1209  : micro (f1) =  0.6747236776685662  : macro (f2) =  0.6634405014972471  : Epoch_runtime =  1.7614169120788574  \n",
            "\n",
            "val_loss =  982.1209\n",
            "Weight Support =  [0.641608   0.09645902 0.18966292 0.07227013]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 453 start !\n",
            "Training  :: loss =  187.8895  : micro (f1) =  0.7356216521003666  : macro (f2) =  0.7278816101684461  : Epoch_runtime =  1.7112390995025635  \n",
            "\n",
            "train_loss =  187.8895\n",
            "Validation  :: loss =  979.9693  : micro (f1) =  0.6763207294180745  : macro (f2) =  0.6645357368340878  : Epoch_runtime =  1.7112390995025635  \n",
            "\n",
            "val_loss =  979.9693\n",
            "Weight Support =  [0.64116424 0.09699739 0.18959945 0.0722389 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 454 start !\n",
            "Training  :: loss =  187.9189  : micro (f1) =  0.7366439162155802  : macro (f2) =  0.7288478236562059  : Epoch_runtime =  1.6817326545715332  \n",
            "\n",
            "train_loss =  187.9189\n",
            "Validation  :: loss =  977.9645  : micro (f1) =  0.6792008508052264  : macro (f2) =  0.6678610647088418  : Epoch_runtime =  1.6817326545715332  \n",
            "\n",
            "val_loss =  977.9645\n",
            "Weight Support =  [0.6414002  0.0970392  0.18928008 0.07228041]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 455 start !\n",
            "Training  :: loss =  187.0245  : micro (f1) =  0.7378123526638378  : macro (f2) =  0.7300318960949819  : Epoch_runtime =  1.67952299118042  \n",
            "\n",
            "train_loss =  187.0245\n",
            "Validation  :: loss =  978.32404  : micro (f1) =  0.6823979591836735  : macro (f2) =  0.671665702429463  : Epoch_runtime =  1.67952299118042  \n",
            "\n",
            "val_loss =  978.32404\n",
            "Weight Support =  [0.64204824 0.09640168 0.18925996 0.07229011]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 456 start !\n",
            "Training  :: loss =  186.80498  : micro (f1) =  0.7383341784690857  : macro (f2) =  0.7305873596067767  : Epoch_runtime =  1.7033195495605469  \n",
            "\n",
            "train_loss =  186.80498\n",
            "Validation  :: loss =  1002.9665  : micro (f1) =  0.6801037867107885  : macro (f2) =  0.6694784410579979  : Epoch_runtime =  1.7033195495605469  \n",
            "\n",
            "val_loss =  1002.9665\n",
            "Weight Support =  [0.64156085 0.09655933 0.18954743 0.07233243]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 457 start !\n",
            "Training  :: loss =  187.29204  : micro (f1) =  0.7359909740503949  : macro (f2) =  0.7282815672462782  : Epoch_runtime =  1.7176525592803955  \n",
            "\n",
            "train_loss =  187.29204\n",
            "Validation  :: loss =  989.89514  : micro (f1) =  0.6777975942017986  : macro (f2) =  0.6670682181689409  : Epoch_runtime =  1.7176525592803955  \n",
            "\n",
            "val_loss =  989.89514\n",
            "Weight Support =  [0.64132464 0.09629679 0.19021913 0.07215957]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 458 start !\n",
            "Training  :: loss =  186.77625  : micro (f1) =  0.7361267870579382  : macro (f2) =  0.7284008637592561  : Epoch_runtime =  1.7227697372436523  \n",
            "\n",
            "train_loss =  186.77625\n",
            "Validation  :: loss =  995.21136  : micro (f1) =  0.6760122230710466  : macro (f2) =  0.665062582911701  : Epoch_runtime =  1.7227697372436523  \n",
            "\n",
            "val_loss =  995.21136\n",
            "Weight Support =  [0.64156944 0.09669362 0.18971749 0.07201947]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 459 start !\n",
            "Training  :: loss =  187.0403  : micro (f1) =  0.7357445958177095  : macro (f2) =  0.7279859516474418  : Epoch_runtime =  1.7346980571746826  \n",
            "\n",
            "train_loss =  187.0403\n",
            "Validation  :: loss =  979.6158  : micro (f1) =  0.6767452541334967  : macro (f2) =  0.6653608627593522  : Epoch_runtime =  1.7346980571746826  \n",
            "\n",
            "val_loss =  979.6158\n",
            "Weight Support =  [0.6411705  0.09691066 0.18983532 0.07208358]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 460 start !\n",
            "Training  :: loss =  187.0888  : micro (f1) =  0.7364508844561536  : macro (f2) =  0.7287077583207522  : Epoch_runtime =  1.7155735492706299  \n",
            "\n",
            "train_loss =  187.0888\n",
            "Validation  :: loss =  966.32086  : micro (f1) =  0.675723251186285  : macro (f2) =  0.663969222309382  : Epoch_runtime =  1.7155735492706299  \n",
            "\n",
            "val_loss =  966.32086\n",
            "Weight Support =  [0.6410753  0.09706141 0.18963447 0.07222881]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 461 start !\n",
            "Training  :: loss =  187.15997  : micro (f1) =  0.7370329812662639  : macro (f2) =  0.7292656820532593  : Epoch_runtime =  1.7035093307495117  \n",
            "\n",
            "train_loss =  187.15997\n",
            "Validation  :: loss =  984.3684  : micro (f1) =  0.6796167973039494  : macro (f2) =  0.6684828030694779  : Epoch_runtime =  1.7035093307495117  \n",
            "\n",
            "val_loss =  984.3684\n",
            "Weight Support =  [0.64222705 0.09662995 0.189103   0.07203995]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 462 start !\n",
            "Training  :: loss =  186.37209  : micro (f1) =  0.7386466875110613  : macro (f2) =  0.730880172258682  : Epoch_runtime =  1.7816426753997803  \n",
            "\n",
            "train_loss =  186.37209\n",
            "Validation  :: loss =  1006.568  : micro (f1) =  0.681770205066345  : macro (f2) =  0.670671495835861  : Epoch_runtime =  1.7816426753997803  \n",
            "\n",
            "val_loss =  1006.568\n",
            "Weight Support =  [0.64265996 0.09632609 0.18922107 0.0717928 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 463 start !\n",
            "Training  :: loss =  186.87317  : micro (f1) =  0.7370688640376691  : macro (f2) =  0.7293200807779546  : Epoch_runtime =  1.8082563877105713  \n",
            "\n",
            "train_loss =  186.87317\n",
            "Validation  :: loss =  991.0448  : micro (f1) =  0.6818696669938835  : macro (f2) =  0.6711843432607184  : Epoch_runtime =  1.8082563877105713  \n",
            "\n",
            "val_loss =  991.0448\n",
            "Weight Support =  [0.64156234 0.09692854 0.18971166 0.07179746]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 464 start !\n",
            "Training  :: loss =  186.97684  : micro (f1) =  0.7363814945949444  : macro (f2) =  0.7285912535056038  : Epoch_runtime =  1.7548885345458984  \n",
            "\n",
            "train_loss =  186.97684\n",
            "Validation  :: loss =  980.3626  : micro (f1) =  0.6785659917795708  : macro (f2) =  0.6671568450631334  : Epoch_runtime =  1.7548885345458984  \n",
            "\n",
            "val_loss =  980.3626\n",
            "Weight Support =  [0.6410879  0.09710789 0.18995291 0.07185128]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 465 start !\n",
            "Training  :: loss =  186.78516  : micro (f1) =  0.7353176094719037  : macro (f2) =  0.727573108449429  : Epoch_runtime =  1.7109754085540771  \n",
            "\n",
            "train_loss =  186.78516\n",
            "Validation  :: loss =  984.08966  : micro (f1) =  0.676239211792561  : macro (f2) =  0.6653549879230847  : Epoch_runtime =  1.7109754085540771  \n",
            "\n",
            "val_loss =  984.08966\n",
            "Weight Support =  [0.6408574  0.09728695 0.18993646 0.07191918]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 466 start !\n",
            "Training  :: loss =  187.0316  : micro (f1) =  0.7357929502474113  : macro (f2) =  0.7280176042046891  : Epoch_runtime =  1.702526569366455  \n",
            "\n",
            "train_loss =  187.0316\n",
            "Validation  :: loss =  980.07684  : micro (f1) =  0.6746526847564008  : macro (f2) =  0.6631876609731383  : Epoch_runtime =  1.702526569366455  \n",
            "\n",
            "val_loss =  980.07684\n",
            "Weight Support =  [0.64156854 0.09670281 0.18982588 0.07190275]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 467 start !\n",
            "Training  :: loss =  187.18022  : micro (f1) =  0.736249265138154  : macro (f2) =  0.7285031070474309  : Epoch_runtime =  1.6879360675811768  \n",
            "\n",
            "train_loss =  187.18022\n",
            "Validation  :: loss =  974.9808  : micro (f1) =  0.6798100664767333  : macro (f2) =  0.6686624855931518  : Epoch_runtime =  1.6879360675811768  \n",
            "\n",
            "val_loss =  974.9808\n",
            "Weight Support =  [0.6420707  0.09677706 0.1893687  0.07178355]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 468 start !\n",
            "Training  :: loss =  186.8825  : micro (f1) =  0.737755559223713  : macro (f2) =  0.7299767673182403  : Epoch_runtime =  1.7332496643066406  \n",
            "\n",
            "train_loss =  186.8825\n",
            "Validation  :: loss =  1006.2098  : micro (f1) =  0.6799365463060886  : macro (f2) =  0.6689239343373657  : Epoch_runtime =  1.7332496643066406  \n",
            "\n",
            "val_loss =  1006.2098\n",
            "Weight Support =  [0.6427788  0.09666637 0.1890442  0.0715106 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 469 start !\n",
            "Training  :: loss =  186.71017  : micro (f1) =  0.7378782698904831  : macro (f2) =  0.7301109664007802  : Epoch_runtime =  1.7366530895233154  \n",
            "\n",
            "train_loss =  186.71017\n",
            "Validation  :: loss =  980.246  : micro (f1) =  0.6843315366929252  : macro (f2) =  0.6734402060299909  : Epoch_runtime =  1.7366530895233154  \n",
            "\n",
            "val_loss =  980.246\n",
            "Weight Support =  [0.64142334 0.09748362 0.18959326 0.07149975]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 470 start !\n",
            "Training  :: loss =  186.93843  : micro (f1) =  0.7371852200517769  : macro (f2) =  0.7294323670856123  : Epoch_runtime =  1.695864200592041  \n",
            "\n",
            "train_loss =  186.93843\n",
            "Validation  :: loss =  994.2799  : micro (f1) =  0.6782773667235819  : macro (f2) =  0.6677148460541087  : Epoch_runtime =  1.695864200592041  \n",
            "\n",
            "val_loss =  994.2799\n",
            "Weight Support =  [0.64110464 0.09719632 0.18999878 0.0717003 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 471 start !\n",
            "Training  :: loss =  186.62189  : micro (f1) =  0.7358215620372763  : macro (f2) =  0.7281014180415248  : Epoch_runtime =  1.6961863040924072  \n",
            "\n",
            "train_loss =  186.62189\n",
            "Validation  :: loss =  995.9219  : micro (f1) =  0.6753880773484876  : macro (f2) =  0.6644237354781235  : Epoch_runtime =  1.6961863040924072  \n",
            "\n",
            "val_loss =  995.9219\n",
            "Weight Support =  [0.6410434  0.09717049 0.18997891 0.07180724]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 472 start !\n",
            "Training  :: loss =  186.6546  : micro (f1) =  0.7355055440706635  : macro (f2) =  0.7277647126869969  : Epoch_runtime =  1.7378110885620117  \n",
            "\n",
            "train_loss =  186.6546\n",
            "Validation  :: loss =  1005.79706  : micro (f1) =  0.6752300613496933  : macro (f2) =  0.663584877852114  : Epoch_runtime =  1.7378110885620117  \n",
            "\n",
            "val_loss =  1005.79706\n",
            "Weight Support =  [0.6410847  0.09726464 0.18993388 0.07171685]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 473 start !\n",
            "Training  :: loss =  186.90227  : micro (f1) =  0.736241768579492  : macro (f2) =  0.72849168886589  : Epoch_runtime =  1.692857027053833  \n",
            "\n",
            "train_loss =  186.90227\n",
            "Validation  :: loss =  974.55524  : micro (f1) =  0.6787444219840574  : macro (f2) =  0.6669691856193833  : Epoch_runtime =  1.692857027053833  \n",
            "\n",
            "val_loss =  974.55524\n",
            "Weight Support =  [0.64159036 0.09723452 0.18951288 0.07166225]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 474 start !\n",
            "Training  :: loss =  186.73619  : micro (f1) =  0.7372771589383699  : macro (f2) =  0.7294978220544381  : Epoch_runtime =  1.7384350299835205  \n",
            "\n",
            "train_loss =  186.73619\n",
            "Validation  :: loss =  980.75696  : micro (f1) =  0.6771450265755505  : macro (f2) =  0.6658600709907212  : Epoch_runtime =  1.7384350299835205  \n",
            "\n",
            "val_loss =  980.75696\n",
            "Weight Support =  [0.6425408  0.09711381 0.18905602 0.07128932]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 475 start !\n",
            "Training  :: loss =  186.1586  : micro (f1) =  0.7381457390648568  : macro (f2) =  0.7304070936013193  : Epoch_runtime =  1.673114538192749  \n",
            "\n",
            "train_loss =  186.1586\n",
            "Validation  :: loss =  997.0289  : micro (f1) =  0.6799033670542051  : macro (f2) =  0.6688392652604682  : Epoch_runtime =  1.673114538192749  \n",
            "\n",
            "val_loss =  997.0289\n",
            "Weight Support =  [0.64238614 0.09713629 0.18925045 0.07122704]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 476 start !\n",
            "Training  :: loss =  186.41078  : micro (f1) =  0.7375931616686093  : macro (f2) =  0.729866803856074  : Epoch_runtime =  1.7196149826049805  \n",
            "\n",
            "train_loss =  186.41078\n",
            "Validation  :: loss =  995.0658  : micro (f1) =  0.6783078444799513  : macro (f2) =  0.6674636242729206  : Epoch_runtime =  1.7196149826049805  \n",
            "\n",
            "val_loss =  995.0658\n",
            "Weight Support =  [0.64164996 0.09715631 0.18978098 0.07141273]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 477 start !\n",
            "Training  :: loss =  186.32207  : micro (f1) =  0.7360835165351923  : macro (f2) =  0.7283417490705442  : Epoch_runtime =  1.7268729209899902  \n",
            "\n",
            "train_loss =  186.32207\n",
            "Validation  :: loss =  988.43774  : micro (f1) =  0.6781911848883799  : macro (f2) =  0.6667233024339263  : Epoch_runtime =  1.7268729209899902  \n",
            "\n",
            "val_loss =  988.43774\n",
            "Weight Support =  [0.6415484  0.0972252  0.18972327 0.07150315]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 478 start !\n",
            "Training  :: loss =  186.37701  : micro (f1) =  0.735742896409148  : macro (f2) =  0.7279900246579134  : Epoch_runtime =  1.716881275177002  \n",
            "\n",
            "train_loss =  186.37701\n",
            "Validation  :: loss =  1001.05994  : micro (f1) =  0.675422138836773  : macro (f2) =  0.6641420236455015  : Epoch_runtime =  1.716881275177002  \n",
            "\n",
            "val_loss =  1001.05994\n",
            "Weight Support =  [0.6418707  0.09745459 0.18943039 0.07124428]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 479 start !\n",
            "Training  :: loss =  186.7941  : micro (f1) =  0.7360502156971072  : macro (f2) =  0.7282656568792591  : Epoch_runtime =  1.7260336875915527  \n",
            "\n",
            "train_loss =  186.7941\n",
            "Validation  :: loss =  967.8083  : micro (f1) =  0.679148676015437  : macro (f2) =  0.6675680504918449  : Epoch_runtime =  1.7260336875915527  \n",
            "\n",
            "val_loss =  967.8083\n",
            "Weight Support =  [0.64248896 0.09714135 0.18916157 0.07120819]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 480 start !\n",
            "Training  :: loss =  186.54971  : micro (f1) =  0.7364904603947585  : macro (f2) =  0.7287840818419451  : Epoch_runtime =  1.7274470329284668  \n",
            "\n",
            "train_loss =  186.54971\n",
            "Validation  :: loss =  975.8981  : micro (f1) =  0.6790493157954589  : macro (f2) =  0.6682174317470942  : Epoch_runtime =  1.7274470329284668  \n",
            "\n",
            "val_loss =  975.8981\n",
            "Weight Support =  [0.64255327 0.09721246 0.18922885 0.07100543]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 481 start !\n",
            "Training  :: loss =  186.4546  : micro (f1) =  0.738060713233127  : macro (f2) =  0.7303079455236352  : Epoch_runtime =  1.681342363357544  \n",
            "\n",
            "train_loss =  186.4546\n",
            "Validation  :: loss =  986.5612  : micro (f1) =  0.6790384760752892  : macro (f2) =  0.668401014060992  : Epoch_runtime =  1.681342363357544  \n",
            "\n",
            "val_loss =  986.5612\n",
            "Weight Support =  [0.6423863  0.09729929 0.18910931 0.07120506]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 482 start !\n",
            "Training  :: loss =  186.50208  : micro (f1) =  0.7378329661526136  : macro (f2) =  0.7301011898572478  : Epoch_runtime =  1.744335651397705  \n",
            "\n",
            "train_loss =  186.50208\n",
            "Validation  :: loss =  991.027  : micro (f1) =  0.6800955957664733  : macro (f2) =  0.6691721214772223  : Epoch_runtime =  1.744335651397705  \n",
            "\n",
            "val_loss =  991.027\n",
            "Weight Support =  [0.6422174  0.09712026 0.1894889  0.0711733 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 483 start !\n",
            "Training  :: loss =  186.4031  : micro (f1) =  0.7368284820104948  : macro (f2) =  0.7290697678846039  : Epoch_runtime =  1.703852891921997  \n",
            "\n",
            "train_loss =  186.4031\n",
            "Validation  :: loss =  986.99585  : micro (f1) =  0.6796109097844746  : macro (f2) =  0.6680115125575705  : Epoch_runtime =  1.703852891921997  \n",
            "\n",
            "val_loss =  986.99585\n",
            "Weight Support =  [0.6420268  0.09750973 0.18934298 0.07112043]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 484 start !\n",
            "Training  :: loss =  186.5902  : micro (f1) =  0.7363939978362105  : macro (f2) =  0.7286296998212213  : Epoch_runtime =  1.732990026473999  \n",
            "\n",
            "train_loss =  186.5902\n",
            "Validation  :: loss =  989.2182  : micro (f1) =  0.6760134105455653  : macro (f2) =  0.6649676496137351  : Epoch_runtime =  1.732990026473999  \n",
            "\n",
            "val_loss =  989.2182\n",
            "Weight Support =  [0.6424081  0.09736106 0.18938302 0.07084791]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 485 start !\n",
            "Training  :: loss =  186.93498  : micro (f1) =  0.7361947318908749  : macro (f2) =  0.7284431561941979  : Epoch_runtime =  1.7454299926757812  \n",
            "\n",
            "train_loss =  186.93498\n",
            "Validation  :: loss =  978.0342  : micro (f1) =  0.6773836343502997  : macro (f2) =  0.666641518600709  : Epoch_runtime =  1.7454299926757812  \n",
            "\n",
            "val_loss =  978.0342\n",
            "Weight Support =  [0.64271945 0.09756415 0.18902096 0.07069542]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 486 start !\n",
            "Training  :: loss =  186.62599  : micro (f1) =  0.7364595462404292  : macro (f2) =  0.7287062268600741  : Epoch_runtime =  1.7364482879638672  \n",
            "\n",
            "train_loss =  186.62599\n",
            "Validation  :: loss =  972.74536  : micro (f1) =  0.6775761257498671  : macro (f2) =  0.666942219257951  : Epoch_runtime =  1.7364482879638672  \n",
            "\n",
            "val_loss =  972.74536\n",
            "Weight Support =  [0.6425958  0.0972711  0.18939918 0.07073382]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 487 start !\n",
            "Training  :: loss =  186.76495  : micro (f1) =  0.737014363079821  : macro (f2) =  0.7292794232851915  : Epoch_runtime =  1.7673540115356445  \n",
            "\n",
            "train_loss =  186.76495\n",
            "Validation  :: loss =  974.06604  : micro (f1) =  0.6787189691112375  : macro (f2) =  0.6676082487868584  : Epoch_runtime =  1.7673540115356445  \n",
            "\n",
            "val_loss =  974.06604\n",
            "Weight Support =  [0.6418677  0.09796418 0.18925704 0.07091103]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 488 start !\n",
            "Training  :: loss =  186.3181  : micro (f1) =  0.7374283258568519  : macro (f2) =  0.7296761111167824  : Epoch_runtime =  1.7500720024108887  \n",
            "\n",
            "train_loss =  186.3181\n",
            "Validation  :: loss =  989.71716  : micro (f1) =  0.6796367367101113  : macro (f2) =  0.6679517622943203  : Epoch_runtime =  1.7500720024108887  \n",
            "\n",
            "val_loss =  989.71716\n",
            "Weight Support =  [0.6419391  0.0975282  0.18955524 0.07097745]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 489 start !\n",
            "Training  :: loss =  186.1656  : micro (f1) =  0.7373875359178483  : macro (f2) =  0.7295864207558563  : Epoch_runtime =  1.7424695491790771  \n",
            "\n",
            "train_loss =  186.1656\n",
            "Validation  :: loss =  971.75476  : micro (f1) =  0.6817597702018294  : macro (f2) =  0.6701866450814692  : Epoch_runtime =  1.7424695491790771  \n",
            "\n",
            "val_loss =  971.75476\n",
            "Weight Support =  [0.64226365 0.09737599 0.18952511 0.07083526]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 490 start !\n",
            "Training  :: loss =  186.14633  : micro (f1) =  0.7366698839426542  : macro (f2) =  0.7289009578554635  : Epoch_runtime =  1.7346391677856445  \n",
            "\n",
            "train_loss =  186.14633\n",
            "Validation  :: loss =  968.5896  : micro (f1) =  0.6805355927074666  : macro (f2) =  0.6699047122043493  : Epoch_runtime =  1.7346391677856445  \n",
            "\n",
            "val_loss =  968.5896\n",
            "Weight Support =  [0.6421139  0.0975485  0.18972749 0.07061012]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 491 start !\n",
            "Training  :: loss =  185.87206  : micro (f1) =  0.7360648991828818  : macro (f2) =  0.72834754654497  : Epoch_runtime =  1.720076560974121  \n",
            "\n",
            "train_loss =  185.87206\n",
            "Validation  :: loss =  977.8709  : micro (f1) =  0.6772978116079924  : macro (f2) =  0.6665584521187407  : Epoch_runtime =  1.720076560974121  \n",
            "\n",
            "val_loss =  977.8709\n",
            "Weight Support =  [0.6420734  0.09760918 0.18972908 0.07058838]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 492 start !\n",
            "Training  :: loss =  185.81848  : micro (f1) =  0.7365116689201106  : macro (f2) =  0.728808822827459  : Epoch_runtime =  1.7715013027191162  \n",
            "\n",
            "train_loss =  185.81848\n",
            "Validation  :: loss =  991.59863  : micro (f1) =  0.6770174429748897  : macro (f2) =  0.6659013080486768  : Epoch_runtime =  1.7715013027191162  \n",
            "\n",
            "val_loss =  991.59863\n",
            "Weight Support =  [0.641669   0.09798951 0.189775   0.07056646]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 493 start !\n",
            "Training  :: loss =  185.98857  : micro (f1) =  0.736443587452382  : macro (f2) =  0.7286957775865597  : Epoch_runtime =  1.7266826629638672  \n",
            "\n",
            "train_loss =  185.98857\n",
            "Validation  :: loss =  976.9639  : micro (f1) =  0.6775981347704775  : macro (f2) =  0.6658638188173218  : Epoch_runtime =  1.7266826629638672  \n",
            "\n",
            "val_loss =  976.9639\n",
            "Weight Support =  [0.64200455 0.09798085 0.18938416 0.07063045]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 494 start !\n",
            "Training  :: loss =  185.71902  : micro (f1) =  0.7370316324609302  : macro (f2) =  0.7292349845831471  : Epoch_runtime =  1.7374753952026367  \n",
            "\n",
            "train_loss =  185.71902\n",
            "Validation  :: loss =  981.5492  : micro (f1) =  0.6811517131353034  : macro (f2) =  0.6696398081589283  : Epoch_runtime =  1.7374753952026367  \n",
            "\n",
            "val_loss =  981.5492\n",
            "Weight Support =  [0.64223844 0.09776272 0.18939449 0.07060437]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 495 start !\n",
            "Training  :: loss =  185.58337  : micro (f1) =  0.7378595002357378  : macro (f2) =  0.7300500609730832  : Epoch_runtime =  1.7587602138519287  \n",
            "\n",
            "train_loss =  185.58337\n",
            "Validation  :: loss =  969.34827  : micro (f1) =  0.6839024390243903  : macro (f2) =  0.6726328188652785  : Epoch_runtime =  1.7587602138519287  \n",
            "\n",
            "val_loss =  969.34827\n",
            "Weight Support =  [0.6422913  0.09741535 0.18970129 0.070592  ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 496 start !\n",
            "Training  :: loss =  185.80952  : micro (f1) =  0.7368173258003767  : macro (f2) =  0.7290681718623544  : Epoch_runtime =  1.7970831394195557  \n",
            "\n",
            "train_loss =  185.80952\n",
            "Validation  :: loss =  969.2488  : micro (f1) =  0.6800482036604655  : macro (f2) =  0.6693040752355315  : Epoch_runtime =  1.7970831394195557  \n",
            "\n",
            "val_loss =  969.2488\n",
            "Weight Support =  [0.64230984 0.09760959 0.18957753 0.07050299]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 497 start !\n",
            "Training  :: loss =  185.67992  : micro (f1) =  0.7370118785538539  : macro (f2) =  0.7293040021984964  : Epoch_runtime =  1.7472286224365234  \n",
            "\n",
            "train_loss =  185.67992\n",
            "Validation  :: loss =  982.1367  : micro (f1) =  0.6774890953916176  : macro (f2) =  0.6666436250755687  : Epoch_runtime =  1.7472286224365234  \n",
            "\n",
            "val_loss =  982.1367\n",
            "Weight Support =  [0.642334   0.09762747 0.18956226 0.07047638]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 498 start !\n",
            "Training  :: loss =  185.8624  : micro (f1) =  0.7362953736236618  : macro (f2) =  0.728541378897239  : Epoch_runtime =  1.7349128723144531  \n",
            "\n",
            "train_loss =  185.8624\n",
            "Validation  :: loss =  998.9879  : micro (f1) =  0.6757305350235533  : macro (f2) =  0.6650939087922296  : Epoch_runtime =  1.7349128723144531  \n",
            "\n",
            "val_loss =  998.9879\n",
            "Weight Support =  [0.6423014  0.09766509 0.18967527 0.07035824]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 499 start !\n",
            "Training  :: loss =  185.74959  : micro (f1) =  0.7360280203570716  : macro (f2) =  0.7282559736079022  : Epoch_runtime =  1.7501213550567627  \n",
            "\n",
            "train_loss =  185.74959\n",
            "Validation  :: loss =  999.68713  : micro (f1) =  0.6759967703487254  : macro (f2) =  0.6643283594220433  : Epoch_runtime =  1.7501213550567627  \n",
            "\n",
            "val_loss =  999.68713\n",
            "Weight Support =  [0.641476   0.09839993 0.189698   0.0704262 ]\n",
            "\n",
            "================================================================\n",
            "\n",
            "Epoch 500 start !\n",
            "Training  :: loss =  185.9816  : micro (f1) =  0.7358415190111389  : macro (f2) =  0.7280582559057505  : Epoch_runtime =  1.7620863914489746  \n",
            "\n",
            "train_loss =  185.9816\n",
            "Validation  :: loss =  976.9272  : micro (f1) =  0.6776194852941175  : macro (f2) =  0.6661004421950223  : Epoch_runtime =  1.7620863914489746  \n",
            "\n",
            "val_loss =  976.9272\n",
            "Weight Support =  [0.6421234  0.09812731 0.1893073  0.07044201]\n",
            "\n",
            "================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sUTSSzjc_sFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UM4gzEN_CJsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "trues = []\n",
        "x_batch1 =[]\n",
        "y_batch1 = []\n",
        "x_batch2 = []\n",
        "err = []\n",
        "pp = []\n",
        "x_test3 = np.array(x_test)\n",
        "sub=0\n",
        "for i in range(len(x_test)):\n",
        "    i-=sub\n",
        "    x_batch = x_test3[i:min(len(x_test)-1,timeSize+(i))]\n",
        "    x_anomaly = x_test2[i:min(len(x_test)-1,timeSize+(i))]\n",
        "    if len(x_batch) < timeSize:\n",
        "      continue\n",
        "    x_batch = x_batch\n",
        "    x_anomaly = x_anomaly\n",
        "    y_batch = x_test[min(len(x_test)-1,timeSize+(i))].T\n",
        "    x_batch1.append(x_batch)\n",
        "    x_batch2.append(x_anomaly)\n",
        "    y_batch1.append(y_batch)\n",
        "    if (i+1)% BATCH_SIZE >0:\n",
        "      continue\n",
        "      sub=3\n",
        "    fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "    l, acc,oht,weightSupport = sess.run([loss, accuracy,one_hot_prediction,weight_soft], feed_dict=fd)\n",
        "    err.append(l)\n",
        "   \n",
        "    for j in range(1):\n",
        "      preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "      trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "    f1 = f1_score(y_true=np.where(np.array(trues)>0,1,0), y_pred=np.where(np.array(preds)>0,1,0), average='micro')\n",
        "    f2 = f1_score(y_true=np.where(np.array(trues)>0,1,0), y_pred=np.where(np.array(preds)>0,1,0), average='macro')\n",
        "    print(f1,\" : \",f2)\n",
        "    pp.append([f1,f2])\n",
        "    x_batch1 =[]\n",
        "    y_batch1 = []\n",
        "    x_batch2 = []\n",
        "    x_test3[min(len(x_test)-1,timeSize+(i-3))]=np.array(oht[0]).T\n",
        "\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "ts.append([f1,f2])\n",
        "print(np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \")\n",
        "print(weightSupport)"
      ],
      "metadata": {
        "id": "GdM4BagHDCwa",
        "outputId": "513754e4-fc98-4921-dd01-f4c4a9fa9cd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6797385620915032  :  0.6699278406789808\n",
            "0.7008547008547008  :  0.6914772241829538\n",
            "0.6971428571428571  :  0.6874121689379842\n",
            "0.6987693953986088  :  0.6903043743033999\n",
            "0.7058322690506598  :  0.6961254394024108\n",
            "0.700355871886121  :  0.692818121691895\n",
            "0.6851680542707371  :  0.6780154783128162\n",
            "0.6871463217461601  :  0.6793540019610379\n",
            "0.6978622327790973  :  0.6892688097639234\n",
            "0.6967686710892361  :  0.6878541954061433\n",
            "0.6951100720826028  :  0.6865482920683915\n",
            "0.6936147379717403  :  0.6841766841749879\n",
            "0.6892898526733985  :  0.679916514155778\n",
            "0.6914338348173166  :  0.6822996762537867\n",
            "0.6828143021914649  :  0.6744990737376426\n",
            "772.25903  : micro  0.6828143021914649  : macro 0.6744990737376426  : \n",
            "[0.29811054 0.17845328 0.28983265 0.23360346]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.plot(np.array(pp).T[0].T,label=\"testProg_micro\")\n",
        "plt.plot(np.array(pp).T[1].T,label=\"testProg_macro\")\n",
        "plt.title('F1 fall progressive prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Tpel1CLDHMn",
        "outputId": "d47a6579-6b32-4ae2-e17a-f3004935e8a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}