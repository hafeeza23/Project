{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataPreprocessing__v1.ipynb","provenance":[{"file_id":"https://github.com/Prabhat1808/CrimePrediction/blob/master/Jupyter%20Notebooks/DataPreprocessing.ipynb","timestamp":1653133349996}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7EPj2yuFSCw-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653133717257,"user_tz":-330,"elapsed":27954,"user":{"displayName":"keerthana k","userId":"05767184302900016960"}},"outputId":"382c3873-9454-4330-adb4-2410047c1276"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"0JUv-o34UqVY","outputId":"924e282c-c880-47c0-dffd-9641b013fac0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653132303493,"user_tz":-330,"elapsed":104601,"user":{"displayName":"","userId":""}}},"source":["# LOADS 3 FILES\n","#     1. 311_06.csv -> contains 311 Public complaints data from New York City\n","#     2. CrimeData.csv -> Multi-year crime records\n","#     3. Regions.csv -> Geographical divisions of the city into precincts\n","\n","import pandas as pd\n","import numpy as np\n","import shapely.wkt\n","from shapely.geometry import Point, Polygon, MultiPolygon\n","\n","# Change the 'year' and file locations according to your dataset\n","year = '2006'\n","file311 = \"/content/gdrive/MyDrive/Dataset/311_06.csv\"\n","fileCrime = \"/content/gdrive/MyDrive/Dataset/CrimeData.csv\"\n","fileRegions = \"/content/gdrive/MyDrive/Dataset/Regions.csv\"\n","\n","# Extract:\n","#     1. Relevant columns\n","#     2. Relevant types of complaints, from the 'Complaint Type' column\n","\n","data311 = pd.read_csv(file311,low_memory=False)\n","relevantColumns311 = ['Created Date','Latitude','Longitude','Complaint Type']\n","relevantComplaints311 = ['Blocked Driveway','Building/Use','Noise','Safety']\n","finalColumns311 = ['Created Date','Complaint Type','Precincts']\n","data311 = data311[relevantColumns311]\n","data311 = data311.loc[(data311['Complaint Type'] == 'Blocked Driveway') | (data311['Complaint Type'] == 'Building/Use') | (data311['Complaint Type'] == 'Noise') | (data311['Complaint Type'] == 'Safety')]\n","print('Anomalies preprocessed!!!')\n","\n","# Extract:\n","#     1. Relevant columns\n","#     2. Relevant types of crimes, from the 'OFNS_DESC' column\n","\n","crimeData = pd.read_csv(fileCrime, low_memory=False)\n","relevantColumnsCR = ['CMPLNT_TO_DT','CMPLNT_TO_TM','CMPLNT_FR_DT','RPT_DT','Lat_Lon','Latitude','Longitude','OFNS_DESC']\n","finalColumnsCR = ['Date','OFNS_DESC','CMPLNT_TO_TM','Longitude','Latitude','Precincts']\n","relevantCrimesCR = ['ROBBERY', 'BURGLARY', 'FELONY ASSAULT','GRAND LARCENY']\n","locations = pd.read_csv(\"/content/gdrive/MyDrive/Dataset/Regions.csv\", low_memory=False)\n","crime2006 = crimeData.loc[crimeData['RPT_DT'].str.endswith(year)]\n","crime2006 = crime2006[relevantColumnsCR]\n","crime2006 = crime2006.loc[(crime2006['OFNS_DESC'] == 'ROBBERY') | (crime2006['OFNS_DESC'] == 'BURGLARY') | (crime2006['OFNS_DESC'] == 'FELONY ASSAULT') | (crime2006['OFNS_DESC'] == 'GRAND LARCENY')]\n","print('Crime Preprocessed!!!')\n","\n","# Load the regions and store a MultiPolygon object(corresponding to the geometry of the region) and the precinct number\n","\n","locations = pd.read_csv(fileRegions, low_memory=False)\n","precincts = {}\n","for index, row in locations.iterrows():\n","  precincts[row['Precinct']] = shapely.wkt.loads(row['the_geom'])\n","\n","print('Locations loaded!!!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Anomalies preprocessed!!!\n","Crime Preprocessed!!!\n","Locations loaded!!!\n"]}]},{"cell_type":"code","metadata":{"id":"YJ70JHNMJKpQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653132415469,"user_tz":-330,"elapsed":86582,"user":{"displayName":"","userId":""}},"outputId":"fcb37680-df24-47f1-9452-44b2d1b3593a"},"source":["# Extracts the latitude/longitude information of each 311 complaint reported\n","# And assigns it to one of the precincts defined earlier in form of a MultiPolygon\n","\n","pos = 0\n","prec = np.ndarray((data311.shape[0],))\n","\n","for index,row in data311.iterrows():\n","\n","  poo = Point(row['Longitude'],row['Latitude'])  \n","  for key,val in precincts.items():\n","    if poo.within(val):\n","      prec[pos] = key\n","      break\n","\n","  pos=pos+1\n","  if(pos%1000 == 0):\n","    print (\"processed \"+str(pos)+\" records!!\")\n","\n","# Adds the precincts information to the dataset\n","data311['Precincts'] = prec.astype(int)\n","print(\"Done!!\")\n","\n","# Extract the final columns needed for training the model\n","data311['Precincts'] = data311['Precincts'].astype(np.int64)\n","data311 = data311[data311['Precincts'] >= 0]\n","data311 = data311[finalColumns311]\n","\n","print('Anomaly Preprocessing complete!!!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processed 1000 records!!\n","processed 2000 records!!\n","processed 3000 records!!\n","processed 4000 records!!\n","processed 5000 records!!\n","processed 6000 records!!\n","processed 7000 records!!\n","processed 8000 records!!\n","processed 9000 records!!\n","processed 10000 records!!\n","processed 11000 records!!\n","processed 12000 records!!\n","processed 13000 records!!\n","processed 14000 records!!\n","processed 15000 records!!\n","processed 16000 records!!\n","processed 17000 records!!\n","processed 18000 records!!\n","processed 19000 records!!\n","processed 20000 records!!\n","processed 21000 records!!\n","processed 22000 records!!\n","processed 23000 records!!\n","processed 24000 records!!\n","processed 25000 records!!\n","processed 26000 records!!\n","processed 27000 records!!\n","processed 28000 records!!\n","processed 29000 records!!\n","processed 30000 records!!\n","processed 31000 records!!\n","processed 32000 records!!\n","processed 33000 records!!\n","processed 34000 records!!\n","processed 35000 records!!\n","processed 36000 records!!\n","processed 37000 records!!\n","processed 38000 records!!\n","processed 39000 records!!\n","processed 40000 records!!\n","processed 41000 records!!\n","processed 42000 records!!\n","processed 43000 records!!\n","processed 44000 records!!\n","processed 45000 records!!\n","processed 46000 records!!\n","processed 47000 records!!\n","processed 48000 records!!\n","processed 49000 records!!\n","processed 50000 records!!\n","processed 51000 records!!\n","processed 52000 records!!\n","processed 53000 records!!\n","processed 54000 records!!\n","processed 55000 records!!\n","processed 56000 records!!\n","processed 57000 records!!\n","processed 58000 records!!\n","processed 59000 records!!\n","processed 60000 records!!\n","processed 61000 records!!\n","processed 62000 records!!\n","processed 63000 records!!\n","processed 64000 records!!\n","processed 65000 records!!\n","processed 66000 records!!\n","processed 67000 records!!\n","processed 68000 records!!\n","processed 69000 records!!\n","processed 70000 records!!\n","processed 71000 records!!\n","processed 72000 records!!\n","processed 73000 records!!\n","processed 74000 records!!\n","processed 75000 records!!\n","processed 76000 records!!\n","processed 77000 records!!\n","processed 78000 records!!\n","processed 79000 records!!\n","processed 80000 records!!\n","processed 81000 records!!\n","processed 82000 records!!\n","processed 83000 records!!\n","processed 84000 records!!\n","processed 85000 records!!\n","processed 86000 records!!\n","processed 87000 records!!\n","processed 88000 records!!\n","processed 89000 records!!\n","processed 90000 records!!\n","processed 91000 records!!\n","processed 92000 records!!\n","processed 93000 records!!\n","processed 94000 records!!\n","processed 95000 records!!\n","processed 96000 records!!\n","processed 97000 records!!\n","processed 98000 records!!\n","processed 99000 records!!\n","processed 100000 records!!\n","processed 101000 records!!\n","processed 102000 records!!\n","processed 103000 records!!\n","processed 104000 records!!\n","processed 105000 records!!\n","processed 106000 records!!\n","processed 107000 records!!\n","processed 108000 records!!\n","processed 109000 records!!\n","processed 110000 records!!\n","processed 111000 records!!\n","processed 112000 records!!\n","processed 113000 records!!\n","processed 114000 records!!\n","processed 115000 records!!\n","processed 116000 records!!\n","processed 117000 records!!\n","processed 118000 records!!\n","processed 119000 records!!\n","processed 120000 records!!\n","processed 121000 records!!\n","processed 122000 records!!\n","processed 123000 records!!\n","processed 124000 records!!\n","processed 125000 records!!\n","processed 126000 records!!\n","processed 127000 records!!\n","processed 128000 records!!\n","processed 129000 records!!\n","Done!!\n","Anomaly Preprocessing complete!!!\n"]}]},{"cell_type":"code","metadata":{"id":"rdR_e170WZiJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653132518906,"user_tz":-330,"elapsed":73158,"user":{"displayName":"","userId":""}},"outputId":"5b9a52ba-f316-4046-cb7d-679783e2a5e6"},"source":["# Extracts the latitude/longitude information of each crime reported\n","# And assigns it to one of the precincts defined earlier in form of a MultiPolygon\n","\n","# Also, some preprocessing is done on the date columns as well\n","# To ensure that the final date column doesn't have NaN values\n","\n","pos = 0\n","prec = np.ndarray((crime2006.shape[0],))\n","date = np.ndarray((crime2006.shape[0],)).astype(str)\n","\n","for index,row in crime2006.iterrows():\n","  \n","  if not(str(row['CMPLNT_FR_DT']) == 'nan'):\n","    date[pos] = row['CMPLNT_FR_DT']\n","  elif not(str(row['CMPLNT_TO_DT']) == 'nan'):\n","    date[pos] = row['CMPLNT_TO_DT']\n","  else:\n","    date[pos] = row['RPT_DT']\n","  \n","  poo = Point(row['Longitude'],row['Latitude'])  \n","  for key,val in precincts.items():\n","    if poo.within(val):\n","      prec[pos] = key\n","      break\n","\n","  pos=pos+1\n","  if(pos%1000 == 0):\n","    print (\"processed \"+str(pos)+\" records!!\")\n","\n","# Adds the precincts information to the dataset\n","# Adds the dates information to the dataset\n","\n","crime2006['Precincts'] = prec.astype(int)\n","crime2006['Date'] = date\n","crime2006 = crime2006[crime2006['Precincts'] >= 0]\n","\n","print(\"Done!!\")\n","\n","# Extract the final columns needed for training the model\n","crime2006 = crime2006[finalColumnsCR]\n","crime2006.sort_values(['Precincts','Date','OFNS_DESC'],ascending=[True,True,True],inplace=True)\n","crime2006 = crime2006[crime2006['Date'].str.contains(year)]\n","crime2006['Precincts'] = crime2006['Precincts'].astype(np.int64)\n","\n","print('Crime Preprocessing complete!!!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processed 1000 records!!\n","processed 2000 records!!\n","processed 3000 records!!\n","processed 4000 records!!\n","processed 5000 records!!\n","processed 6000 records!!\n","processed 7000 records!!\n","processed 8000 records!!\n","processed 9000 records!!\n","processed 10000 records!!\n","processed 11000 records!!\n","processed 12000 records!!\n","processed 13000 records!!\n","processed 14000 records!!\n","processed 15000 records!!\n","processed 16000 records!!\n","processed 17000 records!!\n","processed 18000 records!!\n","processed 19000 records!!\n","processed 20000 records!!\n","processed 21000 records!!\n","processed 22000 records!!\n","processed 23000 records!!\n","processed 24000 records!!\n","processed 25000 records!!\n","processed 26000 records!!\n","processed 27000 records!!\n","processed 28000 records!!\n","processed 29000 records!!\n","processed 30000 records!!\n","processed 31000 records!!\n","processed 32000 records!!\n","processed 33000 records!!\n","processed 34000 records!!\n","processed 35000 records!!\n","processed 36000 records!!\n","processed 37000 records!!\n","processed 38000 records!!\n","processed 39000 records!!\n","processed 40000 records!!\n","processed 41000 records!!\n","processed 42000 records!!\n","processed 43000 records!!\n","processed 44000 records!!\n","processed 45000 records!!\n","processed 46000 records!!\n","processed 47000 records!!\n","processed 48000 records!!\n","processed 49000 records!!\n","processed 50000 records!!\n","processed 51000 records!!\n","processed 52000 records!!\n","processed 53000 records!!\n","processed 54000 records!!\n","processed 55000 records!!\n","processed 56000 records!!\n","processed 57000 records!!\n","processed 58000 records!!\n","processed 59000 records!!\n","processed 60000 records!!\n","processed 61000 records!!\n","processed 62000 records!!\n","processed 63000 records!!\n","processed 64000 records!!\n","processed 65000 records!!\n","processed 66000 records!!\n","processed 67000 records!!\n","processed 68000 records!!\n","processed 69000 records!!\n","processed 70000 records!!\n","processed 71000 records!!\n","processed 72000 records!!\n","processed 73000 records!!\n","processed 74000 records!!\n","processed 75000 records!!\n","processed 76000 records!!\n","processed 77000 records!!\n","processed 78000 records!!\n","processed 79000 records!!\n","processed 80000 records!!\n","processed 81000 records!!\n","processed 82000 records!!\n","processed 83000 records!!\n","processed 84000 records!!\n","processed 85000 records!!\n","processed 86000 records!!\n","processed 87000 records!!\n","processed 88000 records!!\n","processed 89000 records!!\n","processed 90000 records!!\n","processed 91000 records!!\n","processed 92000 records!!\n","processed 93000 records!!\n","processed 94000 records!!\n","processed 95000 records!!\n","processed 96000 records!!\n","processed 97000 records!!\n","processed 98000 records!!\n","processed 99000 records!!\n","processed 100000 records!!\n","processed 101000 records!!\n","processed 102000 records!!\n","processed 103000 records!!\n","processed 104000 records!!\n","processed 105000 records!!\n","processed 106000 records!!\n","processed 107000 records!!\n","processed 108000 records!!\n","processed 109000 records!!\n","Done!!\n","Crime Preprocessing complete!!!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}]},{"cell_type":"code","metadata":{"id":"PWknfNTtU9Dt"},"source":["# Generates inverted dictionaries for\n","#     1. Date for the particular year, according to the 'year' var defined earlier\n","#     2. Complaints: according to the 4 categories that we are considering\n","#     3. Precincts: the grographical divisions of the city\n","#     4. Crimes: according to the 4 categories we are considering for prediction\n","\n","# The final input that we feed to the Heirarchical LSTMs is in form of matrices\n","# These inverted indices help in conversion of the dataframe to thse matrices\n","\n","daterange = pd.date_range(start='01/01/'+year,end='12/31/'+year)\n","daterange = daterange.strftime('%m/%d/%Y')\n","daterange = daterange.tolist()\n","\n","inv_dict = {}\n","for i in range(len(daterange)):\n","  inv_dict[daterange[i]] = i\n","\n","inv_complaints = {}\n","for i in range(len(relevantComplaints311)):\n","  inv_complaints[relevantComplaints311[i]] = i\n","\n","n_precincts = data311['Precincts'].nunique()\n","uniq_precincts = data311['Precincts'].unique()\n","inv_prec = {}\n","for i in range(n_precincts):\n","  inv_prec[uniq_precincts[i]] = i\n","\n","inv_crimes = {}\n","for i in range(len(relevantCrimesCR)):\n","  inv_crimes[relevantCrimesCR[i]] = i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2ec1hJVWxXO","outputId":"d722ddb0-6b60-44a8-a221-798414f159af","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653132558410,"user_tz":-330,"elapsed":24788,"user":{"displayName":"","userId":""}}},"source":["# Generate the matrices for Crime (365x78x4) and 311 Complaints (365x78x4)\n","# Here the 2nd dimension represents the precinct and is 78, instead of 77\n","# This is because all the complaints/crimes with no geographical location have been assigned 0th position\n","# This 0th position is later discarded during the training process, but can be used if one wishes to do so\n","\n","# Each matrix represents crime/complaint occurence over different categories(4), over different localities(78) over a period of 1 year(365)\n","\n","\n","# Generating the crime records matrices\n","matricesCR = [np.zeros((n_precincts,4),dtype=np.int64) for x in range(365)]\n","exceptions = 0\n","\n","for idx, row in crime2006.iterrows():\n","  try:\n","    id1 = inv_prec[row['Precincts']]\n","    id2 = inv_dict[row['Date']]\n","    id3 = inv_crimes[row['OFNS_DESC']]\n","    matricesCR[id2][id1][id3] = matricesCR[id2][id1][id3] + 1\n","  except:\n","    print(\"Exception!!!\")\n","    print(\"Precincts\",id1)\n","    print(\"Date\",id2)\n","    print(\"Offense\",id3)\n","    exceptions = exceptions + 1\n","\n","print ('Created crime matrices!!!')\n","\n","# Generating the 311 complaints records matrices\n","matrices311 = [np.zeros((n_precincts,4),dtype=np.int64) for x in range(365)]\n","exceptions = 0\n","\n","for idx, row in data311.iterrows():\n","  try:\n","    id1 = inv_prec[row['Precincts']]\n","    id2 = inv_dict[str(row['Created Date']).split()[0]]\n","    id3 = inv_complaints[row['Complaint Type']]\n","    matrices311[id2][id1][id3] = matrices311[id2][id1][id3] + 1\n","  except:\n","    print(\"Exception!!!\")\n","    print(\"Precincts\",id1)\n","    print(\"Date\",id2)\n","    print(\"Offense\",id3)\n","    exceptions = exceptions + 1\n","\n","print('Created anomaly matrices!!!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Created crime matrices!!!\n","Created anomaly matrices!!!\n"]}]},{"cell_type":"code","source":["!mkdir \"/content/gdrive/MyDrive/Dataset/generated\""],"metadata":{"id":"258Rz_ChYVJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47hlZc9naCK9"},"source":["import pickle\n","\n","\n","# Dump the matrices using pickle, to a file\n","output311 = '/content/gdrive/MyDrive/Dataset/generated/matrices311'\n","outputCrime = '/content/gdrive/MyDrive/Dataset/generated/matricesCR'\n","\n","file311 = open(output311,'wb')\n","pickle.dump(matrices311,file311)\n","\n","fileCR = open(outputCrime,'wb')\n","pickle.dump(matricesCR,fileCR)\n","\n","# To load these files, use:\n","# with open(filename,'rb') as toOpen:\n","#     data = pickle.load(toOpen)"],"execution_count":null,"outputs":[]}]}